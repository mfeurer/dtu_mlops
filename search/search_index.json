{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Machine Learning Operations <p>Repository for course 02476 at DTU.</p> <p>Checkout the homepage!</p> </p> <p> </p>"},{"location":"#i-course-information","title":"\u2139\ufe0f Course information","text":"<ul> <li>Course responsible<ul> <li>Postdoc Nicki Skafte Detlefsen, nsde@dtu.dk</li> <li>Professor S\u00f8ren Hauberg, sohau@dtu.dk</li> </ul> </li> <li>5 ECTS (European Credit Transfer System), corresponding to 140 hours of work</li> <li>3 week period of January 2023</li> <li>Master course</li> <li>Grade: Pass/not passed</li> <li>Type of assessment: weekly project updates + final oral examination/presentation</li> <li>Recommended prerequisites: DTU course 02456 (Deep Learning) or   experience with the following topics:<ul> <li>General understanding of machine learning (datasets, probability, classifiers, overfitting ect.)</li> <li>Basic knowledge about deep learning (backpropagation, convolutional neural networks, auto-encoders ect.)</li> <li>Coding in PyTorch. The first day we provide a number of exercises in PyTorch to     get everyone's skills up-to-date as fast as possible.</li> </ul> </li> </ul>"},{"location":"#course-setup","title":"\ud83d\udcbb Course setup","text":"<p>Start by cloning or downloading this repository</p> <pre><code>git clone https://github.com/SkafteNicki/dtu_mlops\n</code></pre> <p>If you do not have git installed (yet) we will touch upon it in the course. The folder will contain all exercise material for this course and lectures. Additionally, you should join our Slack channel which we use for communication. Link may be expired, write to me.</p>"},{"location":"#course-organization","title":"\ud83d\udcc2 Course organization","text":"<p>We highly recommend that when going through the material that you use the homepage which is the corresponding Github pages version of this repository that is more nicely rendered, that also includes some special HTML magic provided by just the docs.</p> <p>The course is divided into sessions, denoted by capital S, and modules, denoted by capital M. A session corresponds to full day of work if you are following the course, meaning approx 9 hours of work. Each session (S) corresponds to topic within MLOps and consist of multiple modules (M) that each covers an tool within the session.</p> <p>Importantly we differ between core modules and optional modules. Core modules will be marked by</p> <p>Core Module</p> <p>at the top of their corresponding page. Core modules are important to go through to be able to pass the course. You are highly recommended to still do the optional modules.</p>"},{"location":"#mlops-what-is-it","title":"\ud83c\udd92 MLOps: What is it?","text":"<p>Machine Learning Operations (MLOps) is a rather new field that has seen its uprise as machine learning and particular deep learning has become a technology that is widely available. The term itself is a compound of \"machine learning\" and \"operations\" and covers everything that has to do with the management of the production ML lifecycle.</p> <p>The lifecycle of production ML can largely be divided into three phases:</p> <ol> <li> <p>Design: The initial phase starts with a investigation of the problem. Based on this analysis, a number of requirements can be prioritized of what we want our future model to actually do. Since machine learning requires data to be trained, we also investigate in this step what data we have and if we need to source it in some other way.</p> </li> <li> <p>Model development: Based on the design phase we can actually begin to conjurer some machine learning algorithm to solve our problems. As always, the initial step often involve doing some data analysis to make sure that our model is actually learning the signal that we want it to learn. Secondly, is the machine learning engineering phase, where the particular model architecture is chosen. Finally, we also need to do validation and testing to make sure that our model is generalizing well.</p> </li> <li> <p>Operations: Based on the model development phase, we now have a model and we actual want to use. The operations is where create an automatic pipeline that makes sure that whenever we make changes to our codebase they gets automatically incorporated into our model, such that we do not slow down production. Equally important is also the ongoing monitoring of already deployed models to make sure that they behave exactly as we specified them.</p> </li> </ol> <p>It is important to note that the three steps are in fact a cycle, meaning that we you have successfully deployed a machine learning model that is not the end of it. Your initial requirements may change, forcing you to revisit the design phase. Some new algorithm may show promising results, so you revisit the model development phase to implement this. And finally, you may try to cut the cost of running your model in production, making you revisit the operations phase, trying to optimize some steps.</p> <p>The focus in this course is particular on the Operations part of MLOps as this is what many data scientist are missing in their toolbox to take all the knowledge they have about data processing and model development into a production setting.</p>"},{"location":"#learning-objectives","title":"\u2754 Learning objectives","text":"<p>General course objective</p> <p>Introduce the student to a number of coding practices that will help them organization, scale, monitor and deploy machine learning models either in a research or production setting. To provide hands-on experience with a number of frameworks, both local and in the cloud, for doing large scale machine learning models.</p> <p>This includes:</p> <ul> <li>Organize code in a efficient way for easy maintainability and shareability</li> <li>Understand the importance of reproducibility and how to create reproducible containerized applications and experiments</li> <li>Cable of using version control to efficiently collaborate on code development</li> <li>Knowledge of continuous integration (CI) and continuous machine learning (CML) for automating code development</li> <li>Being able to debug, profile, visualize and monitor multiple experiments to assess model performance</li> <li>Cable of using online cloud based computing services to scale experiments</li> <li>Demonstrate knowledge about different distributed training paradigms within  machine learning and how to apply them</li> <li>Deploy machine learning models, both locally and in the cloud</li> <li>Conduct a research project in collaboration with follow students using the frameworks taught in the course</li> <li>Have lots of fun and share memes! :)</li> </ul>"},{"location":"#references","title":"\ud83d\udcd3 References","text":"<p>Additional reading resources (in no particular order):</p> <ul> <li> <p>Ref 1   Introduction blog post for those that have never heard about MLOps and want to get an overview.</p> </li> <li> <p>Ref 2   Great document from Google about the different levels of MLOps.</p> </li> <li> <p>Ref 3   Another introduction to the principles of MLOps and the different stages of MLOps.</p> </li> <li> <p>Ref 4   Great paper about the technical depth in machine learning.</p> </li> <li> <p>Ref 5   Interview study that uncovers many of the pain points that ML engineers go through when doing MLOps.</p> </li> </ul> <p>Other courses with content similar to this:</p> <ul> <li> <p>Made with ML. Great online MLOps course that also covers additional topics on the   foundations of working with ML.</p> </li> <li> <p>Full stack deep learning. Another MLOps online course the does through the hole   developer pipeline.</p> </li> <li> <p>MLOps Zoomcamp. MLOps online course that includes many of the same   topics.</p> </li> </ul>"},{"location":"#contributing","title":"\ud83d\udc68\u200d\ud83c\udfeb Contributing","text":"<p>If you want to contribute to the course, we are happy to have you! Anything from fixing typos to adding new content is welcome. For building the course material locally, it is a simple two step process:</p> <pre><code>pip install -r requirements.txt\nmkdocs serve\n</code></pre> <p>Which will start a local server that you can access at <code>localhost:8000</code> and will automatically update when you make changes to the course material. When you have something that you want to contribute, please make a pull request.</p>"},{"location":"#license","title":"\u2755 License","text":"<p>I highly value open-source, and the content of this course is therefore free to use under the Apache 2.0 license. If you use parts of this course in your own work, please cite using:</p> <pre><code>@misc{skafte_mlops,\nauthor       = {Nicki Skafte Detlefsen},\ntitle        = {Machine Learning Operations},\nhowpublished = {\\url{https://github.com/SkafteNicki/dtu_mlops}},\nyear         = {2022}\n}\n</code></pre>"},{"location":"challenges/","title":"Challenges","text":"<p>If you have managed to go through all other material, congratulations, you are already a good way to becoming an MLOps engineer with a great overview of tools, concepts and techniques within the field. Below are listed some technical hard problems regarding MLOps. These are meant as inspiration to get you to deep dive more into using all the cloud services that <code>gcp</code> offers. You are also free to continue work on your project.</p> <p></p> <ul> <li> <p>Currently testing takes place in Github, but it should come as no     surprise that <code>gcp</code> can also take care of this. Implementing testing     on <code>gcp</code>. This     blogpost     can probably help.</p> </li> <li> <p>In the lectures we setup cloud build to automatically build a docker     container for training whenever we pushed code to our github repository.     However, we also setup CI testing in github. If tests are failing on     github the building of the docker image is still being done, essentially     wasting our precious cloud credit. Setup a system so cloud building only     commence when all tests are passing.</p> </li> <li> <p>Authenticating between <code>gcp</code>, <code>wandb</code> and <code>dvc</code> can be tricky to do in     a secure way. Figure out how to use the Secret Manager in <code>gcp</code> to     pass secrets e.g. API keys during the build process of docker images.     This page     may help</p> </li> <li> <p>We have already done deployment through <code>Cloud Functions</code>. The native extension     to cloud functions is the service <code>Cloud Run</code> which allows for more than     just code snippets to be deployed. Checkout this service and try to deploy     a container using it.</p> </li> <li> <p>All deployments we have done in the course have been serverless, because     it makes it easier for us to focus on the actual application we are trying     to deploy instead of focusing on server management. That said, going through     the trouble of using a server orchestrator yourself can be worth it in many     situations. Figure out how to use kubernetes in <code>gcp</code>. It will involve getting     familiar with the kubernetes API and probably also kubeflow for managing     pipelines on the server.</p> </li> <li> <p>Vertex AI is the newest ML service on <code>gcp</code>. It combines many of the features     of the AI platform service you have already used with the AutoML service. Figure     out how to use Vertex AI service to either train a custom model or use their     AutoML feature. This     blogpost     can be a good place to start.</p> </li> <li> <p>If you want different services to be able to talk to each other the correct way     is to setup a system using Pub and Sub     (publish and subscription) service in <code>gcp</code>. Essentially it allows a service     to publish a message and other services to subscribe and react to it. For     example the AI platform could publish a message every time a model was done     training and cloud build could subscribe to that, automatically staring to     build a docker image using the trained model. Investigate Pub and Sub and     try to make two services talk to each other.</p> </li> <li> <p>In the deployment exercises you probably looked at least once on the logs. We can     automate what we do with the logs using the Logs Explorer service, which collects     all logs from all services that you are using. Setup     Logs routing for one of     your deployed services to your cloud storage. Afterwards setup a VM that consumes     the logs and accumulate them.</p> </li> </ul>"},{"location":"overview/","title":"Summary of course content","text":"<p>There are a lot of moving parts in this course, so it may be hard to understand how it all fits together. This page provides a summary of the frameworks in this course e.g. the stack of tools used. In the figure below we have provided an overview on how the different tools of the course interacts with each other. The table after the figure provides a short description of each of the parts.</p> <p></p>  The MLOps stack in the course. This is just an example of one stack, and depending on your use case you may want to use a different stack of tools that better fits your needs. Regardless of the stack, the principles of MLOps are the same.  Framework Description Pytorch is the backbone of our code, it provides the computational engine and the data structures that we need to define our data structures. Pytorch lightning is a framework that provides a high-level interface to Pytorch. It provides a lot of functionality that we need to train our models, such as logging, checkpointing, early stopping, etc. such that we do not have to implement it ourselves. It also allows us to scale our models to multiple GPUs and multiple nodes. We control the dependencies and python interpreter using Conda that enables us to construct reproducible virtual environments For configuring our experiments we use Hydra that allows us to define a hierarchical configuration structure config files Using Weights and Bias allows us to track and log any values and hyperparameters for our experiments Whenever we run into performance bottlenecks with our code we can use the Profiler to find the cause of the bottleneck When we run into bugs in our code we can use the Debugger to find the cause of the bug For organizing our code and creating templates we can use Cookiecutter Docker is a tool that allows us to create a container that contains all the dependencies and code that we need to run our code For controlling the versions of our data and synchronization between local and remote data storage, we can use DVC that makes this process easy For version control of our code we use Git (in complement with Github) that allows multiple developers to work together on a shared codebase We can use Pytest to write unit tests for our code, to make sure that new changes to the code does break the code base For linting our code and keeping a consistent coding style we can use tools such as Pylint and Flake8 that checks our code for common mistakes and style issues For running our unit tests and other checks on our code in a continues manner e.g. after we commit and push our code we can use Github actions that automatize this process Using Cloud build we can automate the process of building our docker images and pushing them to our container registry Container registry is a service that allows us to store our docker images for later use by other services For storing our data and trained models we can use Cloud storage that provides a scalable and secure storage solution For general compute tasks we can use Compute engine that provides a scalable and secure compute solution For training our experiments in a easy and scalable manner we can use Vertex AI For creating a REST API for our model we can use FastAPI that provides a high-level interface for creating APIs For simple deployments of our code we can use Cloud functions that allows us to run our code in response to events through simple python functions For more complex deployments of our code we can use Cloud run that allows us to run our code in response to events through docker containers Cloud monitoring gives us the tools to keep track of important logs and errors from the other cloud services For monitoring our deployed model is experiencing any drift we can use Evidently AI that provides a framework and dashboard for monitoring drift For monitoring the telemetry of our deployed model we can use OpenTelemetry that provides a standard for collecting and exporting telemetry data"},{"location":"projects/","title":"Project work","text":"<p>Slides</p> <p>Approximately 1/3 of the course time is dedicated to doing project work. The projects will serve as the basis of your exam. In the project, you will essentially re-apply everything that you learn throughout the course to a self chosen project. The overall goals with the project is:</p> <ul> <li>To formulate a project within the provided guidelines</li> <li>Apply the material though in the course to the problem</li> <li>Present your findings</li> </ul> <p>In the projects you are free to work on whatever problem that you want. That said, the only requirement that we have is that you should incorporate one of the listed below frameworks from the Pytorch ecosystem into your project.</p>"},{"location":"projects/#the-pytorch-ecosystem","title":"The Pytorch Ecosystem","text":"<p>The Pytorch ecosystem is a great place for finding open-source frameworks that can help you accelerate your own research. However, it is important to note that the ecosystem is not a complete list of all the awesome packages that exist to extend the functionality of Pytorch. For the project work you will need to choose between one of three such frameworks which will serve as the basis of your project. The three frameworks are:</p> <ul> <li> <p>PyTorch Image Models. PyTorch Image Models (also known as TIMM)   is the absolutly most used computer vision package (maybe except for <code>torchvision</code>). It contains models, scripts and   pre trained for a lot of state-of-the-art image models within computer vision.</p> </li> <li> <p>Transformers. The Transformers repository from the Huggingface group   focuses on state-of-the-art Natural Language Processing (NLP). It provides many pre-trained model to perform tasks on   texts such as classification, information extraction, question answering, summarization, translation, text generation,   etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.</p> </li> <li> <p>Pytorch-Geometric. PyTorch Geometric (PyG) is a geometric deep   learning. It consists of various methods for deep learning on graphs and other irregular structures, also known as   geometric deep learning, from a variety of published papers.</p> </li> </ul>"},{"location":"projects/#exercises-for-project-day-1","title":"\u2754 Exercises for project day 1","text":"<p>Today is also dedicated to doing project work. Remember that the focus of the project work is not to demonstrate that you can work with the biggest and baddest deep learning model, but instead that you show that you can incorporate the tools that are taught throughout the course in a meaningful way.</p> <p>Also note that the project is not expected to be very large in scope. It may simply be that you want to train X model on Y data. You will approximately be given 4 full days to work on the project. It is better that you start out with a smaller project and then add complexity along the way if you have time.</p> <ol> <li> <p>(Optional) Familiar yourself with each of the libraries. One way to do this is to find relevant tutorials on each     project and try to figure out the code. Such tutorials will give you a rough idea how the API for each library looks     like.</p> </li> <li> <p>Form groups! The recommended group size is 4 persons, but we also accept 3 or 5 man groups. Try to find other people     based on what framework that you would like to work with.</p> </li> <li> <p>Brainstorm projects! Try to figure out exactly what you want to work with and especially how you are going to     incorporate the frameworks (we are aware that you are not familiar with every framework yet) that you have chosen to     work with into your project. The Final exercise for today is to formulate a project description (see bottom of     this page).</p> </li> <li> <p>When you formed groups and formulated a project you are allowed to start working on the actual code. I have included     a to-do list at the bottom that somewhat summaries what we have done in the course until know. You are NOT     expected to fulfill all bullet points on the to-do list today, as you will continue to work on the project in the     following two weeks.</p> </li> </ol> <p>Final exercise for today is making a project description. Write around half to one page about:</p> <ul> <li>Overall goal of the project</li> <li>What framework are you going to use (PyTorch Image Models, Transformer, Pytorch-Geometrics)</li> <li>How to you intend to include the framework into your project</li> <li>What data are you going to run on (initially, may change)</li> <li>What deep learning models do you expect to use</li> </ul> <p>The project description will serve as an guideline for us at the exam that you have somewhat reached the goals that you set out to do. For inspiration you can take a look at the following two projects from the last year:</p> <ol> <li>Classification of tweets using Transformers</li> <li>Classification of scientific papers using PyG</li> </ol> <p>By the end of the day (17:00) you should upload your project description (in the <code>README.md</code> file belonging to your project repository) + whatever you have done on the project until now to your github repository. When this you have done this, on DTU Learn go to assignments and hand in (as a group) the link to your github repository.</p> <p>We will briefly (before next Monday) look over your github repository and project description to check that everything is fine. If we have any questions/concerns we will contact you.</p>"},{"location":"projects/#project-checklist","title":"Project checklist","text":"<p>Please note that all the lists are exhaustive meaning that I do not expect you to have completed very point on the checklist for the exam.</p>"},{"location":"projects/#week-1","title":"Week 1","text":"<ul> <li> Create a git repository</li> <li> Make sure that all team members have write access to the github repository</li> <li> Create a dedicated environment for you project to keep track of your packages</li> <li> Create the initial file structure using cookiecutter</li> <li> Fill out the <code>make_dataset.py</code> file such that it downloads whatever data you need and</li> <li> Add a model file and a training script and get that running</li> <li> Remember to fill out the <code>requirements.txt</code> file with whatever dependencies that you are using</li> <li> Remember to comply with good coding practices (<code>pep8</code>) while doing the project</li> <li> Do a bit of code typing and remember to document essential parts of your code</li> <li> Setup version control for your data or part of your data</li> <li> Construct one or multiple docker files for your code</li> <li> Build the docker files locally and make sure they work as intended</li> <li> Write one or multiple configurations files for your experiments</li> <li> Used Hydra to load the configurations and manage your hyperparameters</li> <li> When you have something that works somewhat, remember at some point to to some profiling and see if       you can optimize your code</li> <li> Use Weights &amp; Biases to log training progress and other important metrics/artifacts in your code. Additionally,       consider running a hyperparameter optimization sweep.</li> <li> Use Pytorch-lightning (if applicable) to reduce the amount of boilerplate in your code</li> </ul>"},{"location":"projects/#week-2","title":"Week 2","text":"<ul> <li> Write unit tests related to the data part of your code</li> <li> Write unit tests related to model construction and or model training</li> <li> Calculate the coverage.</li> <li> Get some continuous integration running on the github repository</li> <li> Create a data storage in GCP Bucket for you data and preferable link this with your data version control setup</li> <li> Create a trigger workflow for automatically building your docker images</li> <li> Get your model training in GCP using either the Engine or Vertex AI</li> <li> Create a FastAPI application that can do inference using your model</li> <li> If applicable, consider deploying the model locally using torchserve</li> <li> Deploy your model in GCP using either Functions or Run as the backend</li> </ul>"},{"location":"projects/#week-3","title":"Week 3","text":"<ul> <li> Check how robust your model is towards data drifting</li> <li> Setup monitoring for the system telemetry of your deployed model</li> <li> Setup monitoring for the performance of your deployed model</li> <li> If applicable, play around with distributed data loading</li> <li> If applicable, play around with distributed model training</li> <li> Play around with quantization, compilation and pruning for you trained models to increase inference speed</li> </ul>"},{"location":"projects/#additional","title":"Additional","text":"<ul> <li> Revisit your initial project description. Did the project turn out as you wanted?</li> <li> Make sure all group members have a understanding about all parts of the project</li> <li> Uploaded all your code to github</li> </ul>"},{"location":"projects/#exam","title":"Exam","text":"<p>The exam consist of a written and oral element, and both contributes to the overall evaluation if you should pass or not pass the course.</p> <p>For the written part of the exam we provide an template folder called reports. As the first task you should copy the folder and all its content to your project repository. Then, you jobs is to fill out the <code>README.md</code> file which contains the report template. The file itself contains instructions on how to fill it out and instructions on using the included <code>report.py</code> file. You will hand-in the template by simple including it in your project repository. By midnight on the 20/1 we will scrape it automatically, and changes after this point are therefore not registered.</p> <p>For the oral part of the exam you will be given a time slot where you have to show up for 5-7 min and give a very short demo of your project. What we are interested in seeing is essentially a live demo of your deployed application/project. We will possibly also ask questions regarding the overall curriculum of the course. Importantly, you should have your deployed application, the github repository with your project code, W&amp;B account and your GCP account ready before you enter the exam so we can quickly jump around. We will send out an the time slots during the last week.</p>"},{"location":"timeplan/","title":"Timeplan","text":"<p>Slides</p> <p>The course is organised into exercise (2/3 of the course) days and project days (1/3 of the course).</p> <p>Exercise days start at 9:00 in the morning with an lecture (15-30 min) that will give some context about atleast one of the topics of that day. Additionally, previous days exercises may shortly be touched upon. The remaining of the day will be spend on solving exercises either individually or in small groups. For some people the exercises may be fast to do and for others it will take the hole day. We will provide help throughout the day. We will try to answer questions on slack but help with be priorities to students physically on campus.</p> <p>Project days are intended for project work and you are therefore responsable for making an agreement with your group when and where you are going to work. The first project days there will be a lecture at 9:00 with project information. Other project days we may also start the day with an external lecture, which we highly recommend that you participate in. During each project day we will have office hours where you can ask questions for the project.</p> <p>Below is an overall timeplan for each day, including the presentation topic of the day and the frameworks that you will be using in the exercises.</p> <p>Legend: \ud83d\udcdd Slides, \ud83c\udfa5 Recording.</p>"},{"location":"timeplan/#week-1","title":"Week 1","text":"<p>In the first week you will be introduced to a number of development practises for organising and developing code, especially with a focus on making everything reproducible.</p> Date Day Presentation topic Frameworks Format 2/1 Monday Deep learning software  \ud83d\udcdd \ud83c\udfa5 Terminal, Conda, IDE, Pytorch Exercises 3/1 Tuesday MLOps: what is it?  \ud83d\udcdd \ud83c\udfa5 Git, CookieCutter, Pep8, DVC Exercises 4/1 Wednesday Reproducibility  \ud83d\udcdd \ud83c\udfa5 Docker, Hydra Exercises 5/1 Thursday Debugging  \ud83d\udcdd \ud83c\udfa5 Debugger, Profiler, Wandb, Lightning Exercises 6/1 Friday Pytorch ecosystem  \ud83d\udcdd \ud83c\udfa5 - Projects"},{"location":"timeplan/#week-2","title":"Week 2","text":"<p>The second week is about automatization and the cloud. Automatization will help use making sure that our code does not break when we make changes to it. The cloud will help us scale up our applications and we learn how to use different services to help develop a full machine learning pipeline.</p> Date Day Presentation topic Frameworks Format 9/1 Monday Continuous Integration  \ud83d\udcdd \ud83c\udfa5 Pytest, Github actions, Pre-commit, CML Exercises 10/1 Tuesday The Cloud  \ud83d\udcdd \ud83c\udfa5 GCP Engine, Bucket, Container registry, Vertex AI Exercises 11/1 Wednesday Deployment  \ud83d\udcdd \ud83c\udfa5 FastAPI, Torchservce, GCP Functions, Run Exercises 12/1 Thursday Guest lecture  \ud83c\udfa5 - Projects 13/1 Friday Guest lecture  \ud83c\udfa5 - Projects"},{"location":"timeplan/#week-3","title":"Week 3","text":"<p>For the final week we look into advance topics such as monitoring and scaling of applications. Monitoring is especially important for the longivity for the applications that we develop, that we actually can deploy them either locally or in the cloud and that we have the tools to monitor how they behave over time. Scaling of applications is an important topic if we ever want our applications to be used by many people at the same time.</p> Date Day Presentation topic Frameworks Format 16/1 Monday Monitoring (Guest lecture)  \ud83c\udfa5 Evidently AI, OpenTelemetry, Signoz Exercises 17/1 Tuesday Scalable applications  \ud83d\udcdd \ud83c\udfa5 Pytorch, Lightning Exercises 18/1 Wednesday - - Projects 19/1 Thursday - - Projects 20/1 Friday - - Exam"},{"location":"reports/","title":"Exam template for 02476 Machine Learning Operations","text":"<p>This is the report template for the exam. Please only remove the text formatted as with three dashes in front and behind like:</p> <p><code>--- question 1 fill here ---</code></p> <p>where you instead should add your answers. Any other changes may have unwanted consequences when your report is auto generated in the end of the course. For questions where you are asked to include images, start by adding the image to the <code>figures</code> subfolder (please only use <code>.png</code>, <code>.jpg</code> or <code>.jpeg</code>) and then add the following code in your answer:</p> <pre><code>![my_image](figures/&lt;image&gt;.&lt;extension&gt;)\n</code></pre> <p>In addition to this markdown file, we also provide the <code>report.py</code> script that provides two utility functions:</p> <p>Running:</p> <pre><code>python report.py html\n</code></pre> <p>will generate an <code>.html</code> page of your report. After deadline for answering this template, we will autoscrape everything in this <code>reports</code> folder and then use this utility to generate an <code>.html</code> page that will be your serve as your final handin.</p> <p>Running</p> <pre><code>python report.py check\n</code></pre> <p>will check your answers in this template against the constrains listed for each question e.g. is your answer too short, too long, have you included an image when asked to.</p> <p>For both functions to work it is important that you do not rename anything. The script have two dependencies that can be installed with <code>pip install click markdown</code>.</p>"},{"location":"reports/#overall-project-checklist","title":"Overall project checklist","text":"<p>The checklist is exhaustic which means that it includes everything that you could possible do on the project in relation the curricilum in this course. Therefore, we do not expect at all that you have checked of all boxes at the end of the project.</p>"},{"location":"reports/#week-1","title":"Week 1","text":"<ul> <li> Create a git repository</li> <li> Make sure that all team members have write access to the github repository</li> <li> Create a dedicated environment for you project to keep track of your packages</li> <li> Create the initial file structure using cookiecutter</li> <li> Fill out the <code>make_dataset.py</code> file such that it downloads whatever data you need and</li> <li> Add a model file and a training script and get that running</li> <li> Remember to fill out the <code>requirements.txt</code> file with whatever dependencies that you are using</li> <li> Remember to comply with good coding practices (<code>pep8</code>) while doing the project</li> <li> Do a bit of code typing and remember to document essential parts of your code</li> <li> Setup version control for your data or part of your data</li> <li> Construct one or multiple docker files for your code</li> <li> Build the docker files locally and make sure they work as intended</li> <li> Write one or multiple configurations files for your experiments</li> <li> Used Hydra to load the configurations and manage your hyperparameters</li> <li> When you have something that works somewhat, remember at some point to to some profiling and see if       you can optimize your code</li> <li> Use Weights &amp; Biases to log training progress and other important metrics/artifacts in your code. Additionally,       consider running a hyperparameter optimization sweep.</li> <li> Use Pytorch-lightning (if applicable) to reduce the amount of boilerplate in your code</li> </ul>"},{"location":"reports/#week-2","title":"Week 2","text":"<ul> <li> Write unit tests related to the data part of your code</li> <li> Write unit tests related to model construction and or model training</li> <li> Calculate the coverage.</li> <li> Get some continuous integration running on the github repository</li> <li> Create a data storage in GCP Bucket for you data and preferable link this with your data version control setup</li> <li> Create a trigger workflow for automatically building your docker images</li> <li> Get your model training in GCP using either the Engine or Vertex AI</li> <li> Create a FastAPI application that can do inference using your model</li> <li> If applicable, consider deploying the model locally using torchserve</li> <li> Deploy your model in GCP using either Functions or Run as the backend</li> </ul>"},{"location":"reports/#week-3","title":"Week 3","text":"<ul> <li> Check how robust your model is towards data drifting</li> <li> Setup monitoring for the system telemetry of your deployed model</li> <li> Setup monitoring for the performance of your deployed model</li> <li> If applicable, play around with distributed data loading</li> <li> If applicable, play around with distributed model training</li> <li> Play around with quantization, compilation and pruning for you trained models to increase inference speed</li> </ul>"},{"location":"reports/#additional","title":"Additional","text":"<ul> <li> Revisit your initial project description. Did the project turn out as you wanted?</li> <li> Make sure all group members have a understanding about all parts of the project</li> <li> Uploaded all your code to github</li> </ul>"},{"location":"reports/#group-information","title":"Group information","text":""},{"location":"reports/#question-1","title":"Question 1","text":"<p>Enter the group number you signed up on  <p>Answer:</p> <p>--- question 1 fill here ---</p>"},{"location":"reports/#question-2","title":"Question 2","text":"<p>Enter the study number for each member in the group</p> <p>Example:</p> <p>sXXXXXX, sXXXXXX, sXXXXXX</p> <p>Answer:</p> <p>--- question 2 fill here ---</p>"},{"location":"reports/#question-3","title":"Question 3","text":"<p>What framework did you choose to work with and did it help you complete the project?</p> <p>Answer length: 100-200 words.</p> <p>Example: We used the third-party framework ... in our project. We used functionality ... and functionality ... from the package to do ... and ... in our project.</p> <p>Answer:</p> <p>--- question 3 fill here ---</p>"},{"location":"reports/#coding-environment","title":"Coding environment","text":"<p>In the following section we are interested in learning more about you local development environment.</p>"},{"location":"reports/#question-4","title":"Question 4","text":"<p>Explain how you managed dependencies in your project? Explain the process a new team member would have to go through to get an exact copy of your environment.</p> <p>Answer length: 100-200 words</p> <p>Example: We used ... for managing our dependencies. The list of dependencies was auto-generated using ... . To get a complete copy of our development enviroment, one would have to run the following commands</p> <p>Answer:</p> <p>--- question 4 fill here ---</p>"},{"location":"reports/#question-5","title":"Question 5","text":"<p>We expect that you initialized your project using the cookiecutter template. Explain the overall structure of your code. Did you fill out every folder or only a subset?</p> <p>Answer length: 100-200 words</p> <p>Example: From the cookiecutter template we have filled out the ... , ... and ... folder. We have removed the ... folder because we did not use any ... in our project. We have added an ... folder that contains ... for running our experiments. Answer:</p> <p>--- question 5 fill here ---</p>"},{"location":"reports/#question-6","title":"Question 6","text":"<p>Did you implement any rules for code quality and format? Additionally, explain with your own words why these concepts matters in larger projects.</p> <p>Answer length: 50-100 words.</p> <p>Answer:</p> <p>--- question 6 fill here ---</p>"},{"location":"reports/#version-control","title":"Version control","text":"<p>In the following section we are interested in how version control was used in your project during development to corporate and increase the quality of your code.</p>"},{"location":"reports/#question-7","title":"Question 7","text":"<p>How many tests did you implement and what are they testing in your code?</p> <p>Answer length: 50-100 words.</p> <p>Example: In total we have implemented X tests. Primarily we are testing ... and ... as these the most critical parts of our application but also ... .</p> <p>Answer:</p> <p>--- question 7 fill here ---</p>"},{"location":"reports/#question-8","title":"Question 8","text":"<p>What is the total code coverage (in percentage) of your code? If you code had an code coverage of 100% (or close to), would you still trust it to be error free? Explain you reasoning.</p> <p>Answer length: 100-200 words.</p> <p>Example: The total code coverage of code is X%, which includes all our source code. We are far from 100% coverage of our ** code and even if we were then...*</p> <p>Answer:</p> <p>--- question 8 fill here ---</p>"},{"location":"reports/#question-9","title":"Question 9","text":"<p>Did you workflow include using branches and pull requests? If yes, explain how. If not, explain how branches and pull request can help improve version control.</p> <p>Answer length: 100-200 words.</p> <p>Example: We made use of both branches and PRs in our project. In our group, each member had an branch that they worked on in addition to the main branch. To merge code we ...</p> <p>Answer:</p> <p>--- question 9 fill here ---</p>"},{"location":"reports/#question-10","title":"Question 10","text":"<p>Did you use DVC for managing data in your project? If yes, then how did it improve your project to have version control of your data. If no, explain a case where it would be beneficial to have version control of your data.</p> <p>Answer length: 100-200 words.</p> <p>Example: We did make use of DVC in the following way: ... . In the end it helped us in ... for controlling ... part of our pipeline</p> <p>Answer:</p> <p>--- question 10 fill here ---</p>"},{"location":"reports/#question-11","title":"Question 11","text":"<p>Discuss you continues integration setup. What kind of CI are you running (unittesting, linting, etc.)? Do you test multiple operating systems, python version etc. Do you make use of caching? Feel free to insert a link to one of your github actions workflow.</p> <p>Answer length: 200-300 words.</p> <p>Example: We have organized our CI into 3 separate files: one for doing ..., one for running ... testing and one for running ... . In particular for our ..., we used ... .An example of a triggered workflow can be seen here:  <p>Answer:</p> <p>--- question 11 fill here ---</p>"},{"location":"reports/#running-code-and-tracking-experiments","title":"Running code and tracking experiments","text":"<p>In the following section we are interested in learning more about the experimental setup for running your code and especially the reproducibility of your experiments.</p>"},{"location":"reports/#question-12","title":"Question 12","text":"<p>How did you configure experiments? Did you make use of config files? Explain with coding examples of how you would run a experiment.</p> <p>Answer length: 50-100 words.</p> <p>Example: We used a simple argparser, that worked in the following way: python my_script.py --lr 1e-3 --batch_size 25</p> <p>Answer:</p> <p>--- question 12 fill here ---</p>"},{"location":"reports/#question-13","title":"Question 13","text":"<p>Reproducibility of experiments are important. Related to the last question, how did you secure that no information is lost when running experiments and that your experiments are reproducible?</p> <p>Answer length: 100-200 words.</p> <p>Example: We made use of config files. Whenever an experiment is run the following happens: ... . To reproduce an experiment one would have to do ...</p> <p>Answer:</p> <p>--- question 13 fill here ---</p>"},{"location":"reports/#question-14","title":"Question 14","text":"<p>Upload 1 to 3 screenshots that show the experiments that you have done in W&amp;B (or another experiment tracking service of your choice). This may include loss graphs, logged images, hyperparameter sweeps etc. You can take inspiration from this figure. Explain what metrics you are tracking and why they are important.</p> <p>Answer length: 200-300 words + 1 to 3 screenshots.</p> <p>Example: As seen in the first image when have tracked ... and ... which both inform us about ... in our experiments. As seen in the second image we are also tracking ... and ...</p> <p>Answer:</p> <p>--- question 14 fill here ---</p>"},{"location":"reports/#question-15","title":"Question 15","text":"<p>Docker is an important tool for creating containerized applications. Explain how you used docker in your experiments? Include how you would run your docker images and include a link to one of your docker files.</p> <p>Answer length: 100-200 words.</p> <p>Example: For our project we developed several images: one for training, inference and deployment. For example to run the training docker image: <code>docker run trainer:latest lr=1e-3 batch_size=64</code>. Link to docker file:  <p>Answer:</p> <p>--- question 15 fill here ---</p>"},{"location":"reports/#question-16","title":"Question 16","text":"<p>When running into bugs while trying to run your experiments, how did you perform debugging? Additionally, did you try to profile your code or do you think it is already perfect?</p> <p>Answer length: 100-200 words.</p> <p>Example: Debugging method was dependent on group member. Some just used ... and others used ... . We did a single profiling run of our main code at some point that showed ...</p> <p>Answer:</p> <p>--- question 16 fill here ---</p>"},{"location":"reports/#working-in-the-cloud","title":"Working in the cloud","text":"<p>In the following section we would like to know more about your experience when developing in the cloud.</p>"},{"location":"reports/#question-17","title":"Question 17","text":"<p>List all the GCP services that you made use of in your project and shortly explain what each service does?</p> <p>Answer length: 50-200 words.</p> <p>Example: We used the following two services: Engine and Bucket. Engine is used for... and Bucket is used for...</p> <p>Answer:</p> <p>--- question 17 fill here ---</p>"},{"location":"reports/#question-18","title":"Question 18","text":"<p>The backbone of GCP is the Compute engine. Explained how you made use of this service and what type of VMs you used?</p> <p>Answer length: 100-200 words.</p> <p>Example: We used the compute engine to run our ... . We used instances with the following hardware: ... and we started the using a custom container: ...</p> <p>Answer:</p> <p>--- question 18 fill here ---</p>"},{"location":"reports/#question-19","title":"Question 19","text":"<p>Insert 1-2 images of your GCP bucket, such that we can see what data you have stored in it. You can take inspiration from this figure.</p> <p>Answer:</p> <p>--- question 19 fill here ---</p>"},{"location":"reports/#question-20","title":"Question 20","text":"<p>Upload one image of your GCP container registry, such that we can see the different images that you have stored. You can take inspiration from this figure.</p> <p>Answer:</p> <p>--- question 20 fill here ---</p>"},{"location":"reports/#question-21","title":"Question 21","text":"<p>Upload one image of your GCP cloud build history, so we can see the history of the images that have been build in your project. You can take inspiration from this figure.</p> <p>Answer:</p> <p>--- question 21 fill here ---</p>"},{"location":"reports/#question-22","title":"Question 22","text":"<p>Did you manage to deploy your model, either in locally or cloud? If not, describe why. If yes, describe how and preferably how you invoke your deployed service?</p> <p>Answer length: 100-200 words.</p> <p>Example: For deployment we wrapped our model into application using ... . We first tried locally serving the model, which worked. Afterwards we deployed it in the cloud, using ... . To invoke the service an user would call <code>curl -X POST -F \"file=@file.json\"&lt;weburl&gt;</code></p> <p>Answer:</p> <p>--- question 22 fill here ---</p>"},{"location":"reports/#question-23","title":"Question 23","text":"<p>Did you manage to implement monitoring of your deployed model? If yes, explain how it works. If not, explain how monitoring would help the longevity of your application.</p> <p>Answer length: 100-200 words.</p> <p>Example: We did not manage to implement monitoring. We would like to have monitoring implemented such that over time we could measure ... and ... that would inform us about this ... behaviour of our application.</p> <p>Answer:</p> <p>--- question 23 fill here ---</p>"},{"location":"reports/#question-24","title":"Question 24","text":"<p>How many credits did you end up using during the project and what service was most expensive?</p> <p>Answer length: 25-100 words.</p> <p>Example: Group member 1 used ..., Group member 2 used ..., in total ... credits was spend during development. The service costing the most was ... due to ...</p> <p>Answer:</p> <p>--- question 24 fill here ---</p>"},{"location":"reports/#overall-discussion-of-project","title":"Overall discussion of project","text":"<p>In the following section we would like you to think about the general structure of your project.</p>"},{"location":"reports/#question-25","title":"Question 25","text":"<p>Include a figure that describes the overall architecture of your system and what services that you make use of. You can take inspiration from this figure. Additionally in your own words, explain the overall steps in figure.</p> <p>Answer length: 200-400 words</p> <p>Example:</p> <p>The starting point of the diagram is our local setup, where we integrated ... and ... and ... into our code. Whenever we commit code and puch to github, it auto triggers ... and ... . From there the diagram shows ...</p> <p>Answer:</p> <p>--- question 25 fill here ---</p>"},{"location":"reports/#question-26","title":"Question 26","text":"<p>Discuss the overall struggles of the project. Where did you spend most time and what did you do to overcome these challenges?</p> <p>Answer length: 200-400 words.</p> <p>Example: The biggest challenges in the project was using ... tool to do ... . The reason for this was ...</p> <p>Answer:</p> <p>--- question 26 fill here ---</p>"},{"location":"reports/#question-27","title":"Question 27","text":"<p>State the individual contributions of each team member. This is required information from DTU, because we need to make sure all members contributed actively to the project</p> <p>Answer length: 50-200 words.</p> <p>Example: Student sXXXXXX was in charge of developing of setting up the initial cookie cutter project and developing of the docker containers for training our applications. Student sXXXXXX was in charge of training our models in the cloud and deploying them afterwards. All members contributed to code by...</p> <p>Answer:</p> <p>--- question 27 fill here ---</p>"},{"location":"s10_extra/","title":"Extra learning modules","text":"<p>All modules listed here are not part of the core course, but expands on some of the other topics. Some of them may still be under construction and may in the future be moved into other sessions.</p>"},{"location":"s10_extra/cli/","title":"M30 - Command Line Interfaces","text":""},{"location":"s10_extra/cli/#command-line-interfaces","title":"Command line interfaces","text":"<p>If you have worked with python for some time you are probably familiar with the <code>argparse</code> package, which allows you to directly pass in additional arguments to your script in the terminal</p> <pre><code>python my_script.py --arg1 val1 --arg2 val2\n</code></pre> <p><code>argparse</code> is a very simple way of constructing what is called a command line interfaces (CLI). CLI allows you to interact with your application directly in the terminal instead of having change things in your code. It is essentially a text-based user interface (UI) (in contrast to an graphical user interface (GUI) that we know from all our desktop applications).</p> <p>However, one limitation of <code>argparse</code> is the possibility of easily defining an CLI with subcommands. If we take <code>git</code> as an example, <code>git</code> is the main command but it has multiple subcommands: <code>push</code>, <code>pull</code>, <code>commit</code> etc. that all can take their own arguments. This kind of second CLI with subcommands is somewhat possible to do using only <code>argparse</code>, however it requires a bit of hacks.</p> <p>You could of course ask the question why we at all would like to have the possibility of defining such CLI. The main argument here is to give users of our code a single entrypoint to interact with our application instead of having multiple scripts. As long as all subcommands are proper documented, then our interface should be simple to interact with (again think <code>git</code> where each subcommand can be given the <code>-h</code> arg to get specific help).</p> <p>Instead of using <code>argparse</code> we are here going to look at the click package. <code>click</code> extends the functionalities of <code>argparse</code> to allow for easy definition of subcommands and many other things, which we are not going to touch upon in this module. For completeness we should also mention that <code>click</code> is not the only package for doing this, and of other excellent frameworks for creating command line interfaces easily we can mention Typer.</p>"},{"location":"s10_extra/cli/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <ol> <li> <p>Install click</p> <pre><code>pip install click\n</code></pre> </li> <li> <p>Create a new python file <code>greetings.py</code> and add the following code:</p> <pre><code>import click\n@click.command()\n@click.option('--count', default=1, help='Number of greetings.')\n@click.option('--name', prompt='Your name', help='The person to greet.')\ndef hello(count, name):\n\"\"\"Simple program that greets NAME for a total of COUNT times.\"\"\"\nfor x in range(count):\nclick.echo(f\"Hello {name}!\")\nif __name__ == '__main__':\nhello()\n</code></pre> <p>try running the program in the following ways</p> <pre><code>python greetings.py\npython greetings.py --count=3\npython greetings.py --help\n</code></pre> </li> <li> <p>Make sure you understand what the <code>click.command()</code> decorator and <code>click.option</code> decorator does. You can find     the full API docs here.</p> </li> <li> <p>As stated above, the power of using a tool like click is due to its ability to define subcommands. In <code>click</code> this     is done through the <code>click.group()</code> decorator. To the code example from above, add another command:</p> <pre><code>@click.command()\n@click.option('--count', default=1, help='Number of greetings.')\n@click.option('--name', prompt='Your name', help='The person to greet.')\ndef howdy(count, name):\nfor x in range(count):\nclick.echo(f\"Howdy {name}!\")\n</code></pre> <p>and by using the <code>click.group()</code> decorator make these commands into subcommands such that you would be able to call the script in the following way</p> <pre><code>python greetings.py hello\npython greetings.py howdy\n</code></pre> </li> <li> <p>As an final exercise we provide you with a script that is ready to run as it is, but your job will be do turn it     into a script with multiple subcommands, with multiple arguments for each subcommand.</p> <ol> <li> <p>Start by taking a look at the provided     code. It is a simple     script that runs the K-nearest neighbour classification algorithm on the iris dataset and produces a plot of     the decision boundary.</p> </li> <li> <p>Create a script that has the following subcommands with input arguments</p> <ul> <li>Subcommand <code>train</code>: Load data, train model and save. Should take a single argument <code>-o</code> that specifics     the filename the trained model should be saved to.</li> <li>Subcommand <code>infer</code>: Load trained model and runs prediction on input data. Should take two arguments: <code>-i</code> that     specifies which trained model to load and <code>-d</code> to specify a user defined datapoint to run inference on.</li> <li>Subcommand <code>plot</code>: Load trained model and constructs the decision boundary plot from the code. Should take two     arguments: <code>-i</code> that specifies a trained model to load and <code>-o</code> the file to write the generated plot to</li> <li>Subcommand <code>optim</code>: Load data, runs hyperparameter optimization and prints optimal parameters. Should at least     take a single argument that in some way adjust the hyperparameter optimization (free to choose how)</li> </ul> <p>In the end we like the script to be callable in the following ways</p> <pre><code>python main.py train -o 'model.ckpt'\npython main.py infer -i 'model.ckpt' -d [[0,1]]\npython main.py plot -i 'model.ckpt' -o 'generated_plot.png'\npython main.py optim\n</code></pre> </li> </ol> </li> </ol>"},{"location":"s10_extra/design/","title":"Designing MLOps pipelines","text":"<p>Danger</p> <p>Module is still under development</p> <p>\"Machine learning engineering is 10% machine learning and 90% engineering.\" - Chip Huyen</p> <p>We highly recommend that you read the book Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications by Chip Huyen which gives an fantastic overview of the thought processes that goes into designing moder machine learning systems.</p>"},{"location":"s10_extra/design/#the-stack","title":"The stack","text":"<p>Have you ever encountered the concept of full stack developer. A full stack developer is an developer who can both develop client and server software or in more general terms, it is a developer who can take care of the complete developer pipeline.</p> <p>Below is seen an image of the massive amounts of tools that exist within the MLOps umbrella. </p>"},{"location":"s10_extra/design/#visualizing-the-design","title":"Visualizing the design","text":""},{"location":"s10_extra/frontend/","title":"Frontend","text":"<p>Danger</p> <p>Module is still under development</p>"},{"location":"s10_extra/frontend/#streamlit","title":"Streamlit","text":"<p><code>steamlit</code></p>"},{"location":"s10_extra/frontend/#exercises","title":"\u2754 Exercises","text":"<ol> <li>Start by installing <code>streamlit</code></li> </ol> <pre><code>pip install streamlit\n</code></pre> <p>and run <code>streamlit hallo</code> afterwards to check that everything works as expected.</p>"},{"location":"s10_extra/high_performance_clusters/","title":"M32 - High Performance Clusters","text":""},{"location":"s10_extra/high_performance_clusters/#high-performance-clusters","title":"High Performance Clusters","text":"<p>As discussed in the intro session on the cloud, cloud providers offers near infinite compute resources. However, using these resources comes at a hefty price often and it is therefore important to be aware of another resource many have access to: High Performance Clusters or HPC. HPCs exist all over the world, and many time you already have access to one or can easily get access to one. If you are an university student you most likely have a local HPC that you can access through your institution. Else, there exist public HPC resources that everybody (with a project) can apply for. As an example in the EU we have EuroHPC initiative that currently has 8 different supercomputers with a centralized location for applying for resources that are both open for research projects and start-ups.</p> <p>Depending on your application, you may have different needs and it is therefore important to be aware also of the different tiers of HPC. In Europe, HPC are often categorized such that Tier-0 are European Centers with petaflop or hexascale machines,\u00a0Tier 1 are National centers of supercomputers, and Tier 2 are Regional centers. The lower the Tier, the larger applications it is possible to run.</p> <p> </p>  Image credit"},{"location":"s10_extra/high_performance_clusters/#cluster-architectures","title":"Cluster architectures","text":"<p>In very general terms, cluster can come as two different kind of systems: supercomputers and LSF (Load Sharing Facility). A supercomputer (as shown below) is organized into different modules, that are seperated by network link. When you login to a supercomputer you will meet the front end which contains all the software needed to run computations. When you submit a job it will get sent to the backend modules which in most cases includes: general compute modules (CPU), acceleration modules (GPU), a memory module (RAM) and finally a storage module (HDD). Depending on your application you may need one module more than another. For example in deep learning the acceleration module is important but in physics simulation the general compute module / storage model is probably more important.</p> <p> </p>  Overview of the Meluxina supercomputer thats part of EuroHPC.    Image credit  <p>Alternatively, LSF are a network of computers where each computer has its own CPU, GPU, RAM etc. and the individual computes (or nodes) are then connected by network. The important different between a supercomputer and as LSF systems is how the resources are organized. When comparing supercomputers to LSF system it is generally the case that it is better to run on a LSF system if you are only requesting resources that can be handled by a single node, however it is better to run on a supercomputer if you have a resource intensive application that requires many devices to communicate with each others.</p> <p>Regardless of cluster architechtures, on the software side of HPC, the most important part is whats called the HPC scheduler. Without a HPC scheduler an HPC cluster would just be a bunch of servers with different jobs interfering with each other. The problem is when you have a large collection of resources and a large collection of users, you cannot rely on the users just running their applications without interfering with each other. A HPC scheduler is in charge of managing that whenever an user request to run an application, they get put in a queue and whenever the resources their application ask for are available the application gets run.</p> <p>The biggest bach control systems for doing scheduling on HPC are:</p> <ul> <li>SLURM</li> <li>MOAB HPC Suite</li> <li>PBS Works</li> </ul> <p>We are going to take a look at PBS works as that is what is installed on our local university cluster.</p>"},{"location":"s10_extra/high_performance_clusters/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>The following exercises are focused on local students at DTU that want to use our local HPC resources. That said, the steps in the exercise are fairly general to other types of cluster. For the purpose of this exercise we are going to see how we can run this image classifier script , but feel free to work with whatever application you want to.</p> <ol> <li> <p>Start by accessing the cluster. This can either be through <code>ssh</code> in a terminal or if you want a graphical interface     thinlinc can be installed. In general we recommend following the steps     here for DTU students as the setup depends on if you are on campus or not.</p> </li> <li> <p>When you have access to the cluster we are going to start with the setup phase. In the setup phase we are going     to setup the environment necessary for our computations. If you have accessed the cluster through graphical interface     start by opening a terminal.</p> </li> <li> <p>Lets start by setting up conda for controlling our dependencies. If you have not already worked with <code>conda</code>,         please checkout module M2 on conda. In general you should be able to         setup (mini)conda through these two commands:</p> <pre><code>```bash\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\n```\n</code></pre> </li> <li> <p>Close the terminal and open a new for the installation to complete. Type <code>conda</code> in the terminal to check that         everything is fine. Go ahead and create a new environment that we can install dependencies in</p> <pre><code>```bash\nconda create -n \"hpc_env\" python=3.10 --no-default-packages\n```\n\nand activate it.\n</code></pre> </li> <li> <p>Copy over any files you need. For the image classifier script you need the         requirements file         and the actual         application.</p> </li> <li> <p>Next, install all the requirements you need. If you want to run the image classifier script you can run this         command in the terminal</p> <pre><code>```bash\npip install -r image_classifier_requirements.txt\n```\n\nusing this [requirements file](https://github.com/SkafteNicki/dtu_mlops/tree/main/s10_extra/exercise_files/image_classifier_requirements.txt).\n</code></pre> </li> <li> <p>Thats all the setup needed. You would need to go through the creating of environment and installation of requirements     whenever you start a new project (no need for reinstalling conda). For the next step we need to look at how to submit     jobs on the cluster. We are now ready to submit the our first job to the cluster:</p> </li> <li> <p>Start by checking the statistics for the different clusters. Try to use both the <code>qstat</code> command which should give         an overview of the different cluster, number of running jobs and number of pending jobs. For many system you can         also try the much more user friendly command <code>classstat</code> command.</p> </li> <li> <p>Figure out which queue you want to use. For the sake of the exercises it needs to be one with GPU support. For         DTU students, any queue that starts with <code>gpu</code> are GPU accelerated.</p> </li> <li> <p>Now we are going to develop a bash script for submitting our job. We have provided an example of such         scripts. Take a         careful look and go each line and make sure you understand it. Afterwards, change it to your needs         (queue and student email).</p> </li> <li> <p>Try to submit the script:</p> <pre><code>```bash\nbsub &lt; jobscript.sh\n```\n\nYou can check the status of your script by running the `bstat` command. Hopefully, the job should go trough\nreally quickly. Take a look at the output file, it should be called something like `gpu_*.out`. Also take a\nlook at the `gpu_*.err` file. Does both files look as they should?\n</code></pre> </li> <li> <p>Lets now try to run our application on the cluster. To do that we need to take care of two things:</p> <ol> <li> <p>First we need to load the correct version of CUDA. A cluster system often contains multiple versions of specific     software to suit the needs of all their users, and it is the users that are in charge of loading the correct     software during job submission. The only extra software that needs to be loaded for most Pytorch applications     are a CUDA module. You can check which modules are available on the cluster with</p> <pre><code>module avail\n</code></pre> <p>Afterwards, add the correct CUDA version you need to the <code>jobscript.sh</code> file. If you are trying to run the provided image classifier script then the correct version is <code>CUDA/11.7</code> (can be seen in the requirements file).</p> <pre><code># add to the bottom of the file\nmodule load cuda/11.7\n</code></pre> </li> <li> <p>We are now ready to add in our application. The only thing we need to take care of is telling the system to run     it using the <code>python</code> version that is connected to our <code>hpc_env</code> we created in the beginning. Try typing:</p> <pre><code>which python\n</code></pre> <p>which should give you the full path. Then add to the bottom of the <code>jobscript</code> file:</p> <pre><code>~/miniconda3/envs/hpc_env/bin/python \\\nimage_classifier.py \\\n--trainer.accelerator 'gpu' --trainer.devices 1  --trainer.max_epochs 5\n</code></pre> <p>which will run the image classifier script (change it if you are runnning something else).</p> </li> <li> <p>Finally submit the job:</p> <pre><code>bsub &lt; jobscript.sh\n</code></pre> <p>and check when it is done that it has produced what you expected.</p> </li> <li> <p>(Optional) If you application supports multi GPUs also try that out. You would first need to change the     jobscript to request multiple GPUs and additionally you would need to tell your application to run on multiple     GPUs. For the image classifier script it can be done by changing the <code>--trainer.devices</code> flag     to <code>2</code> (or higher).</p> </li> </ol> </li> </ol> <p>This ends the module on using HPC systems.</p>"},{"location":"s10_extra/hyperparameters/","title":"M31 - Hyperparameter optimization","text":""},{"location":"s10_extra/hyperparameters/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>Hyperparameter optimization is not a new idea within machine learning but have somewhat seen a renaissance with the uprise of deep learning. This can mainly be contributed to the following:</p> <ul> <li>Trying to beat state-of-the-art often comes down to very small differences in performance, and hyperparameter   optimization can help squeeze out a bit more</li> <li>Deep learning models are in general not that robust towards the choice of hyparameter so choosing the wrong set   may lead to a model that does not work</li> </ul> <p>However the problem with doing hyperparameter optimization of a deep learning models is that it can take over a week to train a single model. In most cases we therefore cannot do a full grid search of all hyperparameter combinations to get the best model. Instead we have to do some tricks that will help us speed up our searching. In these exercises we are going to be integrating optuna into our different models, that will provide the tools for speeding up our search.</p> <p></p> <p>It should be noted that a lot of deep learning models does not optimize every hyperparameter that is included in the model but instead relies on heuristic guidelines (\"rule of thumb\") based on what seems to be working in general e.g. a learning rate of 0.01 seems to work great with the Adam optimizer. That said, these rules probably only apply to 80% of deep learning model, whereas for the last 20% the recommendations may be suboptimal Here is a great site that has collected an extensive list of these recommendations, taken from the excellent deep learning book by Ian Goodfellow, Yoshua Bengio and Aaron Courville.</p> <p>In practice, I recommend trying to identify (through experimentation) which hyperparameters that are important for the performance of your model and then spend your computational budget trying to optimize them while setting the rest to a \"recommended value\".</p>"},{"location":"s10_extra/hyperparameters/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <ol> <li> <p>Start by installing optuna:     <code>pip install optuna</code></p> </li> <li> <p>Initially we will look at the <code>cross_validate.py</code> file. It implements simple K-fold cross validation of     a random forest sklearn digits dataset (subset of MNIST). Look over the script and try to run it.</p> </li> <li> <p>We will now try to write the same code in optune. Please note that the script have a variable <code>OPTUNA=False</code>     that you can use to change what part of the code should run. The three main concepts of optuna is</p> <ul> <li> <p>A trial: a single experiment</p> </li> <li> <p>A study: a collection of trials</p> </li> <li> <p>The objective: function to determine how \"good\" a trial is</p> </li> </ul> <p>Lets start by writing the objective function, which we have already started in the script. For now you do not need to care about the <code>trial</code> argument, just assume that it contains the hyperparameters needed to define your random forest model. The output of the objective function should be a single number that we want to optimize. (HINT: did you remember to do K-fold crossvalidation inside your objective function?)</p> </li> <li> <p>Next lets focus on the trial. Inside the <code>objective</code> function the trial should be used to suggest what     parameters to use next. Take a look at the documentation for     trial or take a look at     the code examples and figure out how to define the hyperparameter of the model.</p> </li> <li> <p>Finally lets launch a study. It can be as simple as</p> <pre><code>study = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n</code></pre> <p>but lets play around a bit with it:</p> <ol> <li> <p>By default the <code>.optimize</code> method will minimize the objective (by definition the optimum of an objective     function is at its minimum). Is the score your objective function is returning something that should     be minimized? If not, a simple solution is to put a <code>-</code> in front of the metric. However, look through     the documentation on how to change the direction of the optimization.</p> </li> <li> <p>Optuna will by default do Bayesian optimization when sampling the hyperparameters (using a evolutionary     algorithm for suggesting new trials). However, since this example is quite simple, we can actually     perform a full grid search. How would you do this in Optuna?</p> </li> <li> <p>Compare the performance of a single optuna run using Bayesian optimization with <code>n_trials=10</code> with a     exhaustive grid search that have search through all hyperparameters. What is the performance/time     trade-off for these two solutions?</p> </li> </ol> </li> <li> <p>In addition to doing baysian optimization, the other great part about Optuna is that it have native support     for Pruning unpromising trials. Pruning refers to the user stopping trials for hyperparameter combinations     that does not seem to lead anywhere. You may have learning rate that is so high that training is diverging or     a neural network with too many parameters so it is just overfitting to the training data. This however begs the     question: what constitutes an unpromising trial? This is up to you to define based on prior experimentation.</p> <ol> <li> <p>Start by looking at the <code>fashion_trainer.py</code> script. Its a simple classification network for classifying     images in the FashionMNIST dataset. Run the script     with the default hyperparameters to get a feeling of how the training should be progress.     Note down the performance on the test set.</p> </li> <li> <p>Start by defining a validation set and a validation dataloader that we can use for hyperparameter optimization     (HINT: use 5-10% of you training data).</p> </li> <li> <p>Now, adjust the script to use Optuna. The 5 hyperparameters listed in the table above should at least be included     in the hyperparameter search. For some we have already defined the search space but for the remaining you need to     come up with a good range of values to investigate. We done integrating optuna, run a small study (<code>n_tirals=3</code>)     to check that the code is working.</p> Hyperparameter Search space Learning rate 1e-6 to 1e0 Number of output features in the second last layer ??? The amount of dropout to apply ??? Batch size ??? Use batch normalize or not {True, False} (Optional) Different activations functions {<code>nn.ReLU</code>, <code>nn.Tanh</code>, <code>nn.RReLU</code>, <code>nn.LeakyReLU</code>, <code>nn.ELU</code>} </li> <li> <p>If implemented correctly the number of hyperparameter combinations should be at least 1000, meaning that     we not only need baysian optimization but probably also need pruning to succeed. Checkout the page for     build-in pruners in Optuna. Implement     pruning in the script. I recommend using either the <code>MedianPruner</code> or the <code>ProcentilePruner</code>.</p> </li> <li> <p>Re-run the study using pruning with a large number of trials (<code>n_trials&gt;50</code>)</p> </li> <li> <p>Take a look at this     visualization page     for ideas on how to visualize the study you just did. Make at least two visualization of the study and     make sure that you understand them.</p> </li> <li> <p>Pruning is great for better spending your computational budged, however it comes with a trade-off. What is     it and what hyperparameter should one be especially careful about when using pruning?</p> </li> <li> <p>Finally, what parameter combination achieved the best performance? What is the test set performance for this     set of parameters. Did you improve over the initial set of hyperparameters?</p> </li> </ol> </li> <li> <p>The exercises until now have focused on doing the hyperparameter searching sequentially, meaning that we test one     set of parameters at the time. It is a fine approach because you can easily let it run for a week without any     interaction. However, assuming that you have the computational resources to run in parallel, how do you do that?</p> <ol> <li> <p>To run hyperparameter search in parallel we need a common database that all experiments can read and     write to. We are going to use the recommended <code>mysql</code>. You do not have to understand what SQL is to     complete this exercise, but it is basically a language (like python)     for managing databases. Install mysql.</p> </li> <li> <p>Next we are going to initialize a database that we can read and write to. For this exercises we are going     to focus on a locally stored database but it could of course also be located in the cloud.</p> <pre><code>mysql -u root -e \"CREATE DATABASE IF NOT EXISTS example\"\n</code></pre> <p>you can also do this directly in python when calling the <code>create_study</code> command by also setting the <code>storage</code> and <code>load_if_exists=True</code> flags.</p> </li> <li> <p>Now we are going to create a Optuna study in our database</p> <pre><code>optuna create-study --study-name \"distributed-example\" --storage \"mysql://root@localhost/example\"\n</code></pre> </li> <li> <p>Change how you initialize the study to read and write to the database. Therefore, instead of doing</p> <pre><code>study = optuna.create_study()\n</code></pre> <p>then do</p> <pre><code>study = optuna.load_study(\nstudy_name=\"distributed-example\", storage=\"mysql://root@localhost/example\"\n)\n</code></pre> <p>where the <code>study_name</code> and <code>storage</code> should match how the study was created.</p> </li> <li> <p>For running in parallel, you can either open up a extra terminal and simple launch your script once     per open terminal or you can use the provided <code>parallel_lancher.py</code> that will launch multiple executions     of your script. It should be used as:</p> <pre><code>python parallel_lancher.py myscript.py --num_parallel 2\n</code></pre> </li> <li> <p>Finally, make sure that you can access the results</p> </li> </ol> </li> </ol> <p>Thats all on how to do hyperparameter optimization in a scalable way. If you feel like it you can try to apply these techniques on the ongoing corrupted MNIST example, where you are free to choose what hyperparameters that you want to use.</p>"},{"location":"s10_extra/kubernetes/","title":"Kubernetes","text":""},{"location":"s10_extra/kubernetes/#kubernetes","title":"Kubernetes","text":"<p>Danger</p> <p>Module is still under development</p>"},{"location":"s10_extra/kubernetes/#kubernetes-architechture","title":"Kubernetes architechture","text":"Image credit"},{"location":"s10_extra/kubernetes/#minikube","title":"Minikube","text":""},{"location":"s10_extra/kubernetes/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Install minikube</p> </li> <li> <p>Make sure that minikube is correctly installed by typing</p> <pre><code>minikube\n</code></pre> <p>in a terminal. Additionally, also check that kubectl (the command line tool for kubernetes, its a dependency of minikube) is correctly installed by typing</p> <pre><code>kubectl\n</code></pre> <p>in a terminal.</p> </li> </ol>"},{"location":"s10_extra/kubernetes/#yatai","title":"Yatai","text":"<p>yatai</p>"},{"location":"s10_extra/onnx/","title":"Onnx","text":""},{"location":"s10_extra/onnx/#onnx","title":"Onnx","text":"<p>Danger</p> <p>Module is still under development</p>"},{"location":"s10_extra/onnx/#model-packaging","title":"Model packaging","text":"<p>Whenever we want to serve an machine learning model, what we are actually interested in is doing predictions e.g. given a new datapoint we pass it through our model (forward pass) and the returned value is the predicted value of that datapoint. At a high-level, model predictions depends on three things:</p> <ul> <li>The codebase that implements the models prediction method</li> <li>The model weights which contains an actual instance of the model</li> <li>Code dependencies nessesary for running the codebase.</li> </ul> <p>We have already in module M9 on Docker touch on how to take care of all these things. Containers makes it easy to link a codebase, model weights and code dependencies into a single object. We in general can refer to this as model packaging, because as the name suggest, we are packaging our model into a format that is independent of the actual environment that we are trying to run the model in.</p> <p>However, containers is not the only way to do model packaging. If we put some light restrictions on the device we want run our model predictions on, we can achieve the same result using ONNX. The Open Neural Network Exchange (ONNX) is a standardized format for creating and sharing machine learning models. ONNX provides an open source format for machine learning models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types.</p> <p></p>  Image credit  <p>As the above image indicates, the idea behind ONNX is that a model trained with a specific framework on a specific device, lets say Pytorch on your local computer, can be exported and run with an entirely different framework and hardware easily. For example, not all frameworks are created equally. For example Pytorch is in general considered an developer friendly framework, however it has historically been slow to run inference with compared to a framework such as Caffe2. ONNX allow you to mix-and-match frameworks based on different usecases, and essentially increases the longivity of your model.</p>"},{"location":"s10_extra/onnx/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by installing ONNX:</p> <pre><code>pip install onnx\npip install onnxruntime\n</code></pre> <p>the first package includes the basic building blocks for implementing generalized ONNX models and the second package is for running ONNX optimal on different hardware.</p> </li> <li> <p>As an test that your installation is working, try executing the following python code</p> <pre><code>import onnxruntime\nonnxruntime.get_all_providers()\n</code></pre> <p>these providers are translation layers that are implemented ONNX, such that the same ONNX model can run on completely different hardware. Can you identify at least two of the providers that are necessary for running standard Pytorch code on CPU and GPU? Can you identify others</p> </li> <li> <p>One big advantage of having a standardized format, is that we can easily visualize the computational graph of our    model because it consist only of core ONNX operations. We are here going to use the open-source tool    netron for visualization. You can either choose to download the program    or just run it in your webbrowser.</p> </li> </ol>"},{"location":"s10_extra/pipeline/","title":"Pipelines and workflows","text":"<p>Danger</p> <p>Module is still under development</p> <p></p>  Image credit"},{"location":"s10_extra/pipeline/#dags","title":"DAGs","text":"<p>Directed Acyclic Graph (DAG)</p>"},{"location":"s10_extra/pipeline/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by installing <code>prefect</code>:</p> <pre><code>pip install prefect\n</code></pre> </li> </ol>"},{"location":"s1_development_environment/","title":"Getting started - Setting up a development environment","text":"<p>Slides</p> <p> </p> <p>Today we start our journey into the world of machine learning operations (MLOps). However, before we can really get started, we need to make sure that you have a basic understanding of a couple of topics, as we will be using these throughout the course. In particular, today is all about getting set up with a proper development environment that can support your journey. Most of you probably already have experience with these topics, and it will be mostly repetition.</p> <p>The reason we are starting here is that many students are missing very basic skills that are never taught but are just expected to be picked up by yourself. This session will only cover the most basic skills to get you started on your development journey. If you wish to learn more about basic computer science skills in general, we highly recommend that you check out The Missing Semester of Your CS Education course from MIT.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the basics of the command line</li> <li>Being able to create reproducible virtual environments</li> <li>Can use a modern IDE / editor for code development</li> <li>Can write and run a Python program implementing a simple deep learning model</li> </ul>"},{"location":"s1_development_environment/command_line/","title":"M1 - The command line","text":""},{"location":"s1_development_environment/command_line/#the-command-line","title":"The command line","text":"<p>Core Module</p> <p> </p>  Image credit  <p>Contrary to popular belief, the command line (also commonly known as the terminal) is not a mythical being that has existed since the dawn of time. Instead, it was created at a time when it was not given that your computer had a graphical interface that you could interact with. Think of it as a text interface to your computer.</p> <p>It is a well-known concept to users of linux, however MAC and (especially) Windows users not so much. Having a basic understanding of how to use a command line can really help improve your workflow. The reason that the command line is an important tool to get to know, is that doing any kind of MLOps will require us to be able to interact with many different tools, many which does not have a graphical interface. Additionally, when we get to working in the cloud later in the course, you will be forced to interact with the command line.</p> <p>Note if you already are a terminal wizard then feel free to skip the exercises below. They are very elementary.</p>"},{"location":"s1_development_environment/command_line/#the-anatomy-of-the-command-line","title":"The anatomy of the command line","text":"<p>Regardless of operating system, all command lines looks more or less the same:</p> <p></p> <p>As already stated, it is essentially just a big text interface to interact with your computer. As the image illustrates, when trying to execute a command, there are several parts to it:</p> <ol> <li>The prompt is the part where you type your commands. It usually contains the name of the current directory you     are in, followed by some kind of sign: <code>$</code>, <code>&gt;</code>, <code>:</code> are the usual onces. It can also contain other information,     such as in the case of the above image it is also showing the current <code>conda</code> environment.</li> <li>The command is the actual command you want to execute. For example, <code>ls</code> or <code>cd</code></li> <li>The options are additional arguments that you can pass to the command. For example, <code>ls -l</code> or <code>cd ..</code>.</li> <li>The arguments are the actual arguments that you pass to the command. For example, <code>ls -l figures</code> or <code>cd ..</code>.</li> </ol> <p>The core difference between options and arguments is that options are optional, while arguments are not.</p> <p></p>  Image credit"},{"location":"s1_development_environment/command_line/#exercises","title":"\u2754 Exercises","text":"<p>We have put a cheat sheet in the exercise files folder belonging to this session, that gives a quick overview of the different commands that can be executed in the command line.</p> Windows users <p>We highly recommend that you install Windows Subsystem for Linux (WSL). This will install a full Linux system on your Windows machine. Please follow this guide. Remember to run commands from an elevated (as administrator) Windows Command Prompt. You can in general complete all exercises in the course from a normal Windows Command Prompt, but some are easier to do if you run from WSL.</p> <p>If you decide to run in WSL you need to remember that you now have two different systems, and install a package on one system does not mean that it is installed on the other. For example, if you install <code>pip</code> in WSL, you need to install it again in Windows if you want to use it there.</p> <p>If you decide to not run in WSL, please always work in a Windows Command Prompt and not Powershell.</p> <ol> <li> <p>Start by opening a terminal.</p> </li> <li> <p>To navigate inside a terminal, we rely on the <code>cd</code> command and <code>pwd</code> command. Make sure you know how to go back and     forth in your file system. HINT: try tab-completion to     save some time.</p> </li> <li> <p>The <code>ls</code> command is important when we want to know the content of a folder. Try to use the command, and also try     it with the additional option <code>-l</code>. What does it show?</p> </li> <li> <p>Make sure to familiar yourself with the <code>which</code>, <code>echo</code>, <code>cat</code>, <code>wget</code>, <code>less</code> and <code>top</code> commands. Also familiarize     yourself with the <code>&gt;</code> operator. You are probably going to use some of them throughout the course or in your future     career. For Windows users these commands may be named something else, e.g. <code>where</code> command on Windows corresponds     to <code>which</code>.</p> </li> <li> <p>It is also significant that you know how to edit a file through the terminal. Most systems should have the     <code>nano</code> editor installed, else try to figure out which one is installed in your system.</p> <ol> <li> <p>Type <code>nano</code> in the terminal</p> </li> <li> <p>Write the following text in the script</p> <pre><code>if __name__ == \"__main__\":\nprint(\"Hello world!\")\n</code></pre> </li> <li> <p>Save the script and try to execute it</p> </li> <li> <p>Afterward, try to edit the file through the terminal (change <code>Hello world</code> to something else)</p> </li> </ol> </li> <li> <p>All terminals come with their own programming language. The most common system is called <code>bash</code>. It can come in handy     being able to write simple programs in bash. For example, one case is that you want to execute multiple python     programs sequentially, which can be done through a bash script.</p> <ol> <li> <p>Write a bash script (in <code>nano</code>) and try executing it:</p> <pre><code>#!/bin/bash\n# A sample Bash script, by Ryan\necho Hello World!\n</code></pre> </li> <li> <p>Change the bash script to call your python program you just wrote.</p> </li> <li> <p>Try to Google how to write a simple for-loop that executes the python script 10 times in a row.</p> </li> </ol> </li> </ol>"},{"location":"s1_development_environment/command_line/#knowledge-check","title":"Knowledge check","text":"Knowledge question 1 <p>Here is one command from later in the course when we are going to work in the cloud</p> <pre><code>gcloud compute instances create-with-container instance-1 \\\n--container-image=gcr.io/&lt;project-id&gt;/gcp_vm_tester\n    --zone=europe-west1-b\n</code></pre> <p>Identify the command, options and arguments.</p> Solution <ul> <li>The command is <code>gcloud compute instances create-with-container</code>.</li> <li>The options are <code>--container-image=gcr.io/&lt;project-id&gt;/gcp_vm_tester</code> and <code>--zone=europe-west1-b</code>.</li> <li>The arguments are <code>instance-1</code>.</li> </ul> <p>The tricky part of this example is that commands can have subcommands, which are also commands. In this case <code>compute</code> is a subcommand to <code>gcloud</code>, <code>instances</code> is a subcommand to <code>compute</code> and <code>create-with-contrainer</code> is a subcommand to <code>instances</code></p> Knowledge question 2 <p>Two common arguments that nearly all commands have is the <code>-h</code> and <code>-v</code> options. What does each of them do?</p> Solution <p>The <code>-h</code> (or <code>--help</code>) option prints the help message for the command, including subcommands and arguments. Try it out by executing <code>python -h</code>.   The <code>-v</code> (or <code>--version</code>) option prints the version of the installed program. Try it out by executing <code>python --version</code>.</p> <p>This ends the module on the command line. If you are still not comfortable working with the command line, fear not as we are going to use it extensively throughout the course. If you want to spend additional time on this topic, we highly recommend that you watch this video on how to use the command line.</p> <p>If you are interested in personalizing your terminal, you can check out the starship project, that allows you to customize your terminal with a lot of different options.</p>"},{"location":"s1_development_environment/conda/","title":"M2 - Conda","text":""},{"location":"s1_development_environment/conda/#conda-and-virtual-enviroments","title":"Conda and virtual enviroments","text":"<p>Core Module</p> <p>You probably already have conda installed on your laptop, which is great. Conda is an environment manager that helps you make sure that the dependencies of different projects do not cross-contaminate each other. However, one thing is having conda installed, another is to actually use it.</p> <p>Before we get on with the exercises, it is probably important to mention the differences between <code>pip</code> and <code>conda</code>. Here is a great summary but it essentially boils down to this:</p> <ul> <li><code>pip</code> always install python packages (in the form of python wheels and distributions) whereas <code>conda</code> can   also install packages written in other languages because it installs from a binary file.</li> <li><code>pip</code> installs dependencies in a serialized-recursive way, meaning that it can lead to dependencies issues,   because all other dependencies are ignored when we are installing the first and so on. On the other hand, <code>conda</code>   go over all dependencies in the beginning checking for compatibility before installing anything.</li> <li><code>pip</code> is bound to a specific python version, whereas <code>conda</code> can manage multiple python versions at the same time</li> </ul> <p>It is therefore highly recommended to use conda environments compared to python virtual environments. However, does that mean that you cannot mix <code>pip install</code> and <code>conda install</code>? If you are using <code>conda&gt;=4.6</code> then you should be fairly safe, because it has build in compatible layer. In general, what works for me</p> <ul> <li>Use <code>conda</code> to create environments</li> <li>Use <code>pip</code> to install packages in that environment (with a few exceptions like <code>pytorch</code>)</li> </ul> <p>Finally, it is worth mentioning that <code>pip</code> and <code>conda</code> are not the only two environment managers that exist for Python. Pipenv is another alternative that is often used that has its own pros and cons compared to other environment managers.</p>"},{"location":"s1_development_environment/conda/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Download and install <code>conda</code>. You are free to either install full <code>conda</code> or the much simpler version <code>miniconda</code>.     The core difference between the two packages is that <code>conda</code> already comes with a lot of packages that you would     normally have to install with <code>miniconda</code>. The downside is that <code>conda</code> is an much larger package which can be a     huge disadvantage on smaller devices. Make sure that your installation is working by writing <code>conda help</code> in a     terminal and it should show you the help message for conda. If this does not work you probably need to set some     system variable to     point to the conda installation</p> </li> <li> <p>Create a new conda environment for the remaining of the exercises using</p> <pre><code>conda create -n \"my_environment\"\n</code></pre> <p>We really recommend that you use multiple conda environment during the course to make sure you do not mix dependencies between your exercises.</p> </li> <li> <p>When you create an environment with <code>conda</code>, how do you specify which python version it should be using?</p> Use python 3.7 or higher <p>We highly recommend that you use python 3.7 or higher for this course. In general, we recommend that you use the second latest version of python that is available (currently python 3.10 as of writing this). This is because many the latest version of python is often not supported by all packages. You can always check the python version support here.</p> </li> <li> <p>Which <code>conda</code> commando gives you a list of the packages installed in the     current environment (HINT: check the <code>conda_cheatsheet.pdf</code> file in the <code>exercise_files</code> folder).</p> <ol> <li> <p>How do you easily export this list to a text file? Do this, and make sure you export it to     a file called <code>enviroment.txt</code>.</p> </li> <li> <p>Finally, inspect the file to see what is in it.</p> </li> <li> <p>The <code>enviroment.txt</code> file you have created is one way to secure reproducibility between users, because     anyone should be able to get an exact copy of you environment if they have your <code>enviroment.txt</code> file.     Try creating a new environment directly from you <code>enviroment.txt</code> file and check that the packages being     installed exactly matches what you originally had.</p> </li> </ol> </li> <li> <p>Which <code>conda</code> commando gives you a list of all the environments that you have created?</p> </li> <li> <p>As the introduction states, it is fairly safe to use <code>pip</code> inside <code>conda</code> today.     What is the corresponding <code>pip</code> command that gives you a list of all <code>pip</code> installed packages?     and how to you export this to a file called <code>requirements.txt</code>?     (We will revisit requirement files at a later point)</p> </li> <li> <p>If you look through the requirements that both <code>pip</code> and <code>conda</code> produces then you will see that it     is often filled with a lot more packages than what you are actually using in your project. What you are     really interested in are the packages that you import in your code: <code>from package import module</code>.     One way to come around this is to use the package <code>pipreqs</code>, that will automatically scan your project     and create a requirement file specific to that.     Lets try it out:</p> <ol> <li> <p>Install <code>pipreqs</code>:</p> <pre><code>pip install pipreqs\n</code></pre> </li> <li> <p>Either try out <code>pipreqs</code> on one of your own projects or try it out on some other online project.     What does the <code>requirements.txt</code> file <code>pipreqs</code> produce look like compared to the files produces     by either <code>pip</code> or <code>conda</code>.</p> </li> </ol> </li> </ol> <p>This ends the module on setting up virtual environments. While the methods mentioned in the exercises are great ways to construct requirements files automatic, sometimes it is just easier to manually sit down and create the files as you in that way secure that only the most necessary requirements are actually installed when creating a new environment.</p>"},{"location":"s1_development_environment/deep_learning_software/","title":"M4 - Deep Learning Software","text":""},{"location":"s1_development_environment/deep_learning_software/#deep-learning-software","title":"Deep Learning Software","text":"<p>Core Module</p> <p>Deep learning have since its revolution back in 2012, transformed our lives. From Google Translate to driverless cars to personal assistants to protein engineering, deep learning is transforming nearly every sector of our economy and or lives. However, it did not take long before people realized that deep learning is not as simple beast to tame and it comes with its own kind of problems, especially if you want to use it in a production setting. In particular the concept of technical debt was invented to indicate the significant maintenance costs at an system level that it takes to run machine learning in production. MLOps should very much be seen as the response to the concept of technical debt, namely that we should develop methods, processes and tools (with inspiration from classical DevOps) to counter the problems we run into when working with deep learning models.</p> <p>It is important to note that all the concepts and tools that have been developed for MLOps can absolutely be used together with more classical machine learning models (think K-nearest neighbor, Random forest etc.), however deep learning comes with its own set of problems which mostly have to do with the sheer size of the data and models we are working with. For these reason, we are focusing on working with deep learning models in this course</p>"},{"location":"s1_development_environment/deep_learning_software/#software-landscape-for-deep-learning","title":"Software landscape for Deep Learning","text":"<p>Regarding software for Deep Learning, the landscape is currently dominated by three software frameworks (listed in order of when they were published):</p> <p> </p> <ul> <li> <p>Tensorflow</p> </li> <li> <p>Pytorch</p> </li> <li> <p>JAX</p> </li> </ul> <p>We won't go into a longer discussion on what framework is the best, as it is pointless. Pytorch and Tensorflow have been around for the longest and therefore have bigger communities and feature sets at this point in time. They both very similar in the sense that they both have features directed against research and production. JAX is kind of the new kid on the block, that in many ways improve on Pytorch and Tensorflow, but is still not as mature as the other frameworks. As the frameworks uses different kind programming principles (object oriented vs. functional programming), comparing them is essentially meaningless.</p> <p>In this course we have chosen to work with Pytorch, because we find it a bit more intuitive and it is the framework that we use for our day to day research life. Additionally, as of right now it is the absolutely the dominating framework for published models, research papers and competition winners</p> <p>\\ The intention behind this set of exercises is to bring everyones Pytorch skills up-to-date. If you already are Pytorch-Jedi feel free to pass the first set of exercises, but I recommend that you still complete it. The exercises are in large part taken directly from the deep learning course at udacity. Note that these exercises are given as notebooks, which is the last time we are going to use them actively in course. Instead after this set of exercises we are going to focus on writing code in python scripts.</p> <p>The notebooks contains a lot of explaining text. The exercises that you are supposed to fill out are inlined in the text in small \"exercise\" blocks:</p> <p></p> <p>If you need a fresh up on any deep learning topic in general throughout the course, we recommend to find the relevant chapter in the deep learning book by Ian Goodfellow, Yoshua Bengio and Aaron Courville (can also be found in the literature folder).</p>"},{"location":"s1_development_environment/deep_learning_software/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <ol> <li> <p>Start a jupyter notebook session in your terminal (assuming you are standing in the root of the course material)</p> <pre><code>jupyter notebook s1_development_environment/exercise_files/\n</code></pre> </li> <li> <p>Complete the     Tensors in Pytorch     notebook. It focuses on basic manipulation of Pytorch tensors. You can pass this notebook if you are comfortable     doing this.</p> <ol> <li>(Bonus exercise): Efficiently write a function that calculates the pairwise squared distance     between an <code>[N,d]</code> tensor and <code>[M,d]</code> tensor. You should use the following identity:     $||a-b||^2 = ||a||^2 + ||b||^2 - 2$. Hint: you need to use broadcasting. <li> <p>Complete the     Neural Networks in Pytorch     notebook. It focuses on building a very simple neural network using the Pytorch <code>nn.Module</code> interface.</p> <ol> <li>(Bonus exercise): One layer that arguably is missing in Pytorch is for doing reshapes.     It is of course possible to do this directly to tensors, but sometimes it is great to     have it directly in a <code>nn.Sequential</code> module. Write a <code>Reshape</code> layer which <code>__init__</code>     takes a variable number arguments e.g. <code>Reshape(2)</code> or <code>Reshape(2,3)</code> and the forward     takes a single input <code>x</code> where the reshape is applied to all other dimensions than the     batch dimension.</li> </ol> </li> <li> <p>Complete the     Training Neural Networks     notebook. It focuses on how to write a simple training loop for training a neural network.</p> <ol> <li> <p>(Bonus exercise): A working training loop in Pytorch should have these three function calls:     <code>optimizer.zero_grad()</code>, <code>loss.backward()</code>, <code>optimizer.step()</code>. Explain what would happen     in the training loop (or implement it) if you forgot each of the function calls.</p> </li> <li> <p>(Bonus exercise): Many state-of-the-art results depend on the concept of learning rate schedulers.     In short a learning rate scheduler go in and either statically or dynamically changes the learning     rate of your optimizer, such that training speed is either increased or decreased. Implement a     learning rate scheduler     in the notebook.</p> </li> </ol> </li> <li> <p>Complete the     Fashion MNIST     notebook, that summaries concepts learned in the notebook 2 and 3 on building a neural network for classifying the     Fashion MNIST dataset.</p> <ol> <li>(Bonus exercise): The exercise focuses on the Fashion MNIST dataset but should without much     work be able to train on multiple datasets. Implement a variable <code>dataset</code> that can take the     values <code>mnist</code>, <code>fashionmnist</code> and <code>cifar</code> and train a model on the respective dataset.</li> </ol> </li> <li> <p>Complete the     Inference and Validation     notebook. This notebook adds important concepts on how to do inference and validation on our neural network.</p> <ol> <li>(Bonus exercise): The exercise shows how dropout can be used to prevent overfitting. However, today it     is often used to get uncertainty estimates of the network predictions using     Monte Carlo Dropout. Implement monte carlo dropout such that we at     inference time gets different predictions for the same input (HINT: do not set the network in evaluation mode).     Construct a histogram of class prediction for a single image using 100 monte carlo dropout samples.</li> </ol> </li> <li> <p>Complete the     Saving_and_Loading_Models     notebook. This notebook addresses how to save and load model weights. This is important if you want to share a     model with someone else.</p> <ol> <li>(Bonus exercise): Being able to save and load weights are important for the concept of early stopping. In     short, early stopping monitors some metric (often on the validation set) and then will stop the training     and save a checkpoint when this metric has not improved for <code>N</code> steps. Implement early stopping in one of     the previous notebooks.</li> </ol> </li>"},{"location":"s1_development_environment/deep_learning_software/#final-exercise","title":"Final exercise","text":"<p>As the final exercise we will develop a simple baseline model which we will continue to develop on during the course. For this exercise we provide the data in the <code>data/corruptedmnist</code> folder. Do NOT use the data in the <code>corruptedmnist_v2</code> folder as that is intended for another exercise. As the name suggest this is a (subsampled) corrupted version of regular mnist. Your overall task is the following:</p> <p>Implement a mnist neural network that achieves at least 85 % accuracy on the test set.</p> <p>Before any training can start, you should identify what corruption that we have applied to the mnist dataset to create the corrupted version. This should give you a clue about what network architechture to use.</p> <p>One key point of this course is trying to stay organized. Spending time now organizing your code, will save time in the future as you start to add more and more features. As subgoals, please fulfill the following exercises</p> <ol> <li> <p>Implement your model in a script called <code>model.py</code></p> </li> <li> <p>Implement your data setup in a script called <code>data.py</code>. Hint: The data can be loaded using     np.load.</p> </li> <li> <p>Implement training and evaluation of your model in <code>main.py</code> script. The <code>main.py</code> script should be able to     take an additional subcommands indicating if the model should train or evaluate. It will look something like this:</p> <pre><code>python main.py train --lr 1e-4\npython main.py evaluate trained_model.pt\n</code></pre> <p>which can be implemented in various ways.</p> </li> </ol> <p>To start you off, a very barebone version of each script is provided in the <code>final_exercise</code> folder. We have already implemented some logic, especially to make sure you can easily run different subcommands in for step 4. If you are interested in how this is done you can checkout this optional module on defining command line interfaces (CLI). We additionally also provide an <code>requirements.py</code> with suggestion to what packages are nessesary to complete the exercise.</p> <p>\\ As documentation that your model is actually working, when running in the <code>train</code> command the script needs to produce a single plot with the training curve (training step vs training loss). When the <code>evaluate</code> command is run, it should write the test set accuracy to the terminal.</p> <p>It is part of the exercise to not implement in notebooks as code development in the real life happens in script. As the model is simple to run (for now) you should be able to complete the exercise on your laptop, even if you are only training on cpu. That said you are allowed to upload your scripts to your own \"Google Drive\" and then you can call your scripts from a Google Colab notebook, which is shown in the image below where all code is place in the <code>fashion_trainer.py</code> script and the Colab notebook is just used to execute it.</p> <p></p> <p>Be sure to have completed the final exercise before the next session, as we will be building on top of the model you have created.</p>"},{"location":"s1_development_environment/editor/","title":"M3 - Editor","text":""},{"location":"s1_development_environment/editor/#editoride","title":"Editor/IDE","text":"<p>Core Module</p> <p>Notebooks can be great for testing out ideas, developing simple code and explaining and visualizing certain aspects of a codebase. Remember that Jupyter notebook was created with intention to \"...allows you to create and share documents that contain live code, equations, visualizations and narrative text.\" However, any larger machine learning project will require you to work in multiple <code>.py</code> files and here notebooks will provide a suboptimal workflow. Therefore, to for truly getting \"work done\" you will need a good editor / IDE.</p> <p>Many opinions exist on this matter, but for simplicity we recommend getting started with one of the following 3:</p> Editor Webpage Comment (Biased opinion) Spyder https://www.spyder-ide.org/ Matlab like environment that is easy to get started with Visual studio code https://code.visualstudio.com/ Support for multiple languages with fairly easy setup PyCharm https://www.jetbrains.com/pycharm/ IDE for python professionals. Will take a bit of time getting used to <p>We highly recommend Visual studio (vs) code if you do not already have a editor installed (or just want to try something new.). We therefore put additional effort into explaining vs code.</p> <p>Below you see an overview of the vs code interface</p> <p></p>  Image credit  <p>The main components of VS code are:</p> <ul> <li> <p>The action bar: VS code is not an editor meant for a single language and can do many things. One of the core reasons     that VC code have become so popular is that custom plug-ins called extensions can be installed to add     functionality to VS code. It is in the action bar that you can navigate between these different applications     when you have installed them.</p> </li> <li> <p>The side bar: The side bar has different functionality depending on what extension that you have open.     In most cases, the side bar will just contain the file explorer.</p> </li> <li> <p>The editor: This where you code is. VS code supports a number of layouts in the editor (one column, two column ect.).     You can make a custom layout by dragging a file to where you want the layout to split.</p> </li> <li> <p>The panel: The panel contains a terminal for you to interact with. This can quickly be used to try out code by     opening a <code>python</code> interpreter, management of environments etc.</p> </li> <li> <p>The status bar: The status bar contains information based on the extensions that you have installed. In particular     for python development, the status bar can be used to change conda environment.</p> </li> </ul>"},{"location":"s1_development_environment/editor/#exercises","title":"\u2754 Exercises","text":"<p>Start by downloading and install one of the editors / IDE and make yourself familiar with it e.g. try out the editor on the files that you created in the final exercise in the last lecture.</p> <p>The remaining of the exercises are specific to Visual studio code but we recommend that you try to answer the questions if using another editor. In the <code>exercise_files</code> folder belonging to this session we have put cheat sheets for vs code (one for Windows and one for Mac/Linux), that can give you an easy overview of the different macros in vs code. The following exercises are just to get you started but you can find many more tutorials here.</p> <ol> <li> <p>VS code is a general editor for many languages and to get proper python support we need to install some     extensions. In the <code>action bar</code> go to the <code>extension</code> tap and search for <code>python</code> in the marketplace. For here     we highly recommend installing the following packages:</p> <ul> <li><code>Python</code>: general python support</li> <li><code>Python Test Explorer for Visual Studio Code</code>: support for testing of python code (we get to that in a later lecture)</li> <li><code>Jupyter</code>: support for jupyter notebooks directly in VSCode</li> </ul> </li> <li> <p>If you install the <code>Python</code> package you should see something like this in your status bar:</p> <p> </p> <p>which indicates that you are using the stock python installation, instead of the one you have created using <code>conda</code>. Click it and change the python environment to the one you actually want to use.</p> </li> <li> <p>One of the most useful tools in VSCode is the ability to navigate a hole project using the build-in     <code>Explorer</code>. To really take advantage of the VC code you need to make sure what you are working on is a project.     Create a folder called <code>hello</code> (somewhere on your laptop) and open it in VScode (Click <code>File</code> in the menu and then     select <code>Open Folder</code>). You should end up with a completly clean workspace (as shown below). Click the <code>New file</code>     button and create a file called <code>hello.py</code>.</p> <p>  Image credit  </p> </li> <li> <p>Finally, lets run some code. Add something simple to the <code>hello.py</code> file like:</p> <p>  Image credit  </p> <p>and click the <code>run</code> button as shown in the image. It should create a new terminal, activate the environment that you have chosen and finally run your script. In addition to clicking the <code>run</code> button, you can also</p> <ul> <li>Select some code and press <code>Shift+Enter</code> to run it in the terminal</li> <li>Select some code and right click, choosing to run in a interactive window (where you can interact with the results     like in a jupyter notebook)</li> </ul> </li> </ol> <p>Thats, the basic of using VScode. We recommend highly that you revisit this tutorial during the course when we get to topics such as debugging and version control which VScode can help with.</p>"},{"location":"s1_development_environment/editor/#a-note-on-jupyter-notebooks-in-production-environments","title":"A note on jupyter notebooks in production environments","text":"<p>As already stated jupyter notebooks are great for development as they allow developers to easily test our new ideas. However, they often lead to pain points when models actually needs to be deployed. We highly recommend reading section 5.1.1 of this paper by Shankar et al. that in more detail discuss the strong opinions to jupyter notebooks that exist within the developer community.</p> <p>All this said there at least exist one simple tool to make notebooks work better in a production setting. Its called <code>nbconvert</code> and can be installed with</p> <pre><code>conda install nbconvert # or pip install nbconvert\n</code></pre> <p>You may need some further dependencies such as Pandoc, TeX and Pyppeteer for it to work (see install instructions here). After this, converting a notebook to a <code>.py</code> script is a simple as:</p> <pre><code>jupyter nbconvert --to=script my_notebook.ipynb\n</code></pre> <p>which will produce a similar named script called <code>my_notebook.py</code>. We highly recommend that you stick to developing scripts directly during the course to get experience with doing so, but <code>nbconvert</code> can be an fantastic tool to have in your toolbox.</p>"},{"location":"s2_organisation_and_version_control/","title":"Getting started with MLOps - Organization and version control","text":"<p>Slides</p> <p> </p> <p>Today we take our first steps into the world of MLOps. The set of modules in this session focuses on getting organized and making sure that you are familiar with good development practices. While many of the practices you will learn about these modules does not seem that important when you are a single person working on a project, it is crucial when working in large groups that the difference in how different people organize and write their code is minimized. The topics in this session will focus on:</p> <ul> <li>Version control for helping tracking and managing changes to your code and data</li> <li>Coding practices for staying organized in lar</li> </ul> <p></p>  Image credit  <p>Some exercises in this course are very loosely stated (including the exercises today). You are expected to seek out information before you ask for help (Google is your friend!) as you will both learn more for trying to solve the problems yourself, and it is more realistic of how the \"real world\" works.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the basics of version control and can use <code>git</code> to track changes to your code</li> <li>Knowledge on how to package python code into a library and how organize your code for reuse</li> <li>Understand different coding practices and how to use them to improve the quality of your code</li> <li>Can use <code>dvc</code> to version control data</li> </ul>"},{"location":"s2_organisation_and_version_control/code_structure/","title":"M6 - Code structure","text":""},{"location":"s2_organisation_and_version_control/code_structure/#code-organization","title":"Code organization","text":"<p>Core Module</p> <p>With a basic understanding of version control, it is now time to really begin filling up our code repository. However, the question then remains how to organize our code? As developers we tend to not think about code organization that much. It is instead something that just dynamically is being created as we may need it. However, maybe we should spend some time initially getting organized with the chance of this making our code easier to develop and maintain in the long run. If we do not spend time organizing our code, we may end up with a mess of code that is hard to understand or maintain</p> <p>Big ball of Mud</p> <p>A Big Ball of Mud is a haphazardly structured, sprawling, sloppy, duct-tape-and-baling-wire, spaghetti-code jungle. These systems show unmistakable signs of unregulated growth, and repeated, expedient repair. Information is shared promiscuously among distant elements of the system, often to the point where nearly all the important information becomes global or duplicated. The overall structure of the system may never have been well defined. If it was, it may have eroded beyond recognition. Programmers with a shred of architectural sensibility shun these quagmires. Only those who are unconcerned about architecture, and, perhaps, are comfortable with the inertia of the day-to-day chore of patching the holes in these failing dikes, are content to work on such systems.  Brian Foote and Joseph Yoder, Big Ball of Mud. Fourth Conference on Patterns Languages of Programs (PLoP '97/EuroPLoP '97) Monticello, Illinois, September 1997</p> <p>We are here going to focus on the organization of data science projects e.g. where some kind of data is involved. The key to modern machine learning/deep learning is without a doubt the vast amounts of data that we have access to today. It is therefore not unreasonable that data should influence our choice of code structure.</p> <p>We are in this course going to use the <code>cookie-cutter</code> approach. We are not going to argue that <code>cookie-cutter</code> is better than other approaches to code organization, we are just focusing on that it is standardized way of creating project structures. By standardized we mean, that if two persons are both using <code>cookie-cutter</code> the layout of their code does follow some specific rules, making one able to faster get understand the other persons code. Code organization is therefore not only to make the code easier for you to maintain but also for others to read and understand.</p> <p>Below is seen the default code structure of cookie-cutter for data science projects.</p> <p> </p>  Image credit  <p>What is important to keep in mind when using a template such as cookie-cutter, is that it exactly is a template. By definition a template is guide to make something. Therefore, not all parts of an template may be important for your project at hand. Your job is to pick the parts from the template that is useful for organizing your data science. project.</p>"},{"location":"s2_organisation_and_version_control/code_structure/#exercises","title":"\u2754 Exercises","text":"<p>After having installed cookiecutter (exercise 1 and 2), the remaining exercises are intended to be used on taking the simple CNN MNIST classifier from yesterdays exercise and force it into this structure. You are not required to fill out every folder and file in the project structure, but try to at least follow the steps in exercises. Whenever you need to run a file I recommend always doing this from the root directory e.g.</p> <pre><code>python src/data/make_dataset.py data/raw data/processed\npython src/models/train_model.py &lt;arguments&gt;\nect...\n</code></pre> <p>in this way paths (for saving and loading files) are always relative to the root.</p> <ol> <li> <p>Start by reading this page, as it will give you insight     to why standardized code organization is important.</p> </li> <li> <p>Install cookie cutter for data science</p> <pre><code># install using the terminal\npip install cookiecutter\n</code></pre> </li> <li> <p>Take a look at the webpage to see how you start a new project. We recommend using <code>v2</code> of cookiecutter.</p> </li> <li> <p>After having created your project we are going to install it as a package in our conda environment. Either run</p> <pre><code># install in a terminal in your conda env\npip install -e .\n# or\nconda develop .\n</code></pre> <p>In addition you may need to run</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>to install additional packages required by <code>cookie-cutter</code>.</p> </li> <li> <p>Start by filling out the <code>src/data/make_dataset.py</code> file. When this file runs, it should take the raw data files in     <code>data/raw</code> (the files that we have provided) process them into a single tensor, normalize the tensor and save this     intermediate representation to the <code>data/processed</code> folder. By normalization here we refer to making sure the     images have mean 0 and standard deviation 1.</p> </li> <li> <p>Every <code>cookie-cutter</code> project comes with a build in <code>Makefile</code> that can be used to easily define common operations in     a project. You do not have to understand the complete file by try taking a look at it. In particular the following     commands may come in handy</p> <pre><code>make data  # runs the make_dataset.py file, try it!\nmake clean  # clean __pycache__ files\nmake requirements  # install everything in the requirements.py file\n</code></pre> Windows users <p><code>make</code> is a GNU build tool that is by default not available on Windows. There are two recommended ways to get it running on Windows. The first is leveraging linux subsystem for Windows which you maybe have already installed. The second option is utilizing the chocolatey package manager, which enables Windows users to install packages similar to Linux system. The second option is running</p> </li> <li> <p>Put your model file (<code>model.py</code>) into <code>src/models</code> folder together and insert the relevant code from the <code>main.py</code>     file into the <code>train_model.py</code> file. Make sure that whenever a model is trained and it is saved, that it gets saved     to the <code>models</code> folder (preferably in sub-folders).</p> </li> <li> <p>When you run <code>train_model.py</code>, make sure that some statistics/visualizations from the trained models gets saved to     the <code>reports/figures/</code> folder. This could be a simple <code>.png</code> of the training curve.</p> </li> <li> <p>(Optional) Can you figure out a way to add a <code>train</code> command to the <code>Makefile</code> such that training can be started     using</p> <pre><code>make train\n</code></pre> </li> <li> <p>Fill out the newly created <code>src/models/predict_model.py</code> file, such that it takes a pre-trained model file and     creates prediction for some data. Recommended interface is that users can give this file either a folder with raw     images that gets loaded in or a <code>numpy</code> or <code>pickle</code> file with already loaded images e.g. something like this</p> <pre><code>python src/models/predict_model.py \\\nmodels/my_trained_model.pt \\  # file containing a pretrained model\ndata/example_images.npy  # file containing just 10 images for prediction\n</code></pre> </li> <li> <p>Fill out the file <code>src/visualization/visualize.py</code> with this (as minimum, feel free to add more visualizations)</p> <ul> <li>Loads a pre-trained network</li> <li>Extracts some intermediate representation of the data (your training set) from your cnn. This could be the     features just before the final classification layer</li> <li>Visualize features in a 2D space using     t-SNE to do the dimensionality     reduction.</li> <li>Save the visualization to a file in the <code>reports/figures/</code> folder.</li> </ul> </li> <li> <p>(Optional) Feel free to create more files/visualizations (what about investigating/explore the data distribution?)</p> </li> <li> <p>Make sure to update the <code>README.md</code> file with a short description on how your scripts should be run</p> </li> <li> <p>Finally make sure to update the <code>requirements.txt</code> file with any packages that are necessary for running your     code (see this set of exercises for help)</p> </li> </ol> <p>That ends the module on code structure and <code>cookiecutter</code>. We again want to stress the point that <code>cookiecutter</code> is just one template for organizing your code. What often happens in a team is that multiple templates are needed in different stages of the development phase or for different product types because they share commen structure, while still having some specifics. Keeping templates up-to-date then becomes critical such that no team member is using an outdated template. If you ever end up in this situation, we highly recommend to checkout cruft that works alongside <code>cookiecutter</code> to not only make projects but update existing ones as template evolves. Cruft additionally also has template validation capabilities to ensure projects match the latest version of a template.</p>"},{"location":"s2_organisation_and_version_control/dvc/","title":"M8 - Data version control","text":""},{"location":"s2_organisation_and_version_control/dvc/#data-version-control","title":"Data Version Control","text":"<p>Core Module</p> <p>In this module we are going to return to version control. However, this time we are going to focus on version control of data. The reason we need to separate between standandard version control and data version control comes down to one problem: size.</p> <p>Classic version control was developed to keep track of code files, which all are simple text files. Even a codebase that contains 1000+ files with million lines of codes can probably be stored in less than a single gigabyte (GB). On the other hand, the size of data can be drastically bigger. As most machine learning algorithms only gets better with the more data that you feed them, we are seeing models today that are being trained on petabytes of data (1.000.000 GB).</p> <p>Because this is a important concept there exist a couple of frameworks that have specialized in versioning data such as dvc, DAGsHub, Hub, Modelstore and ModelDB. We are here going to use <code>dvc</code> provided by iterative.ai as they also provide tools for automatizing machine learning, which we are going to focus on later.</p>"},{"location":"s2_organisation_and_version_control/dvc/#dvc-what-is-it","title":"DVC: What is it?","text":"<p>DVC (Data Version Control) is simply an extension of <code>git</code> to not only take versioning data but also models and experiments in general. But how does it deal with these large data files? Essentially, <code>dvc</code> will just keep track of a small metafile that will then point to some remote location where you original data is store. metafiles essentially works as placeholders for your datafiles. Your large datafiles are then stored in some remote location such as Google drive or an <code>S3</code> bucket from Amazon.</p> <p> </p>  Image credit  <p>As the figure shows, we now have two remote locations: one for code and one for data. We use <code>git pull/push</code> for the code and <code>dvc pull/push</code> for the data. The key concept is the connection between the data file <code>model.pkl</code> that is fairly large and its respective metafile <code>model.pkl.dvc</code> that is very small. The large file is stored in the data remote and the metafile is stored in code remote.</p>"},{"location":"s2_organisation_and_version_control/dvc/#exercises","title":"\u2754 Exercises","text":"<p>If in doubt about some of the exercises, we recommend checking out the documentation for dvc as it contains excellent tutorials.</p> <ol> <li> <p>For these exercises we are going to use Google drive as remote storage     solution for our data. If you do not already have a Google account, please create one (we are going to use it again     in later exercises). Please make sure that you at least have 1GB of free space.</p> </li> <li> <p>Next, install dvc and the Google drive extension</p> <pre><code>pip install dvc\npip install \"dvc[gdrive]\"\n</code></pre> <p>If you installed DVC via pip and plan to use cloud services as remote storage, you might need to install these optional dependencies: [s3], [azure], [gdrive], [gs], [oss], [ssh]. Alternatively, use [all] to include them all. If you encounter that the installation fails, we recommend that you start by updating pip and then trying to update <code>dvc</code>:</p> <pre><code>pip install -U pip\npip install -U \u201ddvc[gdrive]\u201d\n</code></pre> <p>If this does not work for you, it is most likely due to a problem with <code>pygit2</code> and in that case we recommend that you follow the instructions here.</p> </li> <li> <p>In your MNIST repository run the following command from the terminal</p> <pre><code>dvc init\n</code></pre> <p>this will setup <code>dvc</code> for this repository (similar to how <code>git init</code> will initialize a git repository). These files should be committed using standard <code>git</code> to your repository.</p> </li> <li> <p>Go to your Google drive and create a new folder called <code>dtu_mlops_data</code>. Then copy the unique identifier     belonging to that folder as shown in the figure below</p> <p> </p> <p>Using this identifier, add it as a remote storage</p> <pre><code>dvc remote add -d storage gdrive://&lt;your_identifier&gt;\n</code></pre> </li> <li> <p>Check the content of the file <code>.dvc/config</code>. Does it contain a pointer to your remote storage? Afterwards make sure     to add this file to the next commit we are going to make:</p> <pre><code>git add .dvc/config\n</code></pre> </li> <li> <p>Call the <code>dvc add</code> command on your data files exactly like you would add a file with <code>git</code> (you do not need to     add every file by itself as you can directly add the <code>data/</code> folder). Doing this should create a human-readable     file with the extension <code>.dvc</code>. This is the metafile  as explained earlier that will serve as a placeholder for     your data. If you are on Windows and this step fail you may need to install <code>pywin32</code>. At the same time the <code>data/</code>     folder should have been added to the <code>.gitignore</code> file that marks which files should not be tracked by git. Confirm     that this is correct.</p> </li> <li> <p>Now we are going to add, commit and tag the metafiles so we can restore to this stage later on. Commit and tag     the files, should look something like this:</p> <pre><code>git add data.dvc .gitignore\ngit commit -m \"First datasets, containing 25000 images\"\ngit tag -a \"v1.0\" -m \"data v1.0\"\n</code></pre> </li> <li> <p>Finally, push your data to the remote storage using <code>dvc push</code>. You will be asked to authenticate, which involves     copy-pasting the code in the link prompted. Checkout your Google drive folder. You will see that the data is not     in a recognizable format anymore due to the way that <code>dvc</code> packs and tracks the data. The boring details is that     <code>dvc</code> converts the data into content-addressable storage     which makes data much faster to get. Finally, make sure that your data is not stored in your github repository.</p> <p>After authenticating the first time, dvc should be setup without having to authenticate again. If you for some reason encounter that dvc fails to authenticate, you can try to reset the authentication. Locate the file <code>$CACHE_HOME/pydrive2fs/{gdrive_client_id}/default.json</code> where <code>$CACHE_HOME</code> depends on your operating system:</p> macOSLinuxWindows <p><code>~/Library/Caches</code></p> <p><code>~/.cache</code>  This is the typical location, but it may vary depending on what distro you are running</p> <p><code>{user}/AppData/Local</code></p> <p>Delete the complete <code>{gdrive_client_id}</code> folder and retry authenticating with <code>dvc push</code>.</p> </li> <li> <p>After completing the above steps, it is very easy for others (or yourself) to get setup with both     code and data by simply running</p> <pre><code>git clone &lt;my_repository&gt;\ncd &lt;my_repository&gt;\ndvc pull\n</code></pre> <p>(assuming that you give them access right to the folder in your drive). Try doing this (in some other location than your standard code) to make sure that the two commands indeed downloads both your code and data.</p> </li> <li> <p>Lets look about the process of updating our data. Remember the important aspect of version control is that we do not     need to store explicit files called <code>data_v1.pt</code>, <code>data_v2.pt</code> ect. but just have a single <code>data.pt</code> that where we     can always checkout earlier versions. Initially start by copying the data <code>data/corruptmnist_v2</code> folder from this     repository to your MNIST code. This contains 3 extra datafiles with 15000 additional observations. Rerun your data     pipeline so these gets incorporated into the files in your <code>processed</code> folder.</p> </li> <li> <p>Redo the above steps, adding the new data using <code>dvc</code>, committing and tagging the metafiles e.g. the following     commands should be executed (with appropriate input):     <code>dvc add -&gt; git add -&gt; git commit -&gt; git tag -&gt; dvc push -&gt; git push</code>.</p> </li> <li> <p>Lets say that you wanted to go back to the state of your data in v1.0. If the above steps have been done correctly,     you should be able to do this using:</p> <pre><code>git checkout v1.0\ndvc checkout\n</code></pre> <p>confirm that you have reverted back to the original data.</p> </li> <li> <p>(Optional) Finally, it is important to note that <code>dvc</code> is not only intended to be used to store data files but also     any other large files such as trained model weights (with billion of parameters these can be quite large). For     example if we always stored out best performing model in a file called <code>best_model.ckpt</code> then we can use <code>dvc</code> to     version control it, store it online and make it easy for other to download. Feel free to experiment with this using     your own model checkpoints.</p> </li> </ol> <p>Thats all for today. With the combined power of <code>git</code> and <code>dvc</code> we should be able to version control everything in our development pipeline such that no changes are lost (assuming we commit regularly). It should be noted that <code>dvc</code> offers such more than just data version control, so if you want to deep dive into <code>dvc</code> we recommend their pipeline feature and how this can be used to setup version controlled experiments. Note that we are going to revisit <code>dvc</code> later for a more permanent (and large scale) storage solution.</p>"},{"location":"s2_organisation_and_version_control/git/","title":"M5 - Git","text":""},{"location":"s2_organisation_and_version_control/git/#git","title":"Git","text":"<p>Core Module</p> <p>Proper collaboration with other people will require that you can work on the same codebase in an organized manner. This is the reason that version control exist. Simply stated, it is a way to keep track of:</p> <ul> <li>Who made changes to the code</li> <li>When did the change happen</li> <li>What changes were made</li> </ul> <p>For a full explanation please see this page</p> <p>Secondly, it is important to note that Github is not git! Github is the dominating player when it comes to hosting repositories but that does not mean that they are the only one providing free repository hosting (see bitbucket or gitlab) for some other examples).</p> <p>That said we will be using git and Github throughout this course. It is a requirement for passing this course that you create a public repository with your code and use git to upload any code changes. How much you choose to integrate this into your own projects depends, but you are at least expected to be familiar with git+github.</p> <p></p>  Image credit"},{"location":"s2_organisation_and_version_control/git/#initial-config","title":"Initial config","text":"<p>What does Git stand for?</p> <p>The name \"git\" was given by Linus Torvalds when he wrote the very first version. He described the tool as \"the stupid content tracker\" and the name as (depending on your mood):</p> <ul> <li>Random three-letter combination that is pronounceable, and not actually used by any common UNIX command. The fact     that it is a mispronunciation of \"get\" may or may not be relevant.</li> <li>Stupid. Contemptible and Despicable. simple. Take your pick from the dictionary of slang.</li> <li>\"Global information tracker\": you're in a good mood, and it actually works for you.     Angels sing, and a light suddenly fills the room.</li> <li>\"Goddamn idiotic truckload of sh*t\": when it breaks</li> </ul> <ol> <li> <p>Install git on your computer and make sure     that your installation is working by writing <code>git help</code> in a terminal and it should show you the help message for     git.</p> </li> <li> <p>Create a github account if you do not already have one.</p> </li> <li> <p>To make sure that we do not have to type in our github username every time that we want to do some changes,     we can once and for all set them on our local machine</p> <pre><code># type in a terminal\ngit config credential.helper store\ngit config --global user.email &lt;email&gt;\n</code></pre> </li> </ol>"},{"location":"s2_organisation_and_version_control/git/#git-overview","title":"Git overview","text":"<p>The most simple way to think of version control, is that it is just nodes with lines connecting them</p> <p></p> <p>Each node, which we call a commit is uniquely identified by a hash string. Each node, stores what our code looked like at that point in time (when we made the commit) and using the hash codes we can easily revert to a specific point in time.</p> <p>The commits are made up of local changes that we make to our code. A basic workflow for adding commits are seen below</p> <p></p> <p>Assuming that we have made some changes to our local working directory and that we want to get these updates to be online in the remote repository we have to do the following steps:</p> <ul> <li> <p>First we run the command <code>git add</code>. This will move our changes to the staging area. While changes are in the     staging area we can very easily revert them (using <code>git restore</code>). There have therefore not been assigned a unique     hash to the code yet, and we can therefore still overwrite it.</p> </li> <li> <p>To take our code from the staging area and make it into a commit, we simply run <code>git commit</code> which will locally     add a note to the graph. It is important again, that we have not pushed the commit to the online repository yet.</p> </li> <li> <p>Finally, we want others to be able to use the changes that we made. We do a simple <code>git push</code> and our     commit gets online</p> </li> </ul> <p>Of course, the real power of version control is the ability to make branches, as in the image below</p> <p></p>  Image credit  <p>Each branch can contain code that are not present on other branches. This is useful when you are many developers working together on the same project.</p>"},{"location":"s2_organisation_and_version_control/git/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>In your github account create an repository, where the intention is that you upload the code from the final     exercise from yesterday</p> <ol> <li> <p>After creating the repository, clone it to your computer</p> <pre><code>git clone https://github.com/my_user_name/my_repository_name.git\n</code></pre> </li> <li> <p>Move/copy the three files from yesterday into the repository (and any other that you made)</p> </li> <li> <p>Add the files to a commit by using <code>git add</code> command</p> </li> <li> <p>Commit the files using <code>git commit</code></p> </li> <li> <p>Finally push the files to your repository using <code>git push</code>. Make sure to check online that the files have been     updated in your repository.</p> </li> <li> <p>You can always use the command <code>git status</code> to check where you are in the process of making a commit.</p> </li> <li> <p>Also checkout the <code>git log</code> command, which will show you the history of commits that you have made.</p> </li> </ol> </li> <li> <p>Make sure that you understand how to make branches, as this will allow you to try out code changes without     messing with your working code. Creating a new branch can be done using:</p> <pre><code># create a new branch\ngit checkout -b &lt;my_branch_name&gt;\n</code></pre> <p>Afterwards, you can use <code>git checkout</code> to change between branches (remember to commit your work!) Try adding something (a file, a new line of code etc.) to the newly created branch, commit it and try changing back to master afterwards. You should hopefully see whatever you added on the branch is not present on the main branch.</p> </li> <li> <p>If you do not already have a cloned version of this repository belonging to the course, make sure to make one!     I am continuously updating/changing some of the material during the course and I therefore recommend that you     each day before the lecture do a <code>git pull</code> on your local copy</p> </li> <li> <p>Git may seems like a waste of time when solutions like dropbox, google drive ect exist, and it is     not completely untrue when you are only one or two working on a project. However, these file management     systems falls short when hundreds to thousands of people work together. For this exercise you will     go through the steps of sending an open-source contribution:</p> <ol> <li>Go online and find a project you do not own, where you can improve the code. For simplicity you can    just choose the repository belonging to the course. Now fork the project by clicking the Fork button.</li> </ol> <p></p> <p>This will create a local copy of the repository which you have complete writing access to. Note that    code updates to the original repository does not update code in your local repository.</p> <ol> <li> <p>Clone your local fork of the project using <code>git clone</code>.</p> </li> <li> <p>As default your local repository will be on the <code>main branch</code> (HINT: you can check this with the    <code>git status</code> command). It is good practice to make a new branch when working on some changes. Use    the <code>git branch</code> command followed by the <code>git checkout</code> command to create a new branch.</p> </li> <li> <p>You are now ready to make changes to the repository. Try to find something to improve (any spelling mistakes?).    When you have made the changes, do the standard git cycle: <code>add -&gt; commit -&gt; push</code></p> </li> <li> <p>Go online to the original repository and go the <code>Pull requests</code> tab. Find <code>compare</code> botton and    choose the to compare the <code>master branch</code> of the original repo with the branch that you just created    in your own repository. Check the diff on the page to make sure that it contains the changes you have made.</p> </li> <li> <p>Write a bit about the changes you have made and click <code>Create pull request</code> :)</p> </li> </ol> </li> <li> <p>Forking a repository has the consequence that your fork and the repository that you forked can diverge. To     mitigate this we can set what is called an remote upstream. Take a look on this     page     , and set a remote upstream for the repository you just forked.</p> </li> <li> <p>After setting the upstream branch, we need to pull and merge any update. Take a look on this     page     and figure out how to do this.</p> </li> <li> <p>As a final exercise we want to simulate a merge conflict, which happens when two users try to commit changes     to exactly same lines of code in the codebase, and git is not able to resolve how the different commits should be     integrated.</p> <ol> <li> <p>In your browser, open your favorite repository (it could be the one you just worked on), go to any file of     your choosing and click the edit button (see image below) and make some change to the file. For example, if     you choose a python file you can just import some random packages at the top of the file. Commit the change.</p> <p> </p> </li> <li> <p>Make sure not to pull the change you just made to your local computer. Locally make changes to the same     file in the same lines and commit them afterwards.</p> </li> <li> <p>Now try to <code>git pull</code> the online changes. What should (hopefully) happen is that git will tell you that it found     a merge conflict that needs to be resolved. Open the file and you should see something like this</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nthis is some content to mess with\ncontent to append\n=======\ntotally different content to merge later\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; master\n</code></pre> <p>this should be interpret as: everything thats between <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code> and <code>=======</code> are the changes made by your local commit and everything between <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code> are the changes you are trying to pull. To fix the merge conflict you simply have to make the code in the two \"cells\" work together. When you are done, remove the identifiers <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>, <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>.</p> </li> <li> <p>Finally, commit the merge and try to push.</p> </li> </ol> </li> <li> <p>(Optional) The above exercises have focused on how to use git from the terminal, which I highly recommend learning.     However, if you are using a proper editor they also have build in support for version control. We recommend getting     familiar with these features (here is a tutorial for     VS Code)</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/git/#knowledge-check","title":"Knowledge check","text":"Knowledge question 1 <p>How do you know if a certain directory is a git repository?</p> Solution <p>You can check if there is a \".git\" directory. Alternative you can use the <code>git status</code> command.</p> Knowledge question 2 <p>Explain what the file <code>gitignore</code> is used for?</p> Solution <p>The file <code>gitignore</code> is used to tell git which files to ignore when doing a <code>git add .</code> command. This is useful for files that are not part of the codebase, but are needed for the code to run (e.g. data files) or files that contain sensitive information (e.g. <code>.env</code> files that contain API keys and passwords).</p> Knowledge question 3 <p>You have two branches - main and devel. What sequence of commands would you need to execute to make sure that devel is in sync with main?</p> Solution <pre><code>git checkout main\ngit pull\ngit checkout devel\ngit merge main\n</code></pre> Knowledge question 4 <p>What best practices are you familiar with regarding version control?</p> Solution <ul> <li>Use a descriptive commit message</li> <li>Make each commit a logical unit</li> <li>Incorporate others' changes frequently</li> <li>Share your changes frequently</li> <li>Coordinate with your co-workers</li> <li>Don't commit generated files</li> </ul> <p>That covers the basics of git to get you started. In the exercise folder you can find a git cheat sheet with the most useful commands for future reference. Finally, we want to point out another awesome feature of Github: in browser editor. Sometimes you have a small edit that you want to make, but still would like to do this in a IDE/editor. Or you may be in the situation where you are working from another device than your usual developer machine. Github has an build-in editor that can simply be enabled by changing any URL from</p> <pre><code>https://github.com/username/repository\n</code></pre> <p>to</p> <pre><code>https://github.dev/username/repository\n</code></pre> <p>Try it out on your newly created repository.</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/","title":"M7 - Good coding practice","text":""},{"location":"s2_organisation_and_version_control/good_coding_practice/#good-coding-practice","title":"Good coding practice","text":"<p>Quote</p> <p>Code is read more often than it is written.  Guido Van Rossum (author of Python)</p> <p>To understand what good coding practice is, it is important to understand what it is not:</p> <ul> <li>Making sure your code run fast</li> <li>Making sure that you use a specific coding paradigm (object orientated programming ect.)</li> <li>Making sure to only use few dependencies</li> </ul> <p>Instead good coding practices really comes down to two topics: documentation and styling.</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#documentation","title":"Documentation","text":"<p>Most programmers have a love-hate relationship with documentation: We absolute hate writing it ourself, but love when someone else has actually taken time to add it to their code. There is no doubt about that well documented code is much easier to maintain, as you do not need to remember all details about the code to still maintain it. It is key to remember that good documentation saves more time, than it takes to write.</p> <p>The problem with documentation is that there is no right or wrong way to do it. You can end up doing:</p> <ul> <li> <p>Under documentation: You document information that is clearly visible from the code and not the complex   parts that are actually hard to understand.</p> </li> <li> <p>Over documentation: Writing too much documentation will have the opposite effect on most people than   what you want: there is too much to read, so people will skip it.</p> </li> </ul> <p>Here is a good rule of thump for inline comments</p> <p>Quote</p> <p>Code tells you how; Comments tell you why.  Jeff Atwood</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Go over the most complicated file in your project. Be critical and add comments where the logic     behind the code is not easily understandable. Hint: In deep learning we often work with tensors that     change shape constantly. It is always a good idea to add comments where a tensor undergoes some reshaping.</p> </li> <li> <p>Add docstrings to at least two python function/methods.     You can see here (example 5) a good example     how to use identifiable keywords such as <code>Parameters</code>, <code>Args</code>, <code>Returns</code> which standardizes the way of     writing docstrings.</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#styling","title":"Styling","text":"<p>While python already enforces some styling (e.g. code should be indented in a specific way), this is not enough to secure that code from different users actually look like each other. Maybe even more troubling is that you will often see that your own style of coding changes as you become more and more experienced. This kind of difference in coding style is not that important to take care of when you are working on a personal project, but when working multiple people together on the same project it is important to consider.</p> <p>The question then remains what styling you should use. This is where Pep8 comes into play, which is the  official style guide for python. It is essentially contains what is considered \"good practice\" and \"bad practice\" when coding python.</p> <p>One way to check if your code is pep8 compliant is to use flake8.</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Install flake8</p> <pre><code>pip install flake8\n</code></pre> </li> <li> <p>run flake8 on your project</p> <pre><code>flake8 .\n</code></pre> <p>are you pep8 compliant or are you a normal mortal?</p> </li> </ol> <p>You could go and fix all the small errors that <code>flake8</code> is giving. However, in practice large projects instead relies on some kind of code formatter, that will automatically format your code for you to be pep8 compliant. Some of the biggest are:</p> <ul> <li>black</li> <li>yapf</li> </ul> <p>It is important to note, that code formatting is in general not about following a specific code style, but rather that all users follow the same.</p> <ol> <li>Install a code formatter of your own choice (I recommend <code>black</code>) and let it format at least one of the script in     your codebase. You can also try to play around with the different formatters to find out which formatter you like     the most</li> </ol> <p>One aspect not covered by <code>pep8</code> is how <code>import</code> statements in python should be organized. If you are like most people, you place your <code>import</code> statements at the top of the file and they are ordered simply by when you needed them. For this reason <code>import</code> statements is something we also want to take care of, but do not want to deal with ourself.</p> <ol> <li> <p>Install isort the standard for sorting imports</p> <pre><code>pip install isort\n</code></pre> </li> <li> <p>run isort on your project</p> <pre><code>isort .\n</code></pre> <p>and check how the imports were sorted.</p> </li> </ol> <p>Finally, we can also configure <code>black</code>, <code>isort</code> etc. to our specific needs. All the different frameworks can be configured directly from the command line. For example the recommended line length in <code>pep8</code> is 79 characters, which by many is considered very restrictive. If we wanted tell <code>flake8</code> and <code>black</code> to only error and correct code with a line length above 100 we could run the following commands</p> <pre><code># the . indicates that\nflake8 . --max-line-length 100\nblack . --line-length 100\n</code></pre> <p>While this is nice, it is much better to put such configurations into special python configuration files. The two commonly used are setup.cfg and pyproject.toml. For example, when you run <code>flake8</code> it will automatically look for a <code>setup.cfg</code> file in the current folder and apply those configs. The corresponding <code>setup.cfg</code> file to the command above would be</p> <pre><code>[flake8]\nexclude = venv\nignore = W503 # W503: Line break occurred before binary operator\nmax-line-length = 100\n</code></pre> <ol> <li> <p>Add the above code snippet to a file named <code>setup.cfg</code> in your project. Add a line with a length longer than the     standard 79 characters but below 100 and run <code>flake8</code> again to check that you get no error.</p> </li> <li> <p>To make sure that your formatter still does not try to format to 79 character length, add a <code>pyproject.toml</code> file to     your project where you customize the rules for the different formatters. For <code>black</code> you can look at this     page on how to configure the file     Again create a line above 79 but below 100 characters and check that it is not being formatted.</p> </li> <li> <p>(Optional) Experiment further with the customization of <code>flake8</code>, <code>black</code> etc. Especially it may be worth looking     into the <code>include</code> and <code>exclude</code> keywords for specifying which files should actually be formatted.</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#typing","title":"Typing","text":"<p>In addition to writing documentation and following a specific styling, in python we have a third way of improving the quality of our code: through typing. Typing goes back to the earlier programming languages like <code>c</code>, <code>c++</code> ect. where data types needed to be explicit stated for variables:</p> <pre><code>int main() {\nint x = 5 + 6;\nfloat y = 0.5;\ncout &lt;&lt; \"Hello World! \" &lt;&lt; x &lt;&lt; std::endl();\n}\n</code></pre> <p>This is not required by python but it can really improve the readability of code, that you can directly read from the code what the expected types of input arguments and returns are. In python the <code>:</code> character have been reserved for type hints. Here is one example of adding typing to a function:</p> <pre><code>def add2(x: int, y: int) -&gt; int:\nreturn x+y\n</code></pre> <p>here we mark that both <code>x</code> and <code>y</code> are integers and using the arrow notation <code>-&gt;</code> we mark that the output type is also a integer. Assuming that we are also going to use the function for floats and <code>torch.Tensor</code>s we could improve the typing by specifying a union of types. Depending on the version of python you are using the syntax for this can be different.</p> python &lt;3.10python &gt;=3.10 <pre><code>from torch import Tensor  # note it is Tensor with upper case T. This is the base class of all tensors\nfrom typing import Union\ndef add2(x: Union[int, float, Tensor], y: Union[int, float, Tensor]) -&gt; Union[int, float, Tensor]:\nreturn x+y\n</code></pre> <pre><code>from torch import Tensor  # note it is Tensor with upper case T. This is the base class of all tensors\ndef add2(x: int | float | Tensor, y: int | float | Tensor) -&gt; int | float | Tensor:\nreturn x+y\n</code></pre> <p>Finally, since this is a very generic function it also works on <code>numpy</code> arrays ect. we can always default to the <code>Any</code> type if we are not sure about all the specific types that a function can take</p> <pre><code>from typing import Any\ndef add2(x: Any, y: Any) -&gt; Any:\nreturn x+y\n</code></pre> <p>However, in this case we basically is in the same case as if our function were not typed, as the type hints does not help us at all. Therefore, use <code>Any</code> only when necessary.</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#exercises_2","title":"\u2754 Exercises","text":"<p>Exercise files</p> <ol> <li> <p>We provide a file called <code>typing_exercise.py</code>. Add typing everywhere in the file. Please note that you will     need the following import:</p> <pre><code>from typing import Callable, Optional, Tuple, Union, List  # you will need all of them in your code\n</code></pre> <p>for it to work. Hint: here is a good resource on typing. We also provide <code>typing_exercise_solution.py</code>, but try to solve the exercise yourself.</p> </li> <li> <p>mypy is what is called a static type checker. If you are using     typing in your code, then a static type checker can help you find common mistakes. <code>mypy</code> does not run your code,     but it scans it and checks that the types you have given are compatible. Install <code>mypy</code></p> <pre><code>pip install mypy\n</code></pre> </li> <li> <p>Try to run <code>mypy</code> on the <code>typing.py</code> file</p> <pre><code>mypy typing_exercise.py\n</code></pre> <p>If you have solved exercise 11 correctly then you should get no errors. If not <code>mypy</code> should tell you where your types are incompatible.</p> </li> </ol>"},{"location":"s3_reproducibility/","title":"Reproducibility","text":"<p>Slides</p> <p> </p> <p>Today is all about reproducibility - one of those concepts that everyone agrees is very important and something should be done about, but reality is that it is very hard to secure full reproducibility. The last sessions have already touched a bit on how tools like <code>conda</code> and code organization can help make code more reproducible. Today we are going all the way securing that our scripts and our compute environment is fully reproducible.</p>"},{"location":"s3_reproducibility/#why-does-reproducibility-matter","title":"Why does reproducibility matter","text":"<p>Reproducibility is closely related to the scientific method:</p> <p>Observe -&gt; Question -&gt; Hypotheses -&gt; Experiment -&gt; Conclude -&gt; Result -&gt; Observe -&gt; ...</p> <p>Not having reproducible experiments essentially break the cycle at between doing experiments and making conclusions. If experiments are not reproducible, then we do not expect that others can arrive at the same conclusion as ourself. As machine learning experiments are fundamentally the same as doing chemical experiments in a laboratory, we should be equally careful making sure our environments are reproducible (think of your laptop as your laboratory).</p> <p>Secondly, if we focus on why reproducibility matters especially to machine learning, it is part of the bigger challenge of making sure that machine learning is trustworthy.</p> <p> </p>  Image credit  <p>Trustworthy ML is basically the idea that machine learning agents can be trusted. Take the example of an machine learning agent being responsible for medical diagnoses. It is s very clear that we need to be able to trust that the agent give us the correct diagnosis for the system to work in practice. Reproducibility plays a big role here, because without we cannot be sure that the exact same agent deployed at two different hospitals will actually give the same diagnosis (given the same input).</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>To understand the importance of reproducibility in computer science</li> <li>To be able to use <code>docker</code> to create a reproducible containers</li> <li>Understand different ways of configuring your code and how to use <code>hydra</code> to integrate with config files</li> </ul>"},{"location":"s3_reproducibility/config_files/","title":"M10 - Config Files","text":""},{"location":"s3_reproducibility/config_files/#config-files","title":"Config files","text":"<p>With docker we can make sure that our compute environment is reproducible, but that does not mean that all our experiments magically becomes reproducible. There are other factors that are important for creating reproducible experiments.</p> <p>In this paper (highly recommended read) the authors tried to reproduce the results of 255 papers and tried to figure out which factors where significant to succeed. One of those factors were \"Hyperparameters Specified\" e.g. whether or not the authors of the paper had precisely specified the hyperparameter that was used to run the experiments. It should come as no surprise that this can be a determining factor for reproducibility, however it is not given that hyperparameters is always well specified.</p>"},{"location":"s3_reproducibility/config_files/#configure-experiments","title":"Configure experiments","text":"<p>There is really no way around it: deep learning contains a lot of hyperparameters. In general, a hyperparameter is any parameter that affects the learning process (e.g. the weights of a neural network are not hyperparameters because they are a consequence of the learning process). The problem with having many hyperparameters to control in your code, is that if you are not careful and structure them it may be hard after running a experiment to figure out which hyperparameters were actually used. Lack of proper configuration management can cause serious problems with reliability, uptime, and the ability to scale a system.</p> <p>One of the most basic ways of structuring hyperparameters, is just to put them directly into you <code>train.py</code> script in some object:</p> <pre><code>class my_hp:\nbatch_size: 64\nlr: 128\nother_hp: 12345\n# easy access to them\ndl = DataLoader(Dataset, batch_size=my_hp.batch_size)\n</code></pre> <p>the problem here is configuration is not easy. Each time you want to run a new experiment, you basically have to change the script. If you run the code multiple times, without committing the changes in between then the exact hyperparameter configuration for some experiments may be lost. Alright, with this in mind you change strategy to use an argument parser e.g. run experiments like this</p> <pre><code>python train.py --batch_size 256 --learning_rate 1e-4 --other_hp 12345\n</code></pre> <p>This at least solves the problem with configurability. However, we again can end up with loosing experiments if we are not careful.</p> <p>What we really want is some way to easy configure our experiments where the hyperparameters are systematically saved with the experiment. For this we turn our attention to Hydra, a configuration tool that is based around writing config files to keep track of hyperparameters. Hydra operates on top of OmegaConf which is a <code>yaml</code> based hierarchical configuration system.</p> <p>A simple <code>yaml</code> configuration file could look like</p> <pre><code>#config.yaml\nhyperparameters:\nbatch_size: 64\nlearning_rate: 1e-4\n</code></pre> <p>with the corresponding python code for loading the file</p> <pre><code>from omegaconf import OmegaConf\n# loading\nconfig = OmegaConf.load('config.yaml')\n# accessing in two different ways\ndl = DataLoader(dataset, batch_size=config.hyperparameters.batch_size)\noptimizer = torch.optim.Adam(model.parameters(), lr=config['hyperparameters']['lr'])\n</code></pre> <p>or using <code>hydra</code> for loading the configuration</p> <pre><code>import hydra\n@hydra.main(config_name=\"basic.yaml\")\ndef main(cfg):\nprint(cfg.hyperparameters.batch_size, cfg.hyperparameters.learning_rate)\nif __name__ == \"__main__\":\nmain()\n</code></pre> <p>The idea behind refactoring our hyperparameters into <code>.yaml</code> files is that we disentangle the model configuration from the model. In this way it is easier to do version control of the configuration because we have it in a separate file.</p>"},{"location":"s3_reproducibility/config_files/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>The main idea behind the exercises is to take a single script (that we provide) and use Hydra to make sure that everything gets correctly logged such that you would be able to exactly report to others how each experiment was configured. In the provided script, the hyperparameters are hardcoded into the code and your job will be to separate them out into a configuration file.</p> <p>Note that we provide an solution (in the <code>vae_solution</code> folder) that can help you get through the exercise, but try to look online for your answers before looking at the solution. Remember: its not about the result, its about the journey.</p> <ol> <li> <p>Start by install hydra: <code>pip install hydra-core --upgrade</code></p> </li> <li> <p>Next take a look at the <code>vae_mnist.py</code> and <code>model.py</code> file and understand what is going on. It is a model we will     revisit during the course.</p> </li> <li> <p>Identify the key hyperparameters of the script. Some of them should be easy to find, but at least 3 have made it     into the core part of the code. One essential hyperparameter is also not included in the script but is needed to be     completely reproducible (HINT: the weights of any neural network is initialized at random).</p> </li> <li> <p>Write a configuration file <code>config.yaml</code> where you write down the hyperparameters that you have found</p> </li> <li> <p>Get the script running by loading the configuration file inside your script (using hydra) that incorporates the     hyperparameters into the script. Note: you should only edit the <code>vae_mnist.py</code> file and not the <code>model.py</code> file.</p> </li> <li> <p>Run the script</p> </li> <li> <p>By default hydra will write the results to a <code>outputs</code> folder, with a sub-folder for the day the experiment was     run and further the time it was started. Inspect your run by going over each file the hydra has generated and check     the information has been logged. Can you find the hyperparameters?</p> </li> <li> <p>Hydra also allows for dynamically changing and adding parameters on the fly from the command-line:</p> <ol> <li> <p>Try changing one parameter from the command-line</p> <pre><code>python vae_mnist.py hyperparameters.seed=1234\n</code></pre> </li> <li> <p>Try adding one parameter from the command-line</p> <pre><code>python vae_mnist.py +experiment.stuff_that_i_want_to_add=42\n</code></pre> </li> </ol> </li> <li> <p>By default the file <code>vae_mnist.log</code> should be empty, meaning that whatever you printed to the terminal did not get     picked up by Hydra. This is due to Hydra under the hood making use of the native python     logging package. This means that to also save all printed output     from the script we need to convert all calls to <code>print</code> with <code>log.info</code></p> <ol> <li> <p>Create a logger in the script:</p> <pre><code>import logging\nlog = logging.getLogger(__name__)\n</code></pre> </li> <li> <p>Exchange all calls to <code>print</code> with calls to <code>log.info</code></p> </li> <li> <p>Try re-running the script and make sure that the output printed to the terminal also gets saved to the     <code>vae_mnist.log</code> file</p> </li> </ol> </li> <li> <p>Make sure that your script is fully reproducible. To check this you will need two runs of the script to compare.     Then run the <code>reproducibility_tester.py</code> script as</p> <pre><code>python reproducibility_tester.py path/to/run/1 path/to/run/2\n</code></pre> <p>the script will go over trained weights to see if the match and that the hyperparameters was the same. Note: for the script to work, the weights should be saved to a file called <code>trained_model.pt</code> (this is the default of the <code>vae_mnist.py</code> script, so only relevant if you have changed the saving of the weights)</p> </li> <li> <p>Finally, make a new experiment using a new configuration file where you have changed a hyperparameter of your own     choice. You are not allowed to change the configuration file in the script but should instead be able to provide it     as an argument when launching the script e.g. something like</p> <pre><code>python vae_mnist.py experiment=exp2\n</code></pre> <p>We recommend that you use a file structure like this</p> <pre><code>|--conf\n|  |--config.yaml\n|  |--experiments\n|     |--exp1.yaml\n|     |--exp2.yaml\n|--my_app.py\n</code></pre> </li> </ol>"},{"location":"s3_reproducibility/config_files/#final-exercise","title":"Final exercise","text":"<p>Make your MNIST code reproducible! Apply what you have just done to the simple script to your MNIST code. Only requirement is that you this time use multiple configuration files, meaning that you should have at least one <code>model_conf.yaml</code> file and a <code>training_conf.yaml</code> file that separates out the hyperparameters that have to do with the model definition and those that have to do with the training. You can also choose to work with even more complex config setups: in the image below the configuration has two layers such that we individually can specify hyperparameters belonging to a specific model architecture and hyperparameters for each individual optimizer that we may try.</p> <p> </p>  Image credit"},{"location":"s3_reproducibility/docker/","title":"M9 - Docker","text":""},{"location":"s3_reproducibility/docker/#docker","title":"Docker","text":"<p>Core Module</p> <p></p>  Image credit  <p>While the above picture may seem silly at first, it is actually pretty close to how docker came to existence. A big part of creating a MLOps pipeline, is that you are able to reproduce it. Reproducibility goes beyond versioning our code with <code>git</code> and using <code>conda</code> environment to keep track of our python installations. To really get reproducibility we need to also capture also system level components like</p> <ul> <li>operating system</li> <li>software dependencies (other than python packages)</li> </ul> <p>Docker provides this kind of system-level reproducibility by creating isolated programs dependencies. In addition to docker providing reproducibility, one of the key features are also scaleability which is important when we later on are going to discuss deployment. Because docker is system-level reproducible, it does not (conceptually) matter if we try to start our program on a single machine or a 1000 machines at once.</p>"},{"location":"s3_reproducibility/docker/#docker-overview","title":"Docker overview","text":"<p>Docker has three main concepts: docker file, docker image and docker container:</p> <p></p> <ul> <li> <p>A docker file is a basic text document that contains all the commands a user could call on the command line to     run an application. This includes installing dependencies, pulling data from online storage, setting up code and     what commands that you want to run (e.g. <code>python train.py</code>)</p> </li> <li> <p>Running, or more correctly building a docker file will create a docker image. An image is a lightweight,     standalone/containerized, executable package of software that includes everything (application code, libraries,     tools, dependencies etc.) necessary to make an application run.</p> </li> <li> <p>Actually running an image will create a docker container. This means that the same image can be launched     multiple times, creating multiple containers.</p> </li> </ul> <p>The exercises today will focus on how to construct the actual docker file, as this is the first step to constructing your own container.</p>"},{"location":"s3_reproducibility/docker/#docker-sharing","title":"Docker sharing","text":"<p>The whole point of using docker is that sharing applications becomes much easier. In general, we have two options</p> <ul> <li> <p>After creating the <code>Dockerfile</code> we can simply commit it to github (its just a text file) and then ask other users     to simple build the image by themselves.</p> </li> <li> <p>After building the image ourself, we can choose to upload it to a image registry such as     Docker Hub where other can get our image by simply running <code>docker pull</code>, making them     able to instantaneous running it as a container, as shown in the figure below</p> </li> </ul> <p></p>  Image credit"},{"location":"s3_reproducibility/docker/#exercises","title":"\u2754 Exercises","text":"<p>In the following exercises we guide you how to build a docker file for your MNIST repository that will make the training and prediction a self contained application. Please make sure that you somewhat understand each step and do not just copy of the exercise. Also note that you probably need to execute the exercise from a elevated terminal e.g. with administrative privilege.</p> <p>The exercises today are only an introduction to docker and some of the steps are going to be unoptimized from a production setting view. For example we often want to keep the size of docker image as small as possible, which we are not focusing on for these exercises.</p> <p>If you are using <code>VScode</code> then we recommend install the docker VScode extension for easy getting an overview of which images have been build and which are running. Additionally the extension named Dev Containers may also be beneficial for you to download.</p> <ol> <li> <p>Start by installing docker. How much trouble that you need to go through     depends on your operating system. For Windows and Mac we recommend they install Docker desktop, which comes with     a graphical user interface (GUI) for quickly viewing docker images and docker containers currently build/in-use.     Windows users that have not installed WSL yet are going to have to do it now (as docker need it as backend for     starting virtual machines) but you do not need to install docker in WSL. After installing docker we recommend that     you restart you laptop.</p> </li> <li> <p>Try running the following to confirm that your installation is working:</p> <pre><code>docker run hello-world\n</code></pre> </li> </ol> <p>which should give the message</p> <pre><code>```bash\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n```\n</code></pre> <ol> <li> <p>Next lets try to download a image from docker hub. Download the <code>busybox</code> image:</p> <pre><code>docker pull busybox\n</code></pre> <p>which is an very small (1-5Mb) containerized application that contains the most essential GNU fileutils, shellutils etc.</p> </li> <li> <p>After pulling the image, write</p> <pre><code>docker images\n</code></pre> <p>which should show you all images that are available. You should see the <code>busybox</code> image that we just downloaded.</p> </li> <li> <p>Lets try to run this image</p> <pre><code>docker run busybox\n</code></pre> <p>you will get that nothing happens! The reason for that is we did that not provide any commands to <code>docker run</code>. We essentially just ask it to start the <code>busybox</code> virtual machine, do nothing and then close it again. Now, try again this time with</p> <pre><code>docker run busybox echo \"hello from busybox\"\n</code></pre> <p>Note how fast this process is. In just a few seconds, Docker is able to start a virtual machine, execute a command and kill it afterwards.</p> </li> <li> <p>Try running</p> <pre><code>docker ps\n</code></pre> <p>what does this command do? What if you add <code>-a</code> to the end?</p> </li> <li> <p>If we wanted to run multiple commands within the virtual machine, we can     start it in interactive mode</p> <pre><code>docker run -it busybox\n</code></pre> <p>this can be a great way to investigate what the filesystem of our virtual machine looks like.</p> </li> <li> <p>As you may have already notice by now, each time we execute <code>docker run</code> we     can still see small remnants of the containers using <code>docker ps -a</code>. These     stray containers can end up take a lot of disk space. To remove them, use     <code>docker rm</code> where you provide the container id that you want to delete</p> <pre><code>docker rm &lt;container_id&gt;\n</code></pre> </li> <li> <p>Lets, now move on to trying to construct an docker file ourself for our     MNIST project. Create a file called <code>trainer.dockerfile</code>. The intention is that we want to develop one dockerfile     for running our training script and one for doing predictions.</p> </li> <li> <p>Instead of starting from scratch we nearly always want to start from some base image. For this exercise we are     going to start from a simple <code>python</code> image. Add the following to your <code>Dockerfile</code></p> <pre><code># Base image\nFROM python:3.9-slim\n</code></pre> </li> <li> <p>Next we are going to install some essentials in our image. The essentials more or less consist of a python     installation. These instructions may seem familiar if you are using linux:</p> <pre><code># install python\nRUN apt update &amp;&amp; \\\napt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\napt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> </li> <li> <p>The previous two steps are common for any docker application where you want to run python. All the remaining steps     are application specific (to some degree):</p> <ol> <li> <p>Lets copy over our application (the essential parts) from our computer to the container:</p> <pre><code>COPY requirements.txt requirements.txt\nCOPY setup.py setup.py\nCOPY src/ src/\nCOPY data/ data/\n</code></pre> <p>Remember that we only want the essential parts to keep our docker image as small as possible. Why do we need each of these files/folders to run training in our docker container?</p> </li> <li> <p>Lets set the working directory in our container and add commands that install the dependencies:</p> <pre><code>WORKDIR /\nRUN pip install -r requirements.txt --no-cache-dir\n</code></pre> <p>the <code>--no-cache-dir</code> is quite important. Can you explain what it does and why it is important in relation to docker.</p> </li> <li> <p>Finally, we are going to name our training script as the entrypoint for our docker image. The entrypoint is     the application that we want to run when the image is being executed:</p> <pre><code>ENTRYPOINT [\"python\", \"-u\", \"src/models/train_model.py\"]\n</code></pre> <p>the <code>\"u\"</code> here makes sure that any output from our script e.g. any <code>print(...)</code> statements gets redirected to our terminal. If not included you would need to use <code>docker logs</code> to inspect your run.</p> </li> </ol> </li> <li> <p>We are now ready to building our docker file into a docker image</p> <pre><code>docker build -f trainer.dockerfile . -t trainer:latest\n</code></pre> MAC M1/M2 users <p>There is a good chance that it docker build will not work out of the box for you, because M1/M2 chips use another build architechture. Thus you need to specify the platform that you want to build for. This can be done by adding the following to your <code>FROM</code> statement:</p> <pre><code>FROM --platform=linux/amd64 python:3.9-slim\n</code></pre> <p>and in you build step you need to add <code>--platform linux/amd64</code> to the command.</p> <p>please note here we are providing two extra arguments to <code>docker build</code>. The <code>-f train.dockerfile .</code> (the dot is important to remember) indicates which dockerfile that we want to run (except if you named it just <code>Dockerfile</code>) and the <code>-t trainer:latest</code> is the respective name and tag that we se afterwards when running <code>docker images</code> (see image below). Please note that building a docker image can take a couple of minutes.</p> <p> </p> </li> <li> <p>Try running <code>docker images</code> and confirm that you get output similar to the one above. If you succeeds with this,     then try running the docker image</p> <pre><code>docker run --name experiment1 trainer:latest\n</code></pre> <p>you should hopefully see your training starting. Please note that we can start as many containers that we want at the same time by giving them all different names using the <code>--name</code> tag.</p> </li> <li> <p>Remember, if you ever are in doubt how files are organized inside a docker image you always have the option to start     the image in interactive mode:</p> <pre><code>docker run -it --entrypoint sh {image_name}:{image_name}\n</code></pre> </li> <li> <p>When your training has completed you will notice that any files that is created when running your training script is     not present on your laptop (for example if your script is saving the trained model to file). This is because the     files were created inside your container (which is its own little machine). To get the files you have two options:</p> <ol> <li> <p>If you already have a completed run then you can use</p> <pre><code>docker cp\n</code></pre> <p>to copy the files between your container and laptop. For example to copy a file called <code>trained_model.pt</code> from a folder you would do:</p> <pre><code>docker cp {container_name}:{dir_path}/{file_name} {local_dir_path}/{local_file_name}\n</code></pre> <p>Try this out.</p> </li> <li> <p>A much more efficient strategy is to mount a volume that is shared between the host (your laptop) and the     container. This can be done with the <code>-v</code> option for the <code>docker run</code> command. For example, if we want to     automatically get the <code>trained_model.pt</code> file after running our training script we could simply execute the     container as</p> <pre><code>docker run --name {container_name} -v %cd%/models:/models/ trainer:latest\n</code></pre> <p>this command mounts our local <code>models</code> folder as a corresponding <code>models</code> folder in the container. Any file save by the container to this folder will be synchronized back to our host machine. Try this out! Note if you have multiple files/folders that you want to mount (if in doubt about file organization in the container try to do the next exercise first). Also note that the <code>%cd%</code> need to change depending on your OS, see this page for help.</p> </li> </ol> </li> <li> <p>With training done we also need to write an application for prediction. Create a new docker image called     <code>predict.dockerfile</code>. This file should call your <code>src/models/predict_model.py</code> script instead. This image will     need some trained model weights to work. Feel free to either includes these during the build process or mount them     afterwards. When you When you created the file try to <code>build</code> and <code>run</code> it to confirm that it works. Hint: if you     are passing in the model checkpoint and prediction data as arguments to your script, your <code>docker run</code> probably     need to look something like</p> <pre><code>docker run --name predict --rm \\\n-v %cd%/trained_model.pt:/models/trained_model.pt \\  # mount trained model file\n-v %cd%/data/example_images.npy:/example_images.npy \\  # mount data we want to predict on\npredict:latest \\\n../../models/trained_model.pt \\  # argument to script, path relative to script location in container\n../../example_images.npy\n</code></pre> </li> <li> <p>(Optional, requires GPU support) By default a virtual machine created by docker only have access to your <code>cpu</code> and     not your <code>gpu</code>. While you do not necessarily have a laptop with a GPU that supports training of neural network     (e.g. one from Nvidia) it is beneficial that you understand how to construct a docker image that can take advantage     of a GPU if you were to run this on a machine in the future that have a GPU (e.g. in the cloud). It does take a bit     more work, but many of the steps will be similar to building a normal docker image.</p> <ol> <li> <p>There are three prerequisites for working with Nvidia GPU accelerated docker containers. First you need to have     the Docker Engine installed (already taken care of), have Nvidia GPU with updated GPU drivers and finally have     the Nvidia container toolkit     installed. The last part you not likely have not installed and needs to do. Some distros of Linux have known     problems with the installation process, so you may have to search through known issues in     nvidia-docker repository to find a solution</p> </li> <li> <p>To test that everything is working start by pulling a relevant Nvidia docker image. In my case this is     the correct image:</p> <pre><code>docker pull nvidia/cuda:11.0.3-base-ubuntu20.04\n</code></pre> <p>but it may differ based on what cuda vision you have. You can find all the different offical Nvidia images here. After pulling the image, try running the <code>nvidia-smi</code> command inside a container based on the image you just pulled. It should look something like this:</p> <pre><code>docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi\n</code></pre> <p>and should show an image like below:</p> <p> </p> <p>If it does not work, try redoing the steps.</p> </li> <li> <p>We should hopefully have a working setup now for running Nvidia accelerated docker containers. Next step is to     get Pytorch inside of our container, such that our Pytorch implementation also correctly identify the GPU.     Luckily for us Nvidia provides a set of docker images for GPU-optimized software for AI, HPC and visualizations     through their NGC Catalog.     The containers that have to do with Pytorch can be seen     here. Try pulling the latest:</p> <pre><code>docker pull nvcr.io/nvidia/pytorch:22.07-py3\n</code></pre> <p>It may take some time, because the NGC images includes a lot of other software for optimizing Pytorch applications. It may be possible for you to find other images for running GPU accelerated applications that have a smaller memory footprint, but NGC are the recommend and supported way.</p> </li> <li> <p>Lets test that this container work:</p> <pre><code>docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:22.07-py3\n</code></pre> <p>this should run the container in interactive mode attached to your current terminal. Try opening <code>python</code> in the container and try writing:</p> <pre><code>import torch\nprint(torch.cuda.is_available())\n</code></pre> <p>which hopefully should return <code>True</code>.</p> </li> <li> <p>Finally, we need to incorporate all this into our already developed docker files for our application. This is     also fairly easy as we just need to change our <code>FROM</code> statement in the beginning of our docker file:</p> <pre><code>FROM python:3.7-slim\n</code></pre> <p>change to</p> <pre><code>FROM  nvcr.io/nvidia/pytorch:22.07-py3\n</code></pre> <p>try doing this to one of your docker files, build the image and run the container. Remember to check that your application is using GPU by printing <code>torch.cuda.is_available()</code>.</p> </li> </ol> </li> </ol>"},{"location":"s3_reproducibility/docker/#knowledge-check","title":"Knowledge check","text":"Knowledge question 1 <p>What is the difference between a docker image and a docker container?</p> Solution <p>A docker image is a template for a docker container. A docker container is a running instance of a docker image. A docker image is a static file, while a docker container is a running process.</p> Knowledge question 2 <p>What advantage is there to running your application inside a docker container instead of running the application directly on your machine?</p> Solution <p>Running inside a docker container gives you a consistent and independent environment for your application. This means that you can be sure that your application will run the same way on your machine as it will on another machine. Thus, docker gives the ability to abstract away the differences between different machines.</p> <p>The covers the absolute minimum you should know about docker to get a working image and container. That said, if you are actively going to be using docker in the near future, one thing to consider is the image size. Even these simple images that we have build still takes up GB in size. A number of optimizations steps can be taken to reduce the image size for you or your end user. If you have time you can read this article on different approaches to reduce image size. Additionally, you can take a look at the dive-in extension for docker desktop that lets you explore in depth your docker images.</p>"},{"location":"s4_debugging_and_logging/","title":"Debugging, Profiling, Logging and Boilerplate","text":"<p>Slides</p> <p> </p> <p>Today we are initially going to go other three different topics that are all fundamentally necessary for any data scientist/devops engineer:</p> <ul> <li>Debugging</li> <li>Profiling</li> <li>Logging</li> </ul> <p>All three topics can be characterized by something you probably already is familiar with. Since you started programming, you have done debugging as nobody can write perfect code in the first try. Similarly, while you have not directly profiled your code, I bet that you at some point have had some very slow code and optimized it to run faster. Identifying and improving is the fundamentals of profiling code. Finally, logging is a very broad term and basically refers to any kind of output from your applications that help you at a later point identify the \"performance\" of you application.</p> <p>However, while we expect you to already be familiar with these topics, we do not expect all of you to be expects in this as it is very rarely topics that are focused on. Today we are going to introduce some best practices and tools to help you overcome each and everyone of these three important topics.</p> <p>As the final topic for today we are going to learn about how we can minimize boilerplate and focus on coding what actually matters for our project instead of all the boilerplate to get it working.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the basics of debugging and how to use a debugger to find bugs in your code</li> <li>Can use a profiler to identify bottlenecks in your code</li> <li>Familiar with an experiment logging framework for tracking experiments and hyperparameters of your code</li> <li>Be able to use <code>pytorch-lightning</code> to minimize boilerplate code and structure deep learning models</li> </ul>"},{"location":"s4_debugging_and_logging/boilerplate/","title":"M14 - Boilerplate","text":""},{"location":"s4_debugging_and_logging/boilerplate/#minimizing-boilerplate","title":"Minimizing boilerplate","text":"<p>Boilerplate is a general term that describes any standardized text, copy, documents, methods, or procedures that may be used over again without making major changes to the original. But how does this relate to doing machine learning projects? If you have already tried doing a couple of projects within machine learning you will probably have seen a pattern: every project usually consist of these three aspects of code:</p> <ul> <li>a model implementation</li> <li>some training code</li> <li>a collection of utilities for saving models, logging images etc.</li> </ul> <p>While the latter two certainly seems important, in most cases the actual development or research often revolves around defining the model. In this sense, both the training code and the utilities becomes boilerplate that should just carry over from one project to another. But the problem usually is that we have not generalized our training code to take care of the small adjusted that may be required in future projects and we therefore end up implementing it over and over again every time that we start a new project. This is of course a waste of our time that we should try to find a solution to.</p> <p>This is where high-level frameworks comes into play. High-level frameworks are build on top of another framework (Pytorch in this case) and tries to abstract/standardize how to do particular tasks such as training. At first it may seem irritating that you need to comply to someone else code structure, however there is a good reason for that. The idea is that you can focus on what really matters (your task, model architecture etc.) and do not have to worry about the actual boilerplate that comes with it.</p> <p>\\ The most popular high-level (training) frameworks within the <code>Pytorch</code> ecosystem are:</p> <ul> <li>fast.ai</li> <li>Ignite</li> <li>skorch</li> <li>Catalyst</li> <li>Composer</li> <li>Pytorch Lightning</li> </ul> <p>They all offer many of the same features, so choosing one over the other for most projects should not matter that much. We are here going to use <code>Pytorch Lightning</code>, as it offers all the functionality that we are going to need later in the course.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#pytorch-lightning","title":"Pytorch Lightning","text":"<p>In general we refer to the documentation from Pytorch lightning if in doubt about how to format your code for doing specific tasks. We are here going to explain the key concepts of the API that you need to understand to use the framework, starting with the <code>LightningModule</code> and the <code>Trainer</code>.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#lightningmodule","title":"LightningModule","text":"<p>The <code>LightningModule</code> is a subclass of a standard <code>nn.Module</code> that basically adds additional structure. In addition to the standard <code>__init__</code> and <code>forward</code> methods that need to be implemented in a <code>nn.Module</code>, a <code>LightningModule</code> further requires two more methods implemented:</p> <ul> <li> <p><code>training_step</code>: should contain your actual training code e.g. given a batch of data this should return the loss   that you want to optimize</p> </li> <li> <p><code>configure_optimizers</code>: should return the optimizer that you want to use</p> </li> </ul> <p>Below is shown these two methods added to standard MNIST classifier</p> <p></p> <p>Compared to a standard <code>nn.Module</code>, the additional methods in the <code>LightningModule</code> basically specifies exactly how you want to optimize your model.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#trainer","title":"Trainer","text":"<p>The second component to lightning is the <code>Trainer</code> object. As the name suggest, the `Trainer object takes care of the actual training, automizing everything that you do not want to worry about.</p> <pre><code>from pytorch_lightning import Trainer\nmodel = MyAwesomeModel()  # this is our LightningModule\ntrainer = Trainer()\ntraier.fit(model)\n</code></pre> <p>Thats is essentially all that you need to specify in lightning to have a working model. The trainer object does not have methods that you need to implement yourself, but it have a bunch of arguments that can be used to control how many epochs that you want to train, if you want to run on gpu ect. To get the training of our model to work we just need to specify how our data should be feed into the lighning framework.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#data","title":"Data","text":"<p>For organizing our code that has to do with data in <code>Lightning</code> we essentially have three different options. However, all three assume that we are using <code>torch.utils.data.DataLoader</code> for the dataloading.</p> <ol> <li>If we already have a <code>train_dataloader</code> and possible also a <code>val_dataloader</code> and <code>test_dataloader</code> defined we can    simply add them to our <code>LightningModule</code> using the similar named methods:</li> </ol> <pre><code>def train_dataloader(self):\nreturn DataLoader(...)\ndef val_dataloader(self):\nreturn DataLoader(...)\ndef test_dataloader(self):\nreturn DataLoader(...)\n</code></pre> <ol> <li>Maybe even simplier, we can directly feed such dataloaders in the <code>fit</code> method of the <code>Trainer</code> object:</li> </ol> <pre><code>trainer.fit(model, train_dataloader, val_dataloader)\ntrainer.test(model, test_dataloader)\n</code></pre> <ol> <li>Finally, <code>Lightning</code> also have the <code>LightningDataModule</code> that organizes data loading into a single structure, see    this page for more info. Putting    data loading into a <code>DataModule</code> makes sense as it is then can be reused between projects.</li> </ol>"},{"location":"s4_debugging_and_logging/boilerplate/#callbacks","title":"Callbacks","text":"<p>Callbacks is one way to add additional functionality to your model, that strictly speaking is not already part of your model. Callbacks should therefore be seen as self-contained feature that can be reused between projects. You have the option to implement callbacks yourself (by inheriting from the <code>pytorch_lightning.callbacks.Callback</code> base class) or use one of the build in callbacks. Of particular interest are <code>ModelCheckpoint</code> and <code>EarlyStopping</code> callbacks:</p> <ul> <li>The <code>ModelCheckpoint</code> makes sure to save checkpoints of you model. This is in pricipal not hard to do yourself, but   the <code>ModelCheckpoint</code> callback offers additional functionality by saving checkpoints only when some metric improves,   or only save the best <code>K</code> performing models ect.</li> </ul> <pre><code>model = MyModel()\ncheckpoint_callback = ModelCheckpoint(\ndirpath=\"./models\", monitor=\"val_loss\", mode=\"min\"\n)\ntrainer = Trainer(callbacks=[checkpoint_callbacks])\ntrainer.fit(model)\n</code></pre> <ul> <li>The <code>EarlyStopping</code> callback can help you prevent overfitting by automatically stopping the training if a certain   value is not improving anymore:</li> </ul> <pre><code>model = MyModel()\nearly_stopping_callback = EarlyStopping(\nmonitor=\"val_loss\", patience=3, verbose=True, mode=\"min\"\n)\ntrainer = Trainer(callbacks=[early_stopping_callback])\ntrainer.fit(model)\n</code></pre> <p>Multiple callbacks can be used by passing them all in a list e.g.</p> <pre><code>trainer = Trainer(callbacks=[checkpoint_callbacks, early_stopping_callback])\n</code></pre>"},{"location":"s4_debugging_and_logging/boilerplate/#exercises","title":"\u2754 Exercises","text":"<p>Please note that the in following exercise we will basically ask you to reformat all your MNIST code to follow the lightning standard, such that we can take advantage of all the tricks the framework has to offer. The reason we did not implement our model in <code>lightning</code> to begin with, is that to truly understand why it is beneficially to use a high-level framework to do some of the heavy lifting you need to have gone through some of implementation troubles yourself.</p> <ol> <li> <p>Convert your corrupted MNIST model into a <code>LightningModule</code>. You can either choose to completly override your old    model or implement it in a new file. The bare minimum that you need to add while converting to get it working with    the rest of lightning:</p> </li> <li> <p>The <code>training_step</code> method. This function should contain essentially what goes into a single    training step and should return the loss at the end</p> </li> <li> <p>The <code>configure_optimizers</code> method</p> </li> </ol> <p>Please read the documentation    for more info.</p> <ol> <li> <p>Make sure your data is formatted such that it can be loaded using the <code>torch.utils.data.DataLoader</code> object.</p> </li> <li> <p>Instantiate a <code>Trainer</code> object. It is recommended to take a look at the    trainer arguments (there    are many of them) and maybe adjust some of them:</p> </li> <li> <p>Investigate what the <code>default_root_dir</code> flag does</p> </li> <li> <p>As default lightning will run for 1000 epochs. This may be too much (for now). Change this by       changing the appropriate flag. Additionally, there also exist a flag to set the maximum number of steps that we       should train for.</p> </li> <li> <p>To start with we also want to limit the amount of training data to 20% of its original size. which       trainer flag do you need to set for this to work?</p> </li> <li> <p>Try fitting your model: <code>trainer.fit(model)</code></p> </li> <li> <p>Now try adding some <code>callbacks</code> to your trainer.</p> </li> <li> <p>The privous module was all about logging in <code>wandb</code>, so the question is naturally how does <code>lightning</code> support this.    Lightning does not only support <code>wandb</code>, but also many    others. Common for all of them, is that    logging just need to happen through the <code>self.log</code> method in your <code>LightningModule</code>:</p> </li> <li> <p>Add <code>self.log</code> to your `LightningModule. Should look something like this:</p> <pre><code>def training_step(self, batch, batch_idx):\ndata, target = batch\npreds = self(data)\nloss = self.criterion(preds, target)\nacc = (target == preds.argmax(dim=-1)).float().mean()\nself.log('train_loss', loss)\nself.log('train_acc', acc)\nreturn loss\n</code></pre> </li> <li> <p>Add the <code>wandb</code> logger to your trainer</p> <pre><code>trainer = Trainer(logger=pl.loggers.WandbLogger(project=\"dtu_mlops\"))\n</code></pre> <p>and try to train the model. Confirm that you are seeing the scalars appearing in your <code>wandb</code> portal.</p> </li> <li> <p><code>self.log</code> does sadly only support logging scalar tensors. Luckily, for logging other quantities we       can still access the standard <code>wandb.log</code> through our model</p> <pre><code>def training_step(self, batch, batch_idx):\n...\n# self.logger.experiment is the same as wandb.log\nself.logger.experiment.log({'logits': wandb.Histrogram(preds)})\n</code></pre> <p>try doing this, by logging something else than scalar tensors.</p> </li> <li> <p>Finally, we maybe also want to do some validation or testing. In lightning we just need to add the <code>validation_step</code>    and <code>test_step</code> to our lightning module and supply the respective data in form of a separate dataloader. Try to at    least implement one of them.</p> </li> <li> <p>(Optional, requires GPU) One of the big advantages of using <code>lightning</code> is that you no more need to deal with device    placement e.g. called <code>.to('cuda')</code> everywhere. If you have a GPU, try to set the <code>gpus</code> flag in the trainer. If you    do not have one, do not worry, we are going to return to this when we are going to run training in the cloud.</p> </li> <li> <p>(Optional) As default Pytorch uses <code>float32</code> for representing floating point numbers. However, research have shown    that neural network training is very robust towards a decrease in precision. The great benefit going from <code>float32</code>    to <code>float16</code> is that we get approximately half the    memory consumption. Try out half-precision training in    Pytorch lightning. You can enable this by setting the    precision flag in the <code>Trainer</code>.</p> </li> <li> <p>(Optional) Lightning also have build-in support for profiling. Checkout how to do this using the     profiler argument in     the <code>Trainer</code> object.</p> </li> <li> <p>(Optional) Another great feature of Lightning is that the allow for easily defining command line interfaces through     the Lightning CLI feature. The     Lightning CLI is essentially a drop in replacement for defining command line interfaces (covered in     this module) and can also replace the need for config files (covered in     this module) for securing reproducibility when working inside the     Lightning framework. We highly recommend checking out the feature and that you try to refactor your code such that     you do not need to call <code>trainer.fit</code> anymore but it is instead directly controlled from the Lightning CLI.</p> </li> <li> <p>Free exercise: Experiment with what the lightning framework is capable of. Either try out more of the trainer flags,     some of the other callbacks, or maybe look into some of the other methods that can be implemented in your lightning     module. Only your imagination is the limit!</p> </li> </ol> <p>That covers everything for today. It has been a mix of topics that all should help you write \"better\" code (by some objective measure). If you want to deep dive more into the Pytorch lightning framework, we highly recommend looking at the different tutorials in the documentation that covers more advanced models and training cases. Additionally, we also want to highlight other frameworks in the lightning ecosystem:</p> <ul> <li>Torchmetrics: collection of machine learning metrics written   in Pytorch</li> <li>lightning flash: High-level framework for fast prototyping,   baselining, finetuning with a even simpler interface than lightning</li> <li>lightning-bolts: Collection of SOTA pretrained models, model   components, callbacks, losses and datasets for testing out ideas as fast a possible</li> </ul>"},{"location":"s4_debugging_and_logging/debugging/","title":"M11 - Debugging","text":""},{"location":"s4_debugging_and_logging/debugging/#debugging","title":"Debugging","text":"<p>Debugging is very hard to teach and is one of the skills that just comes with experience. That said, there are good and bad ways to debug a program. We are all probably familiar with just inserting <code>print(...)</code> statements everywhere in our code. It is easy and can many times help narrow down where the problem happens. That said, this is not a great way of debugging when dealing with a very large codebase. You should therefore familiarize yourself with the build-in python debugger as it may come in handy during the course.</p> <p></p> <p>To invoke the build in python debugger you can either:</p> <ul> <li> <p>Set a trace directly with the python debugger by calling</p> <pre><code>import pdb\npdb.set_trace()\n</code></pre> <p>anywhere you want to stop the code. Then you can use different commands (see the <code>python_debugger_cheatsheet.pdf</code>) to step through the code.</p> </li> <li> <p>If you are using an editor, then you can insert inline breakpoints (in VS code this can be done by pressing <code>F9</code>)     and then execute the script in debug mode (inline breakpoints can often be seen as small red dots to the left of     your code). The editor should then offer some interface to allow you step through your code. Here is a guide to     using the build in debugger in VScode.</p> </li> <li> <p>Additionally, if your program is stopping on an error and you automatically want to start the debugger where it     happens, then you can simply launch the program like this from the terminal</p> <pre><code>python -m pdb -c continue my_script.py\n</code></pre> </li> </ul>"},{"location":"s4_debugging_and_logging/debugging/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>We here provide a script <code>vae_mnist_bugs.py</code> which contains a number of bugs to get it running. Start by going over the script and try to understand what is going on. Hereafter, try to get it running by solving the bugs. The following bugs exist in the script:</p> <ul> <li>One device bug (will only show if running on gpu, but try to find it anyways)</li> <li>One shape bug</li> <li>One math bug</li> <li>One training bug</li> </ul> <p>Some of the bugs prevents the script from even running, while some of them influences the training dynamics. Try to find them all. We also provide a working version called <code>vae_mnist_working.py</code> (but please try to find the bugs before looking at the script). Successfully debugging and running the script should produce three files:</p> <ul> <li><code>orig_data.png</code> containing images from the standard MNIST training set</li> <li><code>reconstructions.png</code> reconstructions from the model</li> <li><code>generated_samples.png</code> samples from the model</li> </ul> <p>Again, we cannot stress enough that the exercise is actually not about finding the bugs but using a proper debugger to find them.</p>"},{"location":"s4_debugging_and_logging/logging/","title":"M13 - Logging","text":""},{"location":"s4_debugging_and_logging/logging/#experiment-logging","title":"Experiment logging","text":"<p>Core Module</p> <p>Experiment logging or model monitoring is an important part of understanding what is going on with your model. It can help you debug your model and help tweak your models to perfection.</p> <p>The most basic logging we can do is writing the metric that our model is producing to the terminal or a file for later inspection. We can then also use tools such as matplotlib for plotting the training curve. This kind of workflow may be enough when doing smaller experiments or working alone on a project, but there is no way around using a proper experiment tracker and visualizer when doing large scale experiments in collaboration with others. It especially becomes important when you want to compare performance between different runs. Organizing monitoring is the topic of this module.</p> <p>There exist many tools for logging your experiments, with some of them being:</p> <ul> <li>Tensorboard</li> <li>Comet</li> <li>MLFlow</li> <li>Neptune</li> <li>Weights and Bias</li> </ul> <p>All of the frameworks offers many of the same functionalities, you can see a (bias) review here. We are going to use Weights and Bias (wandb), as it support everything we need in this course. Additionally, it is an excellent tool for collaboration and sharing of results.</p>"},{"location":"s4_debugging_and_logging/logging/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by creating an account at wandb. I recommend using your github account but feel    free to choose what you want. When you are logged in you should get an API key of length 40. Copy this for later    use (HINT: if you forgot to copy the API key, you can find it under settings).</p> </li> <li> <p>Next install wandb on your laptop</p> </li> </ol> <pre><code>pip install wandb\n</code></pre> <ol> <li>Now connect to your wandb account</li> </ol> <pre><code>wandb login\n</code></pre> <p>you will be asked to provide the 40 length API key. The connection should be remain open to the wandb server    even when you close the terminal, such that you do not have to login each time. If using <code>wandb</code> in a notebook    you need to manually close the connection using <code>wandb.finish()</code>.</p> <ol> <li> <p>With it all setup we are now ready to incorporate <code>wandb</code> into our code. The interface is fairly simple, and    this guide should give enough hints to get you through    the exercise. (HINT: the two methods you need to call are <code>wandb.init</code> and <code>wandb.log</code>). To start with, logging    the training loss of your model will be enough.</p> </li> <li> <p>After running your model, checkout the webpage. Hopefully you should be able to see at least one run with something    logged.</p> </li> <li> <p>Now log something else than scalar values. This could be a image, a histogram or a matplotlib figure. In all    cases the logging is still going to use <code>wandb.log</code> but you need extra calls to <code>wandb.Image</code> ect. depending    on what you choose to log.</p> </li> <li> <p>Finally, lets create a report that you can share. Click the Create report button and include some of the    graphs/plots/images that you have generated in the report.</p> </li> <li> <p>To make sure that you have completed todays exercises, make the report shareable by clicking the Share button    and create view-only-link. Send the link to my email <code>nsde@dtu.dk</code>, so I can checkout your awesome work \ud83d\ude03</p> </li> <li> <p>When calling <code>wandb.init</code> you have two arguments called <code>project</code> and <code>entity</code>. Make sure that you understand these    and try them out. It will come in handy for your group work as they essentially allows multiple users to upload their    own runs to the same project in <code>wandb</code>.</p> </li> <li> <p>Wandb also comes with build in feature for doing hyperparameter sweeping     which can be beneficial to get a better working model. Look through the documentation on how to do a hyperparameter     sweep in Wandb. You at least need to create a new file called <code>sweep.yaml</code> and make sure that you call <code>wandb.log</code>     in your code on an appropriate value. Note: if you want <code>hydra</code> and <code>wandb</code> to work together you will need to change     the <code>command</code> config in your <code>sweep.yaml</code> file, see this     page.</p> </li> <li> <p>In the future it will be important for us to be able to run Wandb inside a docker container (together with whatever     training or inference we specify). The problem here is that we cannot authenticate Wandb in the same way as the     previous exercise, it needs to happen automatically. Lets therefore look into how we can do that.</p> <ol> <li> <p>First we need to generate an authentication key, or more precise an API key. This is in general the way any    service (like a docker container) can authenticate. Start by going https://wandb.ai/home, click your profile    icon in the upper right corner and then go to settings. Scroll down to the danger zone and generate a new API    key and finally copy it.</p> </li> <li> <p>Next create a new docker file called <code>wandb.docker</code> and add the following code</p> </li> </ol> <pre><code>FROM python:3.9\nRUN apt update &amp;&amp; \\\napt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\napt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip install wandb\nCOPY s4_debugging_and_logging/exercise_files/wandb_tester.py wandb_tester.py\nENTRYPOINT [\"python\", \"-u\", \"wandb_tester.py\"]\n</code></pre> <p>please take a look at the script being copied into the image and afterwards build the docker image.</p> <ol> <li>When we want to run the image, what we need to do is including a environment variables that contains the API key    we generated. This will then autheticate the docker container with the wandb server:</li> </ol> <pre><code>docker run -e WANDB_API_KEY=&lt;your-api-key&gt; wandb:latest\n</code></pre> <p>Try running it an confirm that the results are uploaded to the wandb server.</p> </li> <li> <p>Feel free to experiment more with <code>wandb</code> as it is a great tool for logging, organizing and sharing experiments.</p> </li> </ol> <p>That is the module on logging. Please note that at this point in the course you will begin to see some overlap between the different frameworks. While we mainly used <code>hydra</code> for configuring our python scripts it can also be used to save metrics and hyperparameters similar to how <code>wandb</code> can. Similar arguments holds for <code>dvc</code> which can also be used to log metrics. In our opinion <code>wandb</code> just offers a better experience when interacting with the results after logging. We want to stress that the combination of tools presented in this course may not be the best for all your future projects, and we recommend finding a setup that fits you. That said, each framework provide specific features that the others does not.</p> <p>\\ Finally, we want to note that we during the course really try to showcase a lot of open source frameworks, Wandb is not one. It is free to use for personal usage (with a few restrictions) but for enterprise it does require a license. If you are eager to only work with open-source tools we highly recommend trying out MLFlow which offers the same overall functionalities as Wandb.</p>"},{"location":"s4_debugging_and_logging/profiling/","title":"M12 - Profiling","text":""},{"location":"s4_debugging_and_logging/profiling/#profilers","title":"Profilers","text":"<p>Core Module</p>"},{"location":"s4_debugging_and_logging/profiling/#profilers_1","title":"Profilers","text":"<p>In general profiling code is about improving the performance of your code. In this session we are going to take a somewhat narrow approach to what \"performance\" is: runtime, meaning the time it takes to execute your program.</p> <p>At the bare minimum, the two questions a proper profiling of your program should be able to answer is:</p> <ul> <li>\u201c How many times is each method in my code called?\u201d</li> <li>\u201c How long do each of these methods take?\u201d</li> </ul> <p>The first question is important to priorities optimization. If two methods <code>A</code> and <code>B</code> have approximately the same runtime, but <code>A</code> is called 1000 more times than <code>B</code> we should probably spend time optimizing <code>A</code> over <code>B</code> if we want to speedup our code. The second question is gives itself, directly telling us which methods are the expensive to call.</p> <p>Using profilers can help you find bottlenecks in your code. In this exercise we will look at two different profilers, with the first one being the cProfile. <code>cProfile</code> is pythons build in profiler that can help give you an overview runtime of all the functions and methods involved in your programs.</p>"},{"location":"s4_debugging_and_logging/profiling/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Run the <code>cProfile</code> on the <code>vae_mnist_working.py</code> script. Hint: you can directly call the profiler on a     script using the <code>-m</code> arg</p> <pre><code>python -m cProfile -o &lt;output_file&gt; -s &lt;sort_order&gt; myscript.py\n</code></pre> </li> <li> <p>Try looking at the output of the profiling. Can you figure out which function took the longest to run?</p> </li> <li> <p>Can you explain the difference between <code>tottime</code> and <code>cumtime</code>? Under what circumstances does these differ and     when are they equal.</p> </li> <li> <p>To get a better feeling of the profiled result we can try to visualize it. Python does not     provide a native solution, but open-source solutions such as snakeviz     exist. Try installing <code>snakeviz</code> and load a profiled run into it (HINT: snakeviz expect the run to have the file     format <code>.prof</code>).</p> </li> <li> <p>Try optimizing the run! (Hint: The data is not stored as torch tensor). After optimizing the code make sure     (using <code>cProfile</code> and <code>snakeviz</code>) that the code actually runs faster.</p> </li> </ol>"},{"location":"s4_debugging_and_logging/profiling/#pytorch-profiling","title":"Pytorch profiling","text":"<p>Profiling machine learning code can become much more complex because we are suddenly beginning to mix different devices (CPU+GPU), that can (and should) overlap some of their computations. When profiling this kind of machine learning code we are often looking for bottlenecks. A bottleneck is simple the place in your code that is preventing other processes from performing their best. This is the reason that all major deep learning frameworks also include their own profilers that can help profiling more complex applications.</p> <p>The image below show a typical report using the build in profiler in pytorch. As the image shows the profiler looks both a the <code>kernel</code> time (this is the time spend doing actual computations) and also transfer times such as <code>memcpy</code> (where we are copying data between devices). It can even analyze your code and give recommendations.</p> <p></p> <p>Using the profiler can be as simple as wrapping the code that you want to profile with the <code>torch.profiler.profile</code> decorator</p> <pre><code>with torch.profiler.profile(...) as prof:\n# code that I want to profile\noutput = model(data)\n</code></pre>"},{"location":"s4_debugging_and_logging/profiling/#exercises_1","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>In these investigate the profiler that is build into PyTorch already. Note that these exercises requires that you have PyTorch v1.8.1 installed (or higher). You can always check which version you currently have installed by writing (in a python interpreter):</p> <pre><code>import torch\nprint(torch.__version__)\n</code></pre> <p>But we always recommend to update to the latest Pytorch version for the best experience. Additionally, to display the result nicely (like <code>snakeviz</code> for <code>cProfile</code>) we are also going to use the tensorboard profiler extension</p> <pre><code>pip install torch_tb_profiler\n</code></pre> <ol> <li> <p>A good starting point is too look at the API for the profiler. Here     the important class to look at is the <code>torch.profiler.profile</code> class.</p> </li> <li> <p>Lets try out an simple example (taken from     here):</p> <ol> <li> <p>Try to run the following code</p> <pre><code>import torch\nimport torchvision.models as models\nfrom torch.profiler import profile, ProfilerActivity\nmodel = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\nmodel(inputs)\n</code></pre> <p>this will profile the <code>forward</code> pass of Resnet 18 model.</p> </li> <li> <p>Running this code will produce an <code>prof</code> object that contains all the relevant information about the profiling.     Try writing the following code:</p> <pre><code>print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n</code></pre> <p>what operation is taking most of the cpu?</p> </li> <li> <p>Try running</p> <pre><code>print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=30))\n</code></pre> <p>can you see any correlation between the shape of the input and the cost of the operation?</p> </li> <li> <p>(Optional) If you have a GPU you can also profile the operations on that device:</p> <pre><code>with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\nmodel(inputs)\n</code></pre> </li> <li> <p>(Optional) As an alternative to using <code>profile</code> as an     context-manager we can also use its <code>.start</code> and     <code>.stop</code> methods:</p> <pre><code>prof = profile(...)\nprof.start()\n...  # code I want to profile\nprof.stop()\n</code></pre> <p>Try doing this on the above example.</p> </li> </ol> </li> <li> <p>The <code>torch.profiler.profile</code> function takes some additional arguments. What argument would you need to     set to also profile the memory usage? (Hint: this page)     Try doing it to the simple example above and make sure to sort the sample by <code>self_cpu_memory_usage</code>.</p> </li> <li> <p>As mentioned we can also get a graphical output for better inspection. After having done a profiling     try to export the results with:</p> <pre><code>prof.export_chrome_trace(\"trace.json\")\n</code></pre> <p>you should be able to visualize the file by going to <code>chrome://tracing</code> in any chromium based web browser. Can you still identify the information printed in the previous exercises from the visualizations?</p> </li> <li> <p>Running profiling on a single forward step can produce misleading results as it only provides a single sample that     may depend on what background processes that are running on your computer. Therefore it is recommended to profile     multiple iterations of your model. If this is the case then we need to include <code>prof.step()</code> to tell the profiler     when we are doing a new iteration</p> <pre><code>with profile(...) as prof:\nfor i in range(10):\nmodel(inputs)\nprof.step()\n</code></pre> <p>Try doing this. Is the conclusion this the same on what operations that are taken up most of the time? Have the percentage changed significantly?</p> </li> <li> <p>Additionally, we can also visualize the profiling results using the profiling viewer in tensorboard.</p> <ol> <li> <p>Start by initializing the <code>profile</code> class with an additional argument:</p> <pre><code>from torch.profiler import profile, tensorboard_trace_handler\nwith profile(..., on_trace_ready=tensorboard_trace_handler(\"./log/resnet18\")) as prof:\n...\n</code></pre> <p>Try run a profiling (using a couple of iterations) and make sure that a file with the <code>.pt.trace.json</code> is produced in the <code>log/resnet18</code> folder.</p> </li> <li> <p>Now try launching tensorboard</p> <pre><code>tensorboard --logdir=./log\n</code></pre> <p>and open the page http://localhost:6006/#pytorch_profiler, where you should hopefully see an image similar to the one below:</p> <p>  Image credit  </p> <p>Try poking around in the interface.</p> </li> <li> <p>Tensorboard have a nice feature for comparing runs under the <code>diff</code> tab. Try redoing a profiling run but use     <code>model = models.resnet34()</code> instead. Load up both runs and try to look at the <code>diff</code> between them.</p> </li> </ol> </li> <li> <p>As an final exercise, try to use the profiler on the <code>vae_mnist_working.py</code> file from the previous module on     debugging, where you profile a hole training run (not only the forward pass). What is the bottleneck during the     training? Is it still the forward pass or is it something else? Can you improve the code somehow based on the     information from the profiler.</p> </li> </ol> <p>This end the module on profiling. If you want to go into more details on this topic we can recommend looking into line_profiler and kernprof. A downside of using python's <code>cProfile</code> is that it can only profiling at an functional/modular level, that is great for identifying hotspots in your code. However, sometimes the cause of an computationally hotspot is a single line of code in a function, which will not be caught by <code>cProfile</code>. An example would be an simple index operations such as <code>a[idx] = b</code>, which for large arrays and non-sequential indexes is really expensive. For these cases line_profiler and kernprof are excellent tools to have in your toolbox. Additionally, if you do not like cProfile we can also recommend py-spy which is another open-source profiling tool for python programs.</p>"},{"location":"s5_continuous_integration/","title":"Continuous Integration","text":"<p>Slides</p> <p> </p> <p>Continues integration is an sub discipline of the general field of Continues X. Continuous X is one of the core elements of modern Devops, and by extension MLOps. Continuous X assumes that we have a (long) developer pipeline (see image below) where we want to make some changes to our code e.g:</p> <ul> <li>Update our training data or data processing</li> <li>Update our model architecture</li> <li>Something else...</li> </ul> <p>Basically any code change we will expect will have a influence on the final result. The problem with doing changes to the start of our pipeline is that we want the change to propagate all the way through to the end of the pipeline.</p> <p></p>  Image credit  <p>This is where continuous X comes into play. The word continuous here refer to the fact that the pipeline should continuously be updated as we make code changes. You can also choose to think of this as automatization of processes. The X then covers that the process we need to go through to automatize steps in the pipeline, depends on where we are in the pipeline e.g. the tools needed to do continuous integration is different from the tools need to do continuous delivery.</p> <p>In this session we are going to focus on continuous integration (CI). As indicated in the image above, CI usually takes care of the first part of the developer pipeline that has to do with the code base, code building and code testing. In particular, in this module we are going to take a closer look at these questions:</p> <ul> <li>How to write unittests for our applications</li> <li>How to automatize tests being run on code changes</li> <li>How to secure we do not commit code that does not follow our code standards</li> <li>How we can automatize building of docker images</li> <li>How we can automatize training of our machine learning pipeline</li> </ul>"},{"location":"s5_continuous_integration/auto_docker/","title":"M18 - Continues Containers","text":""},{"location":"s5_continuous_integration/auto_docker/#continuous-docker-building","title":"Continuous docker building","text":"<p>The Github Actions we learned about in M16 are an powerful tool that can be used to much more than simply running our tests tests that we write for our application. In this module we are going to look at how we can use it for continuously building docker images. As you have already seen docker building can take a couple of minutes to build each time we do changes to our code base. For this reason we really just want to build a new image every time we do a commit of our code. Thus, it should come as no surprise that we can also automatize the building process and furthermore we can take advantage of online compute power to parallelize the process.</p> <p>As discussed in the initial module on docker, docker hub is an online solution for storing build docker images in the cloud that is then easy to pull down on whatever machine you want to run on. Docker hub is free to use for personal use, as long as the images you push are public. We are in this session going to look how we can automatically build and push our docker builds to docker hub. In a future module we are also going to look at the exact same process of building and pushing containers but this time to an general cloud provider.</p>"},{"location":"s5_continuous_integration/auto_docker/#exercises","title":"\u2754 Exercises","text":"<p>For these exercises you can choose to work with any docker file of your choosing. If you want an easy docker file, you can use the following:</p> <pre><code>FROM busybox\nCMD echo \"Howdy cowboy\"\n</code></pre> <p>Alternatively, you can choose to focus on automatizing the training and prediction docker files back from M9. You will most likely need to change the docker image for your applications if they contains any references to your data e.g. you have an <code>COPY data/ data/</code> statement in the file. Since we do not store our data in Github, we cannot copy it during the build process.</p> <ol> <li> <p>Start by pushing whatever docker file you want that should be continuously build to your repository</p> </li> <li> <p>Start by creating a Docker Hub account</p> </li> <li> <p>Next, within Docker Hub create an access token by going to <code>Settings -&gt; Security</code>. Click the <code>New Access Token</code>    button and give it a name that you recognize.</p> </li> <li> <p>Copy the newly created access token and head over to your Github repository online. Go to    <code>Settings -&gt; Secrets -&gt; Actions</code> and click the <code>New repository secret</code>. Copy over the access token and give    it the name <code>DOCKER_HUB_TOKEN</code>. Additionally, add two other secrets <code>DOCKER_HUB_USERNAME</code> and <code>DOCKER_HUB_REPOSITORY</code>    that contains your docker username and docker repository name respectively.</p> </li> <li> <p>Next we are going to construct the actual Github actions workflow file:</p> </li> </ol> <pre><code>name: Docker Image CI\non:\npush:\nbranches: [ master ]\njobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- name: Build the Docker image\nrun: |\necho \"${{ secrets.DOCKER_HUB_TOKEN }}\" | docker login \\\n-u \"${{ secrets.DOCKER_HUB_USERNAME }}\" --password-stdin docker.io\ndocker build . --file Dockerfile \\\n--tag docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/${{ secrets.DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA\ndocker push docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/${{ secrets.DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA\n</code></pre> <p>The first part of the workflow file should look somewhat recognizable. However, the last three lines are where    all the magic happens. Carefully go through them and figure out what they do. If you want some help you can looking    at the help page for <code>docker login</code>, <code>docker build</code> and <code>docker push</code>.</p> <ol> <li> <p>Upload the workflow to your github repository and check that it is being executed. If everything you should be able    to see the the build docker image in your container repository in docker hub.</p> </li> <li> <p>Make sure that you can execute <code>docker pull</code> locally to pull down the image that you just continuesly build</p> </li> <li> <p>(Optional) To test that the container works directly in github you can also try to include an additional    step that actually runs the container.</p> </li> </ol> <pre><code>  - name: Run container\nrun: |\ndocker run ...\n</code></pre> <p>That ends the session on continues docker building. We are going to revisit this topic after introducing the basic concepts of working in the cloud, as it will make our life easier in the long run when we get to continues deployment (CD) that our containers are stored the same place where we are going to run them. For completeness it is worth mentioning that docker hub also offers the possibility of building your images in a continues way, by specifying so called build rules.</p>"},{"location":"s5_continuous_integration/cml/","title":"M19 - Continues Machine Learning","text":""},{"location":"s5_continuous_integration/cml/#continuous-machine-learning","title":"Continuous Machine Learning","text":"<p>The continuous integration we have looked at until now is what we can consider \"classical\" continuous integration, that have its roots in DevOps and not MLOps. While the test that we have written and the containers ww have developed in the previous session have be around machine learning, everything we have done translate to completely to how it would be done if we had developed any other application did not include machine learning.</p> <p>In this session, we are now gonna change gear and look at continuous machine learning (CML). As the name may suggest we are now focusing on automatizing actual machine learning processes. You may ask why we need continues integration principals baked into machine learning pipelines? The reason is the same as with any continues integration, namely that we have a bunch of checks that we want our newly trained model to pass before we trust it. Writing <code>unittests</code> secures that our code is not broken, but there are other failure modes of a machine learning pipeline that should be checked before the model is ready for deployment:</p> <ul> <li>Did I train on the correct data?</li> <li>Did my model converge at all?</li> <li>Did it reach a certain threshold at all?</li> </ul> <p>Answering these questions in a continues way are possible through continuous machine learning. For this session, we are going to use <code>cml</code> by iterative.ai for this session. Strictly speaking, using the <code>cml</code> framework is not a necessary component for doing continuous machine learning but it streamlined way of doing this and offers tools to easily get a report about how a specific run performed. If we where just interested in trigging model training every time we do a <code>git push</code> we essentially just need to include</p> <pre><code>run: python train.py\n</code></pre> <p>to any of our workflow files.</p> <p>The figure below describes the overall process using the <code>cml</code> framework. It should be clear that it is the very same process that we go through as in the other continues integration sessions: <code>push code</code> -&gt; <code>trigger github actions</code> -&gt; <code>do stuff</code>. The new part in this session is that we want an report of the finding of the automated run to appear after the run is done.</p> <p></p>  Image credit"},{"location":"s5_continuous_integration/cml/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>We are first going to revisit our <code>train.py</code> script. If we want <code>cml</code> to automatically be able     to report the performance of our trained model to us after it is trained, we need to give it some     statistics to work with. Below is some psedo-code that computes the accuracy and the confusion     matrix of our trained model. Create an copy of your training script (call it <code>train_cml.py</code>) and     make sure your script is also producing an classification report and confusion matrix as in the     pseudo-code.</p> <pre><code># assume we have a trained model\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\npreds, target = [], []\nfor batch in train_dataloader:\nx, y = batch\nprobs = model(x)\npreds.append(probs.argmax(dim=-1))\ntarget.append(y.detach())\ntarget = torch.cat(target, dim=0)\npreds = torch.cat(preds, dim=0)\nreport = classification_report(target, preds)\nwith open(\"classification_report.txt\", 'w') as outfile:\noutfile.write(report)\nconfmat = confusion_matrix(target, preds)\ndisp = ConfusionMatrixDisplay(cm = confmat, )\nplt.savefig('confusion_matrix.png')\n</code></pre> </li> <li> <p>Similar to what we have looked at until now, automation happens using github workflow files.     The main difference from continuous integration we have looked on until now, is that we are actually     going to train our model whenever we do a <code>git push</code>. Copy the following code into a new workflow     (called <code>cml.yaml</code>) and add that file to the folder were you keep your workflow files.</p> <pre><code>name: train-my-model\non: [push]\njobs:\nrun:\nruns-on: [ubuntu-latest]\nsteps:\n- uses: actions/checkout@v2\n- uses: iterative/setup-cml@v1\n- name: Train model\nrun: |\npip install -r requirements.txt  # install dependencies\npython train.py  # run training\n- name: Write report\nenv:\n# this authenticates that the right permissions are in place\nREPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\nrun: |\n# send all information to report.md that will be reported to us when the workflow finish\ncat classification_report.txt &gt;&gt; report.md\ncml-publish confusion_matrix.png --md &gt;&gt; report.md\ncml-send-comment report.md\n</code></pre> <p>Nearly everything in the workflow file should look familar, except the last two lines.</p> </li> <li> <p>Try pushing the workflow file to your github repository and make sure that it completes.     If it does not, you may need to adjust the workflow file slightly.</p> </li> <li> <p>Send yourself a pull-request. I recommend seeing this     very short video on how to send yourself a pull-request with a small change. If you workflow file is     executed correctly you should see <code>github-actions</code> commenting with a performance report on your PR.</p> </li> <li> <p>(Optional) <code>cml</code> is offered by the same people behind <code>dvc</code> and it should therefore come as no surprise     that these features can interact with each other. If you want to deep dive into this,     here is a great starting point.</p> </li> </ol> <p>The ends the session on continues machine learning. If you have not already noticed, one limitation of using github actions is that their default runners e.g. <code>runs-on: [ubuntu-latest]</code> are only CPU machines (see hardware config . As we all know, modern machine learning more or less requires hardware acceleration (=GPUs) to train within reasonable time. Luckily for us <code>cml</code> also integrated with large cloud provides and I therefore recommend that after doing through the modules on cloud computing that you return to this exercise and experiment with setting up self-hosted runners.</p>"},{"location":"s5_continuous_integration/github_actions/","title":"M16 - Github Actions","text":""},{"location":"s5_continuous_integration/github_actions/#github-actions","title":"Github actions","text":"<p>Core Module</p> <p>With the tests established in the previous module we are now ready to move on to actually implementing some continuous integration in our pipeline. As you probably have already realized testing your code locally may take cumbersome to do, because</p> <ul> <li>You need to run it often to make sure to catch bugs early on</li> <li>If you want to have high code coverage of your code base, you will need many tests that takes a long time to run</li> </ul> <p>For these reasons we want to automatize the testing, such that it done every time we push to our repository. If we combine this with only pushing to branches and then only merging these branches whenever all automatized testing have passed, our code should be fairly safe against unwanted bugs (assuming your tests are well covering your code).</p>"},{"location":"s5_continuous_integration/github_actions/#github-actions_1","title":"Github actions","text":"<p>Github actions are the CI solution that Github provides. Each of your repositories gets 2,000 minutes of free testing per month which should be more than enough for the scope of this course (and probably all personal projects you do). Getting Github actions setup in a repository may seem complicated at first, but workflow files that you create for one repository can more or less be reused for any other repository that you have.</p> <p>Lets take a look at how a github workflow file is organized:</p> <ul> <li>Initially we start by giving the workflow a <code>name</code></li> <li>Next we specify on what events the workflow should be triggered. This includes both the action     (pull request, push ect) and on what branches is should activate</li> <li>Next we list the jobs that we want to do. Jobs are by default executed in parallel but can     also be dependent on each other</li> <li>In the <code>runs-on</code> we can specify which operation system we want the workflow to run on. We also     have the possibility to specify multiple.</li> <li>Finally we have the <code>steps</code>. This is where we specify the actual commands that should be     run when the workflow is executed.</li> </ul> <p></p>  Image credit"},{"location":"s5_continuous_integration/github_actions/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by creating a <code>.github</code> folder in the root of your repository.     Add a sub-folder to that called <code>workflows</code>.</p> </li> <li> <p>Go over this page     that explains how to do automated testing of python code in github actions. You do not have     to understand everything, but at least get a feeling of what a workflow file should look like.</p> </li> <li> <p>We have provided a workflow file called <code>tests.yml</code> that should run your tests for you. Place     this file in the <code>.github/workflows/</code> folder. The workflow file consist of three steps</p> <ul> <li> <p>First a python environment is setup (in this case python 3.8)</p> </li> <li> <p>Next all dependencies required to run the test are installed</p> </li> <li> <p>Finally, <code>pytest</code> is called and test will be run</p> </li> </ul> </li> <li> <p>For the script to work you need to define the <code>requirements.txt</code> and <code>requirements_tests.txt</code>.     The first file should contain all packages required to run your code. The second file is all     additional  packages required to run the tests. In your simple case it may very well be that     the second file is empty, however sometimes additional packages are used for testing that are     not strictly required for the scripts to run.</p> </li> <li> <p>Finally, try pushing the changes to your repository. Hopefully your tests should just start,     and you will after sometime see a green check mark next to hash of the commit. Also try to     checkout the Actions  tap where you can see the history of actions run.</p> <p> </p> </li> <li> <p>Normally we develop code one operating system and just hope that it will work on other operating     systems. However, CI enables us to automatically test on other systems than ourself.</p> <ol> <li> <p>The provided <code>tests.yml</code> only runs on one operating system. Which one?</p> </li> <li> <p>Alter the file (or write a new) that executes the test on the two other main operating     systems that exist.</p> </li> </ol> </li> <li> <p>As the workflow is currently setup, github actions will destroy every downloaded package     when the workflow has been executed. To improve this we can take advantage of <code>caching</code>:</p> <ol> <li> <p>Figure out how to implement <code>caching</code> in your workflow file. Hint: checkout this guide     page .     If you need help, you can check out this small repository where     I have setup a simple caching vs non-caching experiment with all the requirements that are installed throughout     this course.</p> </li> <li> <p>When you have implemented a caching system go to <code>Actions-&gt;Caches</code> in your repository and make sure that they     are correctly added. It should look something like the image below</p> <p> </p> </li> <li> <p>Measure how long your workflow takes before and after adding <code>caching</code> to your workflow. Did it improve the     runtime of your workflow?</p> </li> </ol> </li> <li> <p>(Optional) Code coverage can also be added to the workflow file by uploading it as an artifact     after running the coverage. Follow the instructions in this     post     on how to do it.</p> </li> <li> <p>As stated in the introduction, ideally we want to only push our code to branches, such that our workflows run     before we actually merge code into our codebase. We can directly prevent bad behavior by adding branch     protection rules to our repository. Take the image below as an example from one of my own PRs:</p> <p> </p> <p>In this example, the PR cannot be merge to the main branch before the following is fulfilled: At least 2 reviewers with write access have approved the PR, all Github actions marked as Required are passing and all conversations needs to be resolved. Since not all important tests are passing, further changes are necessary. We want to implement something similar. Do the following:</p> <ol> <li> <p>On your Github repository of choice, go to <code>Settings -&gt; Branches -&gt; Add branch protection rule</code>:</p> </li> <li> <p>To your main/master branch add the following rules:</p> <ul> <li>Atleast one person needs to approve any PR</li> <li>All your workflows has to pass</li> <li>All conversations needs to be resolved</li> </ul> </li> <li> <p>To test that everything works, try creating a PR (possibly with a small bug) and see that your main/master     branch is protected</p> </li> </ol> </li> <li> <p>One problem you may have encountered is running your tests that have to do with your data, with the core problem     being that your data is actually not stored in github (assuming you have done module     M8 - DVC) and therefore cannot be tested. However, it is possible     for us to download data while running our CI. Lets try to setup that:</p> <ol> <li> <p>The first problem is that we need our CI needs to be able to authenticate with the our storage solution. We can     take advantage of an authentication file that is created the first time we push with DVC. It is located in     <code>$CACHE_HOME/pydrive2fs/{gdrive_client_id}/default.json</code> where <code>$CACHE_HOME</code> depends on your operating system:</p> macOSLinuxWindows <p><code>~/Library/Caches</code></p> <p><code>~/.cache</code>  This is the typical location, but it may vary depending on what distro you are running</p> <p><code>{user}/AppData/Local</code></p> <p>Find the file. The content should look similar to this (only some fields are shown):</p> <pre><code>{\"access_token\": ...,\n\"client_id\": ...,\n\"client_secret\": ...,\n\"refresh_token\": ...,\n...\n}\n</code></pre> </li> <li> <p>The content of that file is should be treated as an password an not shared with the world and the relevant     question is therefore how to use this info in public repository. The answer is github secrets, where we can     store information, access it in our workflow files and it is still not public. Navigate to the secrets option     (as shown below) and create a secret with the name <code>GDRIVE_CREDENTIALS_DATA</code> that contains the content of the     file you found in the previous exercise.</p> <p> </p> </li> <li> <p>Afterwards, add the following code to your workflow file:</p> <p>```yaml        - uses: iterative/setup-dvc@v1</p> </li> <li> <p>name: Get data      run: dvc pull      env:         GDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}     ```</p> <p>that runs <code>dvc pull</code> using the secret authentication file. For help you can visit this small repository that implements the same workflow.</p> </li> <li> <p>Finally, add the changes, commit, push and confirm that everything works as expected. You should now be able to     run unit tests that depends on your input data.</p> </li> </ol> </li> </ol>"},{"location":"s5_continuous_integration/github_actions/#auto-linter","title":"Auto linter","text":"<p>In this module of the course you where introduced to a couple of good coding practices such as being consistent with how your python packages are sorted and that your code follows certain standards. In this set of exercises we will setup github workflows that will automatically test for this.</p>"},{"location":"s5_continuous_integration/github_actions/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Create a new workflow file called <code>isort.yml</code>, that implements the following three steps</p> <ul> <li> <p>Setup python environment</p> </li> <li> <p>Installs <code>isort</code></p> </li> <li> <p>Runs <code>isort</code> on the repository</p> </li> </ul> <p>(HINT: You should be able to just change the last steps of the <code>tests.yml</code> workflow file)</p> </li> <li> <p>Create a new workflow file called <code>flake8.yml</code>, that implements the following three steps</p> <ul> <li> <p>Setup python environment</p> </li> <li> <p>Installs <code>flake8</code></p> </li> <li> <p>Runs <code>flake8</code> on the repository</p> </li> </ul> <p>(HINT: You should be able to just change the last steps of the <code>tests.yml</code> workflow file)</p> </li> <li> <p>Create a new workflow file  called <code>mypy.yml</code>, that implements the following three steps</p> <ul> <li> <p>Setup python environment</p> </li> <li> <p>Installs <code>mypy</code></p> </li> <li> <p>Runs <code>mypy</code> on the repository</p> </li> </ul> </li> <li> <p>Try to make sure that all tests are passing on repository. Especially <code>mypy</code> can be hard     to get passing, so this exercise formally only requires you to get <code>isort</code> and <code>flake8</code>     passing.</p> </li> </ol> <p>This ends the module on Github workflows. If you have not already stumbled across this feature, if you try to create a workflow file directly in Github you may encounter the following page</p> <p></p> <p>Github comes with many templates for writing actions file for whatever you may need, to make sure you have a good starting point. We highly recommend checking this out if you want to write any other kind of CI pipeline in Github actions. Additionally we can also recommend this repository that have an list of awesome actions.</p>"},{"location":"s5_continuous_integration/pre_commit/","title":"M17 - Pre commit","text":""},{"location":"s5_continuous_integration/pre_commit/#pre-commit","title":"Pre-commit","text":"<p>One of the cornerstones of working with git is remembering to commit your work often. Often committing makes sure that it is easier to identify and revert unwanted changes that you have introduced, because the code changes becomes smaller per commit.</p> <p>However, as you hopefully already seen in the course there are a lot of mental task to do before you actually write <code>git commit</code> in the terminal. The most basic thing is of course making sure that you have saved all your changes, and you are not committing a not up-to-date file. However, this also includes tasks such as styling, formatting, making sure all tests succeeds etc. All these mental to-do notes does not mix well with the principal of remembering to commit often, because you in principal have to do them every time.</p> <p>The obvious solution to this problem is to automate all or some of our mental task every time that we do a commit. This is where pre-commit hooks comes into play, as they can help us attach additional tasks that should be run every time that we do a <code>git commit</code>.</p>"},{"location":"s5_continuous_integration/pre_commit/#configuration","title":"Configuration","text":"<p>Pre-commit simply works by inserting whatever workflow we want to automate in between whenever we do a <code>git commit</code> and afterwards would do a <code>git push</code>.</p> <p></p>  Image credit  <p>The system works by looking for a file called <code>.pre-commit-config.yaml</code> that we can configure. If we execute</p> <pre><code>pre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\n</code></pre> <p>you should get a sample file that looks like</p> <pre><code># See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\nrev: v3.2.0\nhooks:\n-   id: trailing-whitespace\n-   id: end-of-file-fixer\n-   id: check-yaml\n-   id: check-added-large-files\n</code></pre> <p>the file structure is very simple:</p> <ul> <li>It starts by listing the repositories where we want to get our pre-commits from, in this case   https://github.com/pre-commit/pre-commit-hooks. This repository contains a large collection of pre-commit hooks.</li> <li>Next we need to defined what pre-commit hooks that we want to get by specifying the <code>id</code> of the different hooks.   The <code>id</code> corresponds to an <code>id</code> in this file:   https://github.com/pre-commit/pre-commit-hooks/blob/master/.pre-commit-hooks.yaml</li> </ul> <p>When we are done defining our <code>.pre-commit-config.yaml</code> we just need to install it</p> <pre><code>pre-commit install\n</code></pre> <p>this will make sure that the file is automatically executed whenever we run <code>git commit</code></p>"},{"location":"s5_continuous_integration/pre_commit/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Install pre-commit</p> <pre><code>pip install pre-commit\n</code></pre> </li> <li> <p>Next create the sample file</p> <pre><code>pre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\n</code></pre> </li> <li> <p>The sample file already contains 4 hooks. Make sure you understand what each do and if you need them at all.</p> </li> <li> <p>The base repo https://github.com/pre-commit/pre-commit-hooks also have a hook for configuring <code>flake8</code> to run.     Add this to the config file and make sure it works as expected e.g. make something that is not <code>flake8</code> compliant     and then try to commit that change.</p> </li> <li> <p>Running <code>black</code> or <code>yapf</code> is not part of the base repo, however it is still possible to include this as pre-commit     hooks. Google how to do it, include one of them and then test out that it actually works.</p> </li> <li> <p>Make sure that you also can do commits without running <code>pre-commit</code> e.g.</p> <pre><code>git commit -m &lt;message&gt; --no-verify\n</code></pre> </li> <li> <p>Finally, figure out how to disable <code>pre-commit</code> again.</p> </li> </ol> <p>That was all about how <code>pre-commit</code> can be used to automatize tasks. If you want to deep dive more into the topic you can checkout this page on how to define your own <code>pre-commit</code> hooks.</p>"},{"location":"s5_continuous_integration/unittesting/","title":"M15 - Unittesting","text":""},{"location":"s5_continuous_integration/unittesting/#unit-testing","title":"Unit testing","text":"<p>Core Module</p> <p>What often comes to mind for many developers, when discussing continuous integration (CI) is code testing. CI should secure that whenever a codebase is updated it is automatically tested such that if bugs have been introduced in the codebase it will be catched early on. If you look at the MLOps cyclepipeline, CI is one of cornerstones of operations part. However, it should be notes that applying CI does not magically secure that your code does not break. CI is only as strong as the tests that are automatically executed. CI simply structures and automates this.</p> <p>Quote</p> <p>Continuous Integration doesn\u2019t get rid of bugs, but it does make them dramatically easier to find and remove.  Martin Fowler, Chief Scientist, ThoughtWorks</p> <p></p>  Image credit  <p>The kind of tests we are going to look at are called unit testing. Unit testing refer to the practice of writing test that tests individual parts of your code base to test for correctness. By unit you can therefore think a function, module or in general any object. By writing tests in this way it should be very easy to isolate which part of the code that broke after an update to the code base. Another way to test your code base would be through integration testing which is equally important but we are not going to focus on it in this course.</p>"},{"location":"s5_continuous_integration/unittesting/#pytest","title":"Pytest","text":"<p>Before we can begin to automatize testing of our code base we of course need to write the tests first. It is both a hard and tedious task to do but arguable the most important aspects of CI. Python offers a couple of different libraries for writing tests. We are going to use <code>pytest</code>.</p>"},{"location":"s5_continuous_integration/unittesting/#exercises","title":"\u2754 Exercises","text":"<p>The following exercises should be applied to your MNIST repository</p> <ol> <li> <p>The first part of doing CI is writing the unit tests. We do not expect you to cover every part     of the code you have developed but try to at least write tests that cover two files. Start by     creating a <code>tests</code> folder.</p> </li> <li> <p>Read the getting started guide for pytest     which is the testing framework that we are going to use</p> </li> <li> <p>Install pytest:</p> <pre><code>pip install pytest\n</code></pre> </li> <li> <p>Write some tests. Below are some guidelines on some tests that should be implemented, but     you are of course free to implement more tests. You can at any point check if your tests are     passing by typing in a terminal</p> <pre><code>pytest tests/\n</code></pre> <p>When you implement a test you need to follow two standards, for <code>pytest</code> to be able to find your tests. First any files created (except <code>__init__.py</code>) should always start with <code>test_*.py</code>. Secondly, any test implemented needs to be wrapped into its own function that again needs to start with <code>test_</code>:</p> <pre><code># this will be found and executed by pytest\ndef test_something():\n...\n# this will not be found and executed by pytest\ndef something_to_test():\n...\n</code></pre> <ol> <li> <p>Start by creating a <code>tests/__init__.py</code> file and fill in the following:</p> <pre><code>import os\n_TEST_ROOT = os.path.dirname(__file__)  # root of test folder\n_PROJECT_ROOT = os.path.dirname(_TEST_ROOT)  # root of project\n_PATH_DATA = os.path.join(_PROJECT_ROOT, \"Data\")  # root of data\n</code></pre> <p>these can help you refer to your data files during testing. For example, in another test file I could write</p> <pre><code>from tests import _PATH_DATA\n</code></pre> <p>which then contains the root path to my data.</p> </li> <li> <p>Data testing: In a file called <code>tests/test_data.py</code> implement at least a test that checks that data gets     correctly loaded. By this we mean that you should check</p> <pre><code>def test_data():\ndataset = MNIST(...)\nassert len(dataset) == N_train for training and N_test for test\nassert that each datapoint has shape [1,28,28] or [784] depending on how you choose to format\nassert that all labels are represented\n</code></pre> <p>where <code>N_train</code> should be either 25000 or 40000 depending on if you are just the first subset of the corrupted MNIST data or also including the second subset. <code>N_test</code> should be 5000.</p> </li> <li> <p>Model testing: In a file called <code>tests/test_model.py</code> implement at least a test that     checks for a given input with shape X that the output of the model have shape Y</p> </li> <li> <p>Training testing: In a file called <code>tests/test_training.py</code> implement at least one     test that asserts something about your training script. You are here given free hands on     what should be tested, but try to test something the risk being broken when developing the code.</p> </li> <li> <p>Good code raises errors and give out warnings in appropriate places. This is often in     the case of some invalid combination of input to your script. For example, you model     could check for the size of the input given to it (see code below) to make sure it corresponds     to what you are expecting. Not implementing such errors would still result in Pytorch failing     at a later point due to shape errors, however these custom errors will probably make more sense     to the end user. Implement at least one raised error or warning somewhere in your code and     use either <code>pytest.raises</code> or <code>pytest.warns</code> to check that they are correctly raised/warned.     As inspiration, the following implements <code>ValueError</code> in code belonging to the model:</p> <pre><code># src/models/model.py\ndef forward(self, x: Tensor):\nif x.ndim != 4:\nraise ValueError('Expected input to a 4D tensor')\nif x.shape[1] != 1 or x.shape[2] != 28 or x.shape[3] != 28:\nraise ValueError('Expected each sample to have shape [1, 28, 28]')\n</code></pre> <p>which would be captured by a test looking something like this:</p> <pre><code># tests/test_model.py\ndef test_error_on_wrong_shape():\nwith pytest.raises(ValueError, match='Expected input to a 4D tensor')\nmodel(torch.randn(1,2,3))\n</code></pre> </li> <li> <p>A test is only as good as the error message it gives, and by default <code>assert</code>     will only report that the check failed. However, we can help our self and others by adding     strings after <code>assert</code> like</p> <pre><code>assert len(train_dataset) == N_train, \"Dataset did not have the correct number of samples\"\n</code></pre> <p>Add such comments to the assert statements you just did.</p> </li> <li> <p>The tests that involve checking anything that have to do with our data, will of course fail     if the data is not present. To future proof our code, we can take advantage of the     <code>pytest.mark.skipif</code> decorator. Use this decorator to skip your data tests if the corresponding     data files does not exist. It should look something like this</p> <pre><code>import os.path\n@pytest.mark.skipif(not os.path.exists(file_path), reason=\"Data files not found\")\ndef test_something_about_data():\n...\n</code></pre> <p>You can read more about skipping tests here</p> </li> </ol> </li> <li> <p>After writing the different tests, make sure that they are passing locally.</p> </li> <li> <p>We often want to check a function/module for various input arguments. In this case you could     write the same test over and over again for the different input, but <code>pytest</code> also have build     in support for this with the use of the     pytest.mark.parametrize decorator.     Implement a parametrize test and make sure that it runs for different input.</p> </li> <li> <p>There really do not exist any way of measuring how good the test you have written are. However,     what we can measure is the code coverage. Code coverage refers to the percentage of your     codebase that actually gets run when all your tests are executed. Having a high coverage     at least means that all your code will run when executed.</p> <ol> <li> <p>Install coverage</p> <pre><code>pip install coverage\n</code></pre> </li> <li> <p>Instead of running your tests directly with <code>pytest</code>, now do</p> <pre><code>coverage run -m pytest tests/\n</code></pre> </li> <li> <p>To get a simple coverage report simply type</p> <pre><code>coverage report\n</code></pre> <p>which will give you the percentage of cover in each of your files. You can also write</p> <pre><code>coverage report -m\n</code></pre> <p>to get the exact lines that was missed by your tests.</p> </li> <li> <p>Finally, try to increase the coverage by writing a new test that runs some     of the lines in your codebase that is not covered yet.</p> </li> <li> <p>Often <code>coverage</code> reports the code coverage on files that we actually do not want     to get a code coverage for. Figure out how to configure <code>coverage</code> to exclude     some files.</p> </li> </ol> </li> </ol>"},{"location":"s5_continuous_integration/unittesting/#knowledge-check","title":"Knowledge check","text":"Knowledge question 1 <p>Assume you have a code coverage of 100%, would you expect that no bugs are present in your code?</p> Solution <p>No, code coverage is not a guarantee that your code is bug free. It is just a measure of how much many lines of code that are actually run when your tests are executed. It is that there is some corner case that is not covered by your tests and will result in a bug. However, having a high code coverage is a good indicator that you actually have tested your code.</p> <p>That covers the basic of writing unittests for python code. We want to note that <code>pytest</code> of course is not the only framework for doing this. Python actually have an build in framework called unittest for doing this also (but <code>pytest</code> offers a bit more features). Another open-source framework that you could choose to checkout is hypothesis that can really help catch errors in corner cases of your code. In addition to writing unittests it is also highly recommended to test code that you include in your docstring belonging to your functions and modulus to make sure that any code there is in your documentation is also correct. For such testing we can highly recommend using pythons build-in framework doctest.</p>"},{"location":"s6_the_cloud/","title":"Cloud computing","text":"<p>Slides</p> <p> </p> <p>Running computations locally is often sufficient when only playing around with code in initial phase of development. However, to really scale your experiments you will need more computing power than what your standard laptop/desktop can offer. You probably already have experience with running on a local cluster or similar but todays topic is about utilizing cloud computing.</p> <p></p>  Image credit  <p>There exist a numerous amount of cloud compute providers with some of the biggest being:</p> <ul> <li>Azure</li> <li>AWS</li> <li>Google Cloud project</li> <li>Alibaba cloud</li> </ul> <p>The all have slight advantages and disadvantages over each others. In this course we are going to focus on Google cloud, because they have been kindly enough to sponsor $50 of cloud credit to each student. If you happen to run out of credit, you can also get some free credit for a limited amount of time when you signup with a new account. What's important to note is that all these different cloud providers all have the same set of services, and that learning how to use the services of one cloud provider in many cases translate to also know how to use the same services at another cloud provider. The services are called something different and can have a bit of a different interface/interaction pattern but in the end it does not really matter.</p> <p>Todays exercises are about getting to know how to work with the cloud. If you are in doubt about anything or want to deep dive into some topics, I can recommend watching this series of videos or going through the general docs.</p>"},{"location":"s6_the_cloud/cloud_setup/","title":"Cloud setup","text":"<p>Core Module</p> <p>Google cloud project (GCP) is the cloud service provided by Google. The key concept, or selling point, of any cloud provider is the idea of near-infinite resources. Without the cloud it simply is not feasible to do many modern deep learning and machine learning tasks because they cannot be scaled locally.</p> <p>The image below shows a subset of all the different services that the Google cloud platform offers. The ones marked in red are the ones we are actually going to investigate in this course. Therefore, if you get done with exercises early I highly recommend that you deep dive more into the Google cloud platform.</p> <p></p>  Image credit"},{"location":"s6_the_cloud/cloud_setup/#exercises","title":"\u2754 Exercises","text":"<p>As the first step we are going to get you setup with some Google cloud credits.</p> <ol> <li> <p>Go to https://learn.inside.dtu.dk. Go to this course. Find the recent message where there should be a download     link and instructions on how to claim the $50 cloud credit. Please do not share the link anywhere as there are a     limited amount of coupons. If you are not officially taking this course at DTU, Google gives $300 cloud credits     whenever you signup with a new account. NOTE that you need to provide a credit card for this so make     sure to closely monitor your credit use so you do not end spending more than the free credit.</p> </li> <li> <p>Login to the homepage of gcp. It should look like this:</p> <p> </p> </li> <li> <p>Go to billing and make sure that your account is showing $50 of cloud credit</p> <p> </p> <p>make sure to also checkout the <code>Reports</code> throughout the course. When you are starting to use some of the cloud services these tabs will update with info about how much time you can use before your cloud credit runs out. Make sure that you monitor this page as you will not be given another coupon.</p> </li> <li> <p>One way to stay organized within GCP is to create projects.</p> <p> </p> <p>Create a new project called <code>dtumlops</code>. When you click <code>create</code> you should get a notification that the project is being created. The notification bell is good way to make sure how the processes you are running are doing throughout the course.</p> </li> <li> <p>Finally, for setup we are going to install <code>gcloud</code>. <code>gcloud</code> is the command line interface for working with     our Google cloud account. Nearly everything that we can do through the web interface we can also do through     the <code>gcloud</code> interface. Follow the installation instructions here     for your specific OS.</p> <ol> <li> <p>After installation, try in a terminal to type:</p> <pre><code>gcloud -h\n</code></pre> <p>the command should and show the help page. If not, something went wrong in the installation (you may need to restart after installing).</p> </li> <li> <p>Now login by typing</p> <pre><code>gcloud auth login\n</code></pre> <p>you should be sent to an web page where you link your cloud account to the <code>gcloud</code> interface. Afterwards, also run this command:</p> <pre><code>gcloud auth application-default login\n</code></pre> <p>If you at some point want to revoke this you can type:</p> <pre><code>gcloud auth revoke\n</code></pre> </li> <li> <p>Next you will need to set the project that we just created. In your web browser under project info,     you should be able to see the <code>Project ID</code> belonging to your <code>dtumlops</code> project. Copy this an type     the following command in a terminal</p> <pre><code>gcloud config set project &lt;project-id&gt;\n</code></pre> <p>You can also get the project info by running</p> <pre><code>gcloud projects list\n</code></pre> </li> <li> <p>Next install the Google cloud python API:</p> <pre><code>pip install --upgrade google-api-python-client\n</code></pre> <p>Make sure that the python interface is also installed. In a python terminal type</p> <pre><code>import googleapiclient\n</code></pre> <p>this should work without any errors.</p> </li> <li> <p>Finally, we need some additional commands for <code>gcloud</code> which are part of the <code>beta</code> component.     Install with:</p> <pre><code>gcloud components install beta\n</code></pre> <p>You can get a list of all install components using</p> <pre><code>gcloud components list\n</code></pre> </li> <li> <p>(Optional) If you are using VSCode you can also download the relevant     extension     called <code>Cloud Code</code>. After installing it you should see a small <code>Cloud Code</code> button in the action bar.</p> </li> </ol> </li> </ol> <p>After following these step your laptop should hopefully be setup for using <code>gcp</code> locally. You are now ready to use their services, both locally on your laptop and in the cloud console.</p>"},{"location":"s6_the_cloud/cloud_setup/#quotas","title":"Quotas","text":"<p>A big part of using the cloud in a bigger organisation has to do with Admin and quotas. Admin here in general refers to the different roles that users of GCP and quotas refers to the amount of resources that a given user has access to. For example one employee, lets say a data scientist, may only be granted access to certain GCP services that have to do with development and training of machine learning model, with <code>X</code> amounts of GPUs available to use to make sure that the employee does not spend too much money. Another employee, a devops engineer, probably do not need access to the same services and not necessarily the same resources.</p> <p>In this course we are not going to focus too much on this aspect but it is important to know about. What we are going to go through is how to increase the quotas for how many GPUs you have available. By default any free accounts in GCP (or accounts using teaching credits) the default quota for GPUs that you can use is either 0 or 1 (their policies sometimes changes). We will in the exercises below try to increase it.</p>"},{"location":"s6_the_cloud/cloud_setup/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by enabling the <code>Compute Engine</code> service. Simply search for it in the top search bar. It should bring you     to the a page where you can enable the service (may take some time). We are going to look more into this service     in the next module.</p> </li> <li> <p>Next go to the <code>IAM &amp; Admin</code> page, again search for it in the top search bar. The remaining steps are illustrated     in the figure below.</p> <ol> <li> <p>Go to the <code>quotas page</code></p> </li> <li> <p>In the search field search for <code>GPUs (all regions)</code> (needs to match exactly, the search field is case sensitive),     such that you get the same quota as in the image.</p> </li> <li> <p>In the limit you can see what your current quota for the number of GPUs you can use are. Additional, to the     right of the limit you can see the current usage. It is worth checking in on if you are ever in doubt if a job     is running on GPU or not.</p> </li> <li> <p>Click the quota and afterwards the <code>Edit qoutas</code> button.</p> </li> <li> <p>In the pop-op window, increase your limit to either 1 or 2.</p> </li> <li> <p>After sending your request you can try clicking the <code>Increase requests</code> tab to see the status of your request</p> </li> </ol> <p> </p> </li> </ol> <p>If you are ever running into errors when working in GPU that contains statements about <code>quotas</code> you can always try to go to this page and see what you are actually allowed to use currently and try to increase it.</p> <p>Finally, we want to note that a quota increase is sometimes not allowed within 24 hours of creating an account. If your request gets rejected, we recommend to wait a day and try again. If this does still not work, you may need to use their services some more to make sure you are not a bot that wants to mine crypto.</p>"},{"location":"s6_the_cloud/cloud_setup/#knowledge-check","title":"Knowledge check","text":"Knowledge question 1 <p>What considerations to take when choosing an GCP region for running a new application?</p> Solution <p>A series of factors may influence your choice of region, including:</p> <ul> <li>Services availability in the region, not all services are available in all regions</li> <li>Reduced latency: if your application is running in the same region as your users, the latency will be lower</li> <li>Compliance: some countries has strict rules that requires user info to be stored inside a particular region     eg. EU has GDPR rules that requires all user data to be stored in the EU</li> <li>Pricing: some regions may have different pricing than others</li> </ul>"},{"location":"s6_the_cloud/using_the_cloud/","title":"Using the cloud","text":"<p>Core Module</p> <p>In this set of exercises we are going to get more familiar with the using some of the resources that the Google cloud project offers.</p>"},{"location":"s6_the_cloud/using_the_cloud/#compute","title":"Compute","text":"<p>The most basic service of any cloud provider is the ability to create and run virtual machines. In <code>gcp</code> this service is called Compute Engine API. A virtual machine allows you to essentially run an operating system that behaves like a completely separate computer. There are many reasons why one to use virtual machines:</p> <ul> <li>Virtual machines allow you to scale your operations, essentially giving you access to infinitely many individual     computers</li> <li> <p>Virtual machines allow you to use large scale hardware. For example if you are developing an deep learning model on     your laptop and want to know the inference time for a specific hardware configuration, you can just create a virtual     machine with those specs and run your model.</p> </li> <li> <p>Virtual machines allow you to run processes in the \"background\". If you want to train a model for a week or more, you     do not want to do this on your own laptop as you cannot really move it or do anything with while it is training.     Virtual machines allow you to just launch a job and forget about it (at least until you run out of credit).</p> </li> </ul> <p></p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises","title":"\u2754 Exercises","text":"<p>We are now going to start actually using the cloud.</p> <ol> <li> <p>Click on the <code>Compute Engine</code> tab in sidebar on the homepage of <code>gcp</code>.</p> </li> <li> <p>Try to <code>Create instance</code>. You will see the following image below.</p> <p> </p> <p>Give it a meaningful name, set the location to some location that is closer to where you actually is (to reduce latency). Finally try to adjust the the configuration a bit. What two factors are effecting the price of the compute unit?</p> </li> <li> <p>After figuring this out, create a <code>e2-medium</code> instance (leave rest configured as default). Before clicking the     <code>Create</code> button make sure to check the <code>Equavalent Command Line</code> button. You should see a very long command that you     could have typed instead to do the exact same.</p> </li> <li> <p>Now in a local terminal type:</p> <pre><code>gcloud compute instances list\n</code></pre> <p>you should hopefully see the instance you have just created.</p> </li> <li> <p>You can start a terminal directly by typing:</p> <pre><code>gcloud beta compute ssh --zone &lt;zone&gt; &lt;name&gt; --project &lt;project-id&gt;\n</code></pre> <p>You can always see the exact command that you need to run to <code>ssh</code> to an VM by selecting the <code>View gcloud command</code> option in the Compute Engine overview (see image below).</p> <p> </p> </li> <li> <p>While logged into the instance, check if Python and Pytorch is installed?     You should see that neither is installed. The VM we have only specified what     compute resources it should have, and not what software should be in it. We     can fix this by starting VMs based on specific docker images (its all coming together).</p> <ol> <li> <p><code>gcp</code> Comes with a number of ready-to-go images for doing deep learning.     More info can be found here.     Try, running this line:</p> <pre><code>gcloud container images list --repository=\"gcr.io/deeplearning-platform-release\"\n</code></pre> <p>what does the output show?</p> </li> <li> <p>Next, start (in the terminal) a new instance using a Pytorch image. The     command for doing it should look something like this:</p> <pre><code>gcloud compute instances create &lt;instance_name&gt; \\\n--zone=&lt;zone&gt; \\\n--image-family=&lt;image-family&gt; \\\n--image-project=deeplearning-platform-release \\\n</code></pre> <p>Hint: you can find relevant image families here.</p> </li> <li> <p><code>ssh</code> to the VM as one of the previous exercises. Confirm that the container indeed contains     both a python installation and Pytorch is also installed. Hint: you also have the possibility     through the web page to start a browser session directly to the VMs you create:</p> <p> </p> </li> </ol> </li> <li> <p>Finally, everything that you have done locally can also be achieved through the web     terminal, which of course comes pre-installed with the <code>gcloud</code> command etc.</p> <p> </p> <p>Try out launching this and run some of the commands from the previous exercises.</p> </li> </ol>"},{"location":"s6_the_cloud/using_the_cloud/#data-storage","title":"Data storage","text":"<p>Another big part of cloud computing is storage of data. There are many reason that you want to store your data in the cloud including:</p> <ul> <li>Easily being able to share</li> <li>Easily expand as you need more</li> <li>Data is stored multiple locations, making sure that it is not lost in case of an emergency</li> </ul> <p>Cloud storage is luckily also very cheap. Google cloud only takes around $0.026 per GB per month. This means that around 1 TB of data would cost you $26 which is more than what the same amount of data would cost on Goggle Drive, but the storage in Google cloud is much more focused on enterprise where you have a need for accessing data through an API.</p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises_1","title":"\u2754 Exercises","text":"<p>When we did the exercise on data version control, we made <code>dvc</code> work together with our own Google drive to storage data. However, a big limitation of this is that we need to authentic each time we try to either push or pull the data. The reason is that we need to use an API instead which is offered through <code>gcp</code>.</p> <p>We are going to follow the instructions from this page</p> <ol> <li> <p>Lets start by creating a data storage. On the GCP startpage, in the sidebar, click on the <code>Cloud Storage</code>.     On the next page click the <code>Create bucket</code>:</p> <p> </p> <p>Give the bucket an unique name, set it to a region close by and make it of size 20 GB as seen in the image.</p> </li> <li> <p>After creating the storage, you should be able to see it if you type</p> <pre><code>gsutil ls\n</code></pre> <p><code>gsutil</code> is an additional command to <code>gcloud</code>, that provides more command line options.</p> </li> <li> <p>Next we need the Google storage extension for <code>dvc</code></p> <pre><code>pip install dvc[gs]\n</code></pre> </li> <li> <p>Now in your MNIST repository where you have already configured dvc, we are going to change the storage     from our Google drive to our newly created Google cloud storage.</p> <pre><code>dvc remote add -d remote_storage &lt;output-from-gsutils&gt;\n</code></pre> </li> <li> <p>The above command will change the <code>.dvc/config</code> file. <code>git add</code> and <code>git commit</code> the changes to that file.     Finally, push data to the cloud</p> <pre><code>dvc push\n</code></pre> </li> <li> <p>Finally, make sure that you can pull without having to give your credentials. The easiest way to see this     is to delete the <code>.dvc/cache</code> folder that should be locally on your laptop and afterwards do a <code>dvc pull</code>.</p> </li> </ol> <p>If you ever end up with credential issues when working with your data, we in general recommend for this course that you store data in a bucket that is public accessible e.g. no authentication needed. You can read more about how to make your buckets public here.</p>"},{"location":"s6_the_cloud/using_the_cloud/#container-registry","title":"Container registry","text":"<p>You should hopefully at this point have seen the strength of using containers e.g. Docker. They allow us to specify exactly the software that we want to run inside our VMs. However, you should already have run into two problems with docker</p> <ul> <li>Building process can take a lot of time</li> <li>Docker images can be large</li> </ul> <p>For this reason we want to move both the building process and the storage of images to the cloud.</p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises_2","title":"\u2754 Exercises","text":"<p>For the purpose of these exercise I recommend that you start out with a dummy version of some code to make sure that the building process do not take too long. You are more than free to fork this repository. The repository contains a simple python script that does image classification using sklearn. The docker images for this application are therefore going to be substantially faster to build and smaller in size than the images we are used to that uses Pytorch.</p> <ol> <li> <p>Start by enabling the service: <code>Google Container Registry API</code> and <code>Google Cloud Build API</code>. This can be     done through the web side (by searching for the services) or can also be enabled from the terminal:</p> <pre><code>gcloud services enable containerregistry.googleapis.com\ngcloud services enable cloudbuild.googleapis.com\n</code></pre> </li> <li> <p>Google cloud building can in principal work out of the box with docker files. However, the recommended way     is to add specialized <code>cloudbuild.yaml</code> files. They should look something like this:</p> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\nargs: ['build', '-t', 'gcr.io/&lt;project-id&gt;/&lt;image-name&gt;', '.']\n- name: 'gcr.io/cloud-builders/docker'\nargs: ['push', 'gcr.io/&lt;project-id&gt;/&lt;image-name&gt;']\n</code></pre> <p>which essentially is a basic yaml file that contains a list of steps, where each step consist of the service that should be used and the arguments for that service. In the above example we are calling the same service (<code>cloud-builders/docker</code>) with different arguments (<code>build</code> and then <code>push</code>). Implement such a file in your repository. Hint: if you forked the repository then you at least need to change the <code>&lt;project-id&gt;</code>.</p> </li> <li> <p>From the <code>gcp</code> homepage, navigate to the triggers panel:</p> <p> </p> <p>Click on the manage repositories.</p> </li> <li> <p>From there, click the <code>Connect Repository</code> and go through the steps of authenticating your github profile with     <code>gcp</code> and choose the repository that you want to setup build triggers. For now, skip the     <code>Create a trigger (optional)</code> part by pressing <code>Done</code> in the end.</p> <p> </p> </li> <li> <p>Navigate back to the <code>Triggers</code> homepage and click <code>Create trigger</code>. Set the following:</p> <ul> <li>Give a name</li> <li>Event: choose <code>Push to branch</code></li> <li>Source: choose the repository you just connected</li> <li>Branch: choose <code>^main$</code></li> <li>Configuration: choose either <code>Autodetected</code> or <code>Cloud build configuration file</code></li> </ul> <p>Finally click the <code>Create</code> button and the trigger should show up on the triggers page.</p> </li> <li> <p>To activate the trigger, push some code to the chosen repository.</p> </li> <li> <p>Go to the <code>Cloud Build</code> page and you should see the image being build and pushed.</p> <p> </p> <p>Try clicking on the build to checkout the build process and building summary. As you can see from the image, if a build is failing you will often find valuable info by looking at the build summary. If you build is failing try to configure it to run in one of these regions: <code>us-central1, us-west2, europe-west1, asia-east1, australia-southeast1, southamerica-east1</code> as specified in the documentation.</p> </li> <li> <p>If/when your build is successful, navigate to the <code>Container Registry</code> page. You should     hopefully find that the image you just build was pushed here. Congrats!</p> </li> <li> <p>Finally, to to pull your image down to your laptop</p> <pre><code>docker pull gcr.io/&lt;project-id&gt;/&lt;image_name&gt;:&lt;image_tag&gt;\n</code></pre> <p>you will need to authenticate <code>docker</code> with <code>gcp</code> first. Instructions can be found here, but the following command should hopefully be enough to make <code>docker</code> and <code>gcp</code> talk to each other:</p> <pre><code>gcloud auth configure-docker\n</code></pre> <p>Note: To do this you need to have <code>docker</code> actively running in the background, as any other time you want to use <code>docker</code>.</p> </li> <li> <p>Automatization through the cloud is in general the way to go, but sometimes you may     want to manually create images and push them to the registry. Figure out how to push     an image to your <code>Container Registry</code>. For simplicity you can just push the <code>busybox</code>     image you downloaded during the initial docker exercises. This     page should help     you with exercise.</p> </li> </ol>"},{"location":"s6_the_cloud/using_the_cloud/#training","title":"Training","text":"<p>As our final step in our journey through different GCP services in this module we are going to look at training of our models. This is one of the important tasks that GCP can help us with because we can always rent more hardware as long as we have credits, meaning that we can both scale horizontal (run more experiments) and vertical (run longer experiments).</p> <p>We are going to checkout two ways of running our experiments. First we are going to return to the Compute Engine service because it gives the most simple form of scaling of experiments. That is: we create a VM with a appropriate docker image, we start it and login to the VM and we run our experiments. It is possible for most people to run a couple of experiments in parallel this way. However, what if there was an abstract layer that automatically created VM for us, lunched our experiments and the close the VM afterwards?</p> <p>This is where Vertex AI service comes into play. This is a dedicated service for handling ML models in GCP in the cloud. Vertex AI is in principal and end-to-end service that can take care of everything machine learning related in the cloud. In this course we are primarily focused on just the training of our models, and then use other services for different parts of our pipeline.</p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises_3","title":"\u2754 Exercises","text":"<ol> <li> <p>Lets start by see how we could train a model using Pytorch using the Compute Engine service:</p> <ol> <li> <p>Start by creating a appropriate VM. If you want to start a VM that have Pytorch pre-installed with only CPU     support you can run the following command</p> <pre><code>gcloud compute instances create &lt;instance-name&gt; \\\n--zone europe-west1-b \\\n--image-family=pytorch-latest-cpu \\\n--image-project=deeplearning-platform-release\n</code></pre> <p>alternatively, if you have access to GPU in your GCP account you could start a VM in the following way</p> <pre><code>gcloud compute instances create &lt;instance-name&gt; \\\n--zone europe-west4-a \\\n--image-family=pytorch-latest-gpu \\\n--image-project=deeplearning-platform-release \\\n--accelerator=\"type=nvidia-tesla-v100,count=1\" \\\n--metadata=\"install-nvidia-driver=True\" \\\n--maintenance-policy TERMINATE\n</code></pre> </li> <li> <p>Next login into your newly created VM. You can either open an <code>ssh</code> terminal in the cloud console or run the     following command</p> <pre><code>gcloud beta compute ssh &lt;instance-name&gt;\n</code></pre> </li> <li> <p>It is recommend to always check that the VM we get is actually what we asked for. In this case the VM should have     Pytorch pre-installed so lets check for that by running</p> <pre><code>python -c \"import torch; print(torch.__version__)\"\n</code></pre> <p>Additionally, if you have a VM with GPU support also try running the <code>nvidia-smi</code> command.</p> </li> <li> <p>When you have logged in to the VM, it works as your own machine. Therefore to run some training code you would     need to do the same setup step you have done on your own machine: clone your github, install dependencies,     download data, run code. Try doing this to make sure you can train a model.</p> </li> </ol> </li> <li> <p>The last step in the previous exercise involves a lot of setup that would be necessary to do every time we create a     new VM, making horizontal scaling of experiments cumbersome. However, we have already developed docker images that     can take care of most of the setup.</p> <ol> <li> <p>Lets for simplicity just create a very small docker image (called <code>gcp_vm_tester.dockerfile</code>) that you can use</p> <pre><code>FROM gcr.io/deeplearning-platform-release/pytorch-cpu\nRUN pip install matplotlib\n</code></pre> <p>this basically just extends the base Pytorch image to also install matplotlib. The important part about the docker images that we want to use here is that they should not have an <code>ENTRYPOINT</code> at the end, because we do not want the docker container to actually run our scripts, just install dependencies on startup.</p> </li> <li> <p>Lets build docker and manually push it to our container repository in gcp. Build with:</p> <pre><code>docker build -f gcp_vm_tester.dockerfile.dockerfile . -t gcp_vm_tester:latest\n</code></pre> <p>and then push with</p> <pre><code>docker tag tester gcr.io/&lt;project-id&gt;/gcp_vm_tester\ndocker push gcr.io/&lt;project-id&gt;/gcp_vm_tester\n</code></pre> <p>confirm by going to the container registry in the cloud consol and check that the image has been correctly pushed.</p> </li> <li> <p>Lets then create a VM with that particular docker image. Instead of using <code>gcloud compute instances create</code> we     are now using the <code>gcloud compute instances create-with-container</code> command</p> <pre><code>gcloud compute instances create-with-container &lt;instance-name&gt; \\\n--container-image=gcr.io/&lt;project-id&gt;/gcp_vm_tester\n    --zone europe-west1-b\n</code></pre> </li> <li> <p>Confirm that everything works by accessing your newly created VM and run both of these commands</p> <pre><code>python -c \"import torch; print(torch.__version__)\"\npython -c \"import matplotlib; print(matplotlib.__version__)\"\n</code></pre> </li> </ol> </li> <li> <p>We are now moving on to the final way to train our code, using <code>Vertex AI</code> service.</p> <ol> <li> <p>Start by enabling it by searching for <code>Vertex AI</code> in the cloud consol and go to the service</p> </li> <li> <p>The way we are going to use Vertex AI is to create custom jobs because we have already developed docker containers     that contains everything to run our code. Thus the only command that we actually need to use is     <code>gcloud ai custom-jobs create</code> command. An example here would be:</p> <pre><code>gcloud ai custom-jobs create \\\n--region=europe-west1 \\\n--display-name=test-run \\\n--config=config.yaml\n</code></pre> <p>Essentially, this command combines everything into one command: it first creates a VM with the specs specified by a configuration file, then loads a container specified again in the configuration file and finally it runs everything. A example of a config file could be:</p> <pre><code># config_cpu.yaml\nworkerPoolSpecs:\nmachineSpec:\nmachineType: n1-highmem-2\nreplicaCount: 1\ncontainerSpec:\nimageUri: gcr.io/&lt;project-id&gt;/&lt;docker-img&gt;\n</code></pre> <p>if you only want to run on CPU and another example for GPU:</p> <pre><code># config_gpu.yaml\nworkerPoolSpecs:\nmachineSpec:\nmachineType: n1-standard-8\nacceleratorType: NVIDIA_TESLA_T4\nacceleratorCount: 1\nreplicaCount: 1\ncontainerSpec:\nimageUri: gcr.io/&lt;project-id&gt;/&lt;docker-img&gt;\n</code></pre> <p>you can read more about the configuration formatting here and the different types of machines here. Try to execute a job using the <code>gcloud ai custom-jobs create</code> command. For additional documentation you can checkout the documentation on the command and this page and this page</p> </li> <li> <p>Assuming you manage to lunch a job, you should see an output like this:</p> <p> </p> <p>To executing the commands that is outputted to look at both the status and the progress of your job.</p> </li> <li> <p>In addition you can also visit the <code>Custom Jobs</code> tab in <code>training</code> part of Vertex AI</p> <p> </p> <p>Check it out.</p> </li> </ol> </li> </ol> <p>This ends the session on how to use Google cloud services for now. In a future session we are going to investigate a bit more of the services offered in GCP, in particular for deploying the models that we have just trained.</p>"},{"location":"s7_deployment/","title":"08. Model deployment","text":"<p>Slides</p> <p> </p> <p>Lets say that you have spend 1000 GPU hours and trained the most awesome model that you want to share with the world. One way to do this is of course to just place all your code in a github repository, upload a file with the trained model weights to your favorite online storage (assuming it is too big for github to handle) and ask people to just download your code and the weights to run the code by themselves. This is a fine approach in a small research setting, but in production you need to be able to deploy the model to an environment that is fully contained such that people can just execute without looking (too hard) at the code.</p> <p> </p>  Image credit  <p>In this session we try to look at methods specialized towards deployment of models on your local machine and also how to deploy services in the cloud.</p>"},{"location":"s7_deployment/apis/","title":"M22 - Requests and APIs","text":""},{"location":"s7_deployment/apis/#requests-and-apis","title":"Requests and APIs","text":"<p>Core Module</p> <p>Before we can get deployment of our models we need to understand concepts such as APIs and requests. The core reason for this is that we need a new abstraction layer ontop of our applications that are not python specific. While Python is the defacto language for machine learning, we cannot expect everybody else to use it and in particular we cannot expect network protocols (both locally and external) to be able to communicate with our python programs out of the box. For this reason we need to understand requests, in particular HTTP requests and how to create APIs that can interact with those requests.</p>"},{"location":"s7_deployment/apis/#requests","title":"Requests","text":"<p>When we are talking about requests, we are essentially talking about the communication method used in client-server types of architechtures. As shown in the image below, in this architechture the client (user) is going to send requests to a server (our machine learning application) and the server will give a response. For example the user may send a request of getting the class of a specific image, which our application will do and then send back the response in terms of a label.</p> <p></p>  Image credit  <p>The common way of of sending requests are called HTTP (Hypertext Transfer Protocol). It is essentially a specification of the intermediary transportation method between the client and server. A HTTP request essentially consist of two parts:</p> <ul> <li>A request URL: the location of the server we want to send our request to</li> <li>A request Method: what action we want to perform on the server.</li> </ul> <p>The common request methods are (case sensitive):</p> <ul> <li>GET: get data from the server</li> <li>POST/PUT: send data to the server</li> <li>DELETE: delete data on the server</li> </ul> <p>You can read more about the different methods here. For most machine learning applications, GET and POST are the core methods to remember. Additionally, if you want to read more about HTTP in general we highly recommend that you go over this comic strip protocol, but the TLDR is that it provides privacy, integrity and identification over the web.</p>"},{"location":"s7_deployment/apis/#exercises","title":"\u2754 Exercises","text":"<p>We are going to do a couple of exercises on sending requests using requests package to get familiar with the syntax.</p> <ol> <li> <p>Start by install the <code>requests</code> package</p> <pre><code>pip install requests\n</code></pre> </li> <li> <p>Afterwards, create a small script and try to execute the code</p> <pre><code>import requests\nresponse = requests.get('https://api.github.com/this-api-should-not-exist')\nprint(response.status_code)\n</code></pre> <p>As you can see from the syntax, we are sending a request using the GET method. This code should return status code 404. Take a look at this page that contains a list of status codes.</p> </li> <li> <p>Next lets call a page that actually exists</p> <pre><code>import requests\nresponse = requests.get('https://api.github.com')\nprint(response.status_code)\n</code></pre> <p>What is the status code now and what does it mean? Status codes are important when you have an application that is interacting with a server and want to make sure that it does not fail, which can be done with simple <code>if</code> statements on the status codes</p> <pre><code>if response.status_code == 200:\nprint('Success!')\nelif response.status_code == 404:\nprint('Not Found.')\n</code></pre> </li> <li> <p>Next try to call the following</p> <pre><code>response=requests.get(\"https://api.github.com/repos/SkafteNicki/dtu_mlops\")\n</code></pre> <p>which actually gives back a payload. Essentially, payload refers to any additional data that is sent from the client to the server or vice-versa. Try looking at the <code>response.content</code> attribute. What is the type of this attribute?</p> </li> <li> <p>You should hopefully observe that the <code>.content</code> atttribute is of type <code>bytes</code>. It is important to note that this is     the standard way of sending payloads to encode them into <code>byte</code> objects. To get a more human readable version of     the response, we can convert to JSON format</p> <pre><code>response.json()\n</code></pre> <p>Important to remember that a json object in python is just a nested dict, if you ever want to iterate over the object in some way.</p> </li> <li> <p>When we use the GET method we can additionally provide an <code>params</code> argument, that specifies what we want the server     to send back for a specific request URL:</p> <pre><code>response = requests.get(\n'https://api.github.com/search/repositories',\nparams={'q': 'requests+language:python'},\n)\n</code></pre> <p>Before looking at <code>reponse.json()</code> can you explain what the code does? You can try looking at this page for help.</p> </li> <li> <p>Sometimes the content of a page cannot be converted into <code>json</code>, because as already stated data is send as bytes.     Say that we want to download an image, which we can do in the following way</p> <pre><code>import requests\nresponse = requests.get('https://imgs.xkcd.com/comics/making_progress.png')\n</code></pre> <p>Try calling <code>response.json()</code>, what happens? Next try calling <code>response.content</code>. For actually getting the result in this case we would need to convert from bytes to an image:</p> <pre><code>with open(r'img.png','wb') as f:\nf.write(response.content)\n</code></pre> </li> <li> <p>The <code>get</code> method is the most useful method because it allows us to get data from the server. However, as stated in     the beginning multiple request methods exist, for example the POST method for sending data to the server. Try     executing:</p> <pre><code>pload = {'username':'Olivia','password':'123'}\nresponse = requests.post('https://httpbin.org/post', data = pload)\n</code></pre> <p>Investigate the response (this is an artifical example, because we actually does not control the server).</p> </li> <li> <p>Finally, we should also know that requests can be send directly from the command line using the <code>curl</code> command.     Sometimes it is easier to send a request directly from the terminal and sometimes it is easier to do directly from     a script.</p> <ol> <li> <p>Make sure you have <code>curl</code> installed, else find instruction on installing it. To check call <code>curl --help</code> for     the documentation on curl.</p> </li> <li> <p>To execute <code>requests.get('https://api.github.com')</code> using curl we would simply do</p> <pre><code>curl -X GET \"https://api.github.com\"\ncurl -X GET -I \"https://api.github.com\" # if you want the status code\n</code></pre> <p>Try it yourself.</p> </li> <li> <p>Try to redo some of the exercise yourself using <code>curl</code>.</p> </li> </ol> </li> </ol> <p>That ends the intro session on <code>requests</code>. Do not worry if you are still not completly comfortable with sending requests, we are going to return do how we do it in practise when we have actually created our own API. If you want to learn more about the <code>requests</code> package you can checkout this tutorial and if you want to see more example on how to use <code>curl</code> you can checkout this page</p>"},{"location":"s7_deployment/apis/#creating-apis","title":"Creating APIs","text":"<p>Requests are all about being on the client side of our client-server architechture. We are now going to move on to the server side where we will be learning about writing the APIs that requests can interact with. An application programming interface (API) is essentially the way of the developer (you) telling a user how to use the application that you have created. The API is an abstraction layer that allows the user to interact with our application in the way we want them to interact with it, without the user even having to look at the code.</p> <p>We can take the API from github as an example https://api.github.com. This API allows any user to retrieve, integrate and send data to github without ever having to visit their webpage. The API exposes multiple endpoints that have various functions:</p> <ul> <li>https://api.github.com/repos/OWNER/REPO/branches: checkout the branches on a given repository</li> <li>https://api.github.com/search/code: search through github for repositories</li> <li>https://api.github.com/repos/OWNER/REPO/actions/workflows: check the status of workflows for a given repository</li> </ul> <p>and we could go on. However, there may be functionality that github is not interested in users having access to and they may therefore choose not to have endpoints for specific features.</p> <p>The particular kind of API we are going to work with is called REST API (or RESTful API). The REST API specify specific contrains that a particular API needs to fullfill to be considered RESTful. You can read more about what the six guiding principals behind REST API is on this page but one of the most important to have in mind is that the client-server architechture needs to be stateless. This means that whenever a request is send to the server it needs to be self-contained (all information included) and the server cannot rely on any previously stored information from previous requests.</p> <p>To implement APIs in practise we are going to use FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. FastAPI is only one of many frameworks for defining APIs, however compared to other frameworks such as Flask and django it offers a sweet spot of being flexible enough to do what you want without having many additional (unnecessary) features.</p>"},{"location":"s7_deployment/apis/#exercises_1","title":"\u2754 Exercises","text":"<p>The exercises below are a condensed version of this and this tutorial. If you ever need context for the exercises, we can recommend trying to go through these. Additionally, we also provide this solution file that you can look through for help.</p> <ol> <li> <p>Install FastAPI</p> <pre><code>pip install fastapi\n</code></pre> <p>This contais the functions, modules, and variables we are going to need to define our interface.</p> </li> <li> <p>Additionally, also install <code>uvicorn</code> which is a package for defining low level server applications.</p> <pre><code>pip install uvicorn[standard]\n</code></pre> </li> <li> <p>Start by defining a small application like this in a file called <code>main.py</code>:</p> <pre><code>from fastapi import FastAPI\napp = FastAPI()\n@app.get(\"/\")\ndef read_root():\nreturn {\"Hello\": \"World\"}\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\nreturn {\"item_id\": item_id}\n</code></pre> <p>Importantly here is the use of the <code>@app.get</code> decorator. What could this decorator refer to? Explain what the two functions are probably doing.</p> </li> <li> <p>Next lets launch our app. Since we called our script <code>main.py</code> and we inside the script initialized our API with     <code>app = FastAPI</code>, our application that we want to deploy can be referenced by <code>main:app</code>:</p> <pre><code>uvicorn --reload --port 8000 main:app\n</code></pre> <p>this will launch a server at this page: <code>http://localhost:8000/</code>. As you will hopefully see, this page will return the content of the <code>root</code> function, like the image below. Remember to also check the output in your terminal as that will give info on when and how your application is being invoked.</p> <p> </p> <ol> <li> <p>What webpage should you open to get the server to return <code>1</code>?</p> </li> <li> <p>Also checkout the pages: <code>http://localhost:8000/docs</code> and <code>http://localhost:8000/redoc</code>. What does     these pages show?</p> </li> <li> <p>The power of the <code>docs</code> and <code>redoc</code> pages is that they allow you to easily test your application with their     simple UI. As shown in the image below, simply open the endpoint you want to test, click the <code>Try it out</code>     button, input any values and execute it. It will return both the corresponding <code>curl</code> command for invoking     your endpoint, the corresponding URL and response of you application. Try it out.</p> <p> </p> </li> <li> <p>You can also checkout <code>http://localhost:8000/openapi.json</code> to check out the schema that is generated     which essentially is a <code>json</code> file containing the overall specifications of your program.</p> </li> <li> <p>Try to access <code>http://localhost:8000/items/foo</code>, what happens in this case? When you specify types in your API,     FastAPI will automatically do type validation using pydantic, making sure users     can only access your API with the correct types. Therefore, remember to include types in your applications!</p> </li> </ol> </li> <li> <p>With the fundamentals in place lets configure it a bit more:</p> <ol> <li> <p>Lets start by changing the root function to include a bit more info. In particular we are also interested in     returning the status code so the end user can easily read that. Default status codes are included in the     http build-in python package:</p> <pre><code>from http import HTTPStatus\n@app.get(\"/\")\ndef root():\n\"\"\" Health check.\"\"\"\nresponse = {\n\"message\": HTTPStatus.OK.phrase,\n\"status-code\": HTTPStatus.OK,\n}\nreturn response\n</code></pre> <p>try to reload the app and see what is returned now. You should not have to re-launch the app because we initialized the app with the <code>--reload</code> argument.</p> </li> <li> <p>When we decorate our functions with <code>@app.get(\"/items/{item_id}\")</code>, <code>item_id</code> is in the case what we call a     path parameters because it is a parameter that is directly included in the path of our endpoint. We have     already seen how we can restrict a path to a single type, but what if we want to restrict it to specific values?     This is often the case if we are working with parameters of type <code>str</code>. In this case we would need to define a     <code>enum</code>:</p> <pre><code>from enum import Enum\nclass ItemEnum(Enum):\nalexnet = \"alexnet\"\nresnet = \"resnet\"\nlenet = \"lenet\"\n@app.get(\"/restric_items/{item_id}\")\ndef read_item(item_id: ItemEnum):\nreturn {\"item_id\": item_id}\n</code></pre> <p>Add this API, reload and execute both a valid parameter and a non-valid parameter.</p> </li> <li> <p>In contrast to path parameters we have query parameters. In the requests exersices we saw an example of this     where we were calling https://api.github.com/search/code with the query <code>'q': 'requests+language:python'</code>.     Any parameter in FastAPI that is not a path parameter, will be considered a query parameter:</p> <pre><code>@app.get(\"/query_items\")\ndef read_item(item_id: int):\nreturn {\"item_id\": item_id}\n</code></pre> <p>Add this API, reload and figure out how to pass in a query parameter.</p> </li> <li> <p>We have until now worked with the <code>.get</code> method, but lets also see an example of the <code>.post</code> method. As already     described the POST request method is used for uploading data to the server. Here is a simple app that saves     username and password in a database (please never implement this in real life like this):</p> <pre><code>database = {'username': [ ], 'password': [ ]}\n@app.post(\"/login/\")\ndef login(username: str, password: str):\nusername_db = database['username']\npassword_db = database['password']\nif username not in username_db and password not in password_db:\nwith open('database.csv', \"a\") as file:\nfile.write(f\"{username}, {password} \\n\")\nusername_db.append(username)\npassword_db.append(password)\nreturn \"login saved\"\n</code></pre> <p>Make sure you understand what the function does and then try to execute it a couple of times to see your database updating. It is important to note that we sometimes in the following exercises use the <code>.get</code> method and sometimes the <code>.post</code> method. For our usage it does not really matter.</p> </li> </ol> </li> <li> <p>We are now moving to figuring out how to provide different standard inputs like text, images, json to our APIs. It     is important that you try out each example yourself and in particular you look at the <code>curl</code> commands that are     necessary to invoke each application.</p> <ol> <li> <p>Here is a small application, that takes a single text input</p> <pre><code>@app.get(\"/text_model/\")\ndef contains_email(data: str):\nregex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\nresponse = {\n\"input\": data,\n\"message\": HTTPStatus.OK.phrase,\n\"status-code\": HTTPStatus.OK,\n\"is_email\": re.fullmatch(regex, data) is not None\n}\nreturn response\n</code></pre> <p>What does the application do? Try it out yourself</p> </li> <li> <p>Lets say we wanted to extend the application to check for a specific email domain, either <code>gmail</code> or <code>hotmail</code>.     And lets say we want to feed this into our application as a <code>json</code> object e.g.</p> <pre><code>{\n\"email\": \"mlops@gmail.com\",\n\"domain_match\": \"gmail\"\n}\n</code></pre> <p>Figure out how to alter the <code>data</code> parameter such that it takes in the <code>json</code> object and make sure to extend the application to check if the email and domain also matches. Hint: take a look at this page</p> </li> <li> <p>Lets move on to an application that requires a file input:</p> <pre><code>from fastapi import UploadFile, File\nfrom typing import Optional\n@app.post(\"/cv_model/\")\nasync def cv_model(data: UploadFile = File(...)):\nwith open('image.jpg', 'wb') as image:\ncontent = await data.read()\nimage.write(content)\nimage.close()\nresponse = {\n\"input\": data,\n\"message\": HTTPStatus.OK.phrase,\n\"status-code\": HTTPStatus.OK,\n}\nreturn response\n</code></pre> <p>A couple of new things are going on here: we use the specialized <code>UploadFile</code> and <code>File</code> bodies in our input definition. Addtionally, we added the <code>async</code>/<code>await</code> keywords. Figure out what everything does and try to run the application (you can use any image file you like).</p> </li> <li> <p>The above application actually does not do anything. Lets add opencv     as a package and lets resize the image. It can be done with the following three lines:</p> <pre><code>import cv2\nimg = cv2.imread(\"image.jpg\")\nres = cv2.resize(img, (h, w))\n</code></pre> <p>Figure out where to add them in the application and additionally add <code>h</code> and <code>w</code> as optional parameters, with a default value of 28. Try running the application where you specify everything and one more time where you leave out <code>h</code> and <code>w</code>.</p> </li> <li> <p>Finally, lets also figure out how to return a file from our application. You will need to add the following lines:</p> <pre><code>from fastapi.responses import FileResponse\ncv2.imwrite('image_resize.jpg', res)\nFileResponse('image_resize.jpg')\n</code></pre> <p>Figure out where to add them to the code and try running the application one more time to see that you actually get a file back with the resized image.</p> </li> </ol> </li> <li> <p>(Optional) Lets try to figure out how to use FastAPI in a machine learning context. Below is a script that downloads     a <code>VisionEncoderDecoder</code> from     huggingface     . The model can use to create captions for a given image. Thus calling</p> <pre><code>predict_step(['s7_deployment/exercise_files/my_cat.jpg'])\n</code></pre> <p>returns a list of strings like <code>['a cat laying on a couch with a stuffed animal']</code> (try this yourself). Create a FastAPI application that can do inference using this model e.g. it should take in an image, preferrably an optional <code>json</code> object for configuring some of the hyperparameters (like <code>max_length</code>) and should return a string containing the generated caption.</p> <pre><code>from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ngen_kwargs = {\"max_length\": 16, \"num_beams\": 8, \"num_return_sequences\": 1}\ndef predict_step(image_paths):\nimages = []\nfor image_path in image_paths:\ni_image = Image.open(image_path)\nif i_image.mode != \"RGB\":\ni_image = i_image.convert(mode=\"RGB\")\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\npixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\npreds = [pred.strip() for pred in preds]\nreturn preds\n</code></pre> </li> <li> <p>As the final step, we want to figure out how do include our FastAPI application in a docker container as it will help     us when we want to deploy in the cloud because docker as always can take care of the dependencies for our     application. For the following you can take whatever previous FastAPI application as the base application for the     container</p> <ol> <li> <p>Start by creating a <code>requirement.txt</code> file for you application. You will atleast need <code>fastapi</code> and <code>uvicorn</code> in     the file and we as always recommend that you are specific about the version you want to use:</p> <pre><code>fastapi&gt;=0.68.0,&lt;0.69.0\nuvicorn&gt;=0.15.0,&lt;0.16.0\n# add anything else you application needs to be able to run\n</code></pre> </li> <li> <p>Next create a <code>Dockerfile</code> with the following content</p> <pre><code>FROM python:3.9\nWORKDIR /code\nCOPY ./requirements.txt /code/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\nCOPY ./app /code/app\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n</code></pre> <p>The above assumes that your file structure looks like this</p> <pre><code>.\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>Hopefully all these step should look familiar if you already went through module M9, except for maybe the last line. However, this is just the standard way that we have run our FastAPI applications as the last couple of exercises, this time with some extra arguments regarding the ports we allow.</p> </li> <li> <p>Next build the corresponding docker image</p> <pre><code>docker build -t my_fastapi_app .\n</code></pre> </li> <li> <p>Finally, run the image such that a container is spinned up that runs our application. The important part here is     to remember to specify the <code>-p</code> argument (p for port) that should be the same number as the port we have     specified in the last line of our Dockerfile.</p> <pre><code>docker run --name mycontainer -p 80:80 myimage\n</code></pre> </li> <li> <p>Check that everything is working by going to the corresponding localhost page     http://localhost/items/5?q=somequery</p> </li> </ol> </li> </ol> <p>This ends the module on APIs. If you want to go further in this direction we highly recommend that you checkout bentoml that is an API standard that focuses solely on creating easy to understand APIs and services for machine learning applications. Additionally, we can also highly recommend checking out Postman that can help design, document and in particular test the API you are writing to make sure that they work as expected.</p>"},{"location":"s7_deployment/cloud_deployment/","title":"M24 - Cloud Deployment","text":""},{"location":"s7_deployment/cloud_deployment/#cloud-deployment","title":"Cloud deployment","text":"<p>Core Module</p> <p>We are now returning to using the cloud. In this module you should have gone through the steps of having your code in your github repository to automatically build into a docker container, store that, store data and pull it all together to make a training run. After the training is completed you should hopefully have a file stored in the cloud with your trained model weights.</p> <p>Todays exercises will be about serving those model weights to an end user. We focus on two different ways of deploying our model, <code>Google cloud functions</code> and <code>Google Vertex AI endpoints</code>.</p>"},{"location":"s7_deployment/cloud_deployment/#cloud-functions","title":"Cloud Functions","text":"<p>Cloud functions are the easiest way to get started with deployment because they are what is called serverless. For serverless deployment we still need a server to do the actual workload, however the core concept is that you do you have to manage the server. Everything is magically taken care of behind the scene.</p>"},{"location":"s7_deployment/cloud_deployment/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Go to the start page of <code>Cloud Functions</code>. Can be found in the sidebar on the homepage or you can just    search for it. Activate the service if not already active.</p> </li> <li> <p>Click the <code>Create Function</code> button which should take you to a screen like the image below. Give it a name,     set the server region to somewhere close by and change the authentication policy to     <code>Allow unauthenticated invocations</code> so we can access it directly from a browser. Remember to note down the     URL of the service somewhere.      </p> </li> <li> <p>On the next page, for <code>Runtime</code> pick the <code>Python 3.9</code> option. This will make the inline editor show both     a <code>main.py</code> and <code>requirements.py</code> file. Look over them. Click the <code>Deploy</code> button in the lower left corner.</p> </li> <li> <p>Afterwards you should see a green check mark beside your function meaning that it is deployed. Click the     <code>Test function</code> button which will take you to the testing page.      </p> </li> <li> <p>If you know what the application does, it should come as no surprise that it can run without any input. We     therefore just send an empty request by clicking the <code>Test The Function</code> button. Does the function return     the output you expected? Wait for the logs to show up. What do they show?</p> <ol> <li> <p>What should the <code>Triggering event</code> look like in the testing prompt for the program to respond with</p> <pre><code>Good day to you sir!\n</code></pre> <p>Try it out.</p> </li> <li> <p>Click on the metrics tab. Identify what each panel is showing.</p> </li> <li> <p>Go to the trigger tab and go to the url for the application.</p> </li> <li> <p>Checkout the logs tab. You should see that your application have already been invoked multiple times. Also try     to execute this command in a terminal:</p> <pre><code>gcloud functions logs read\n</code></pre> </li> </ol> </li> <li> <p>Next, we are going to create an application that actually takes some input so we can try to send it requests.     We provide a very simple     sklearn_cloud_function.py script     to get started.</p> <ol> <li> <p>Figure out what the script does and run the script. This should create a file with trained model.</p> </li> <li> <p>Next create a storage bucket and upload the model file to the bucket. You can either do this through the     webpage or run the following commands:</p> <pre><code>gsutil mb gs://&lt;bucket-name&gt;  # mb stands for make bucket\ngsutil cp &lt;file-name&gt; gs://&lt;bucket-name&gt;  # cp stands for copy\n</code></pre> <p>check that the file is in the bucket.</p> </li> <li> <p>Create a new cloud function with the same initial settings as the first one. Choose also the <code>Python 3.9</code>     but this time change code to something that can actually use the model we just uploaded. Here is a code     snippet to help you:</p> <pre><code>from google.cloud import storage\nimport pickle\nBUCKET_NAME = ...\nMODEL_FILE = ...\nclient = storage.Client()\nbucket = client.get_bucket(BUCKET_NAME)\nblob = bucket.get_blob(MODEL_FILE)\nmy_model = pickle.loads(blob.download_as_string())\ndef knn_classifier(request):\n\"\"\" will to stuff to your request \"\"\"\nrequest_json = request.get_json()\nif request_json and 'input_data' in request_json:\ndata = request_json['input_data']\ninput_data = list(map(int, data.split(',')))\nprediction = my_model.predict([input_data])\nreturn f'Belongs to class: {prediction}'\nelse:\nreturn 'No input data received'\n</code></pre> <p>Some notes: * For locally testing the above code you will need to install the <code>google-cloud-storage</code> python package * Remember to change the <code>Entry point</code> * Remember to also fill out the <code>requirements.txt</code> file. You need at least two packages to run the application     with <code>google-cloud-storage</code> being one of them. * If you deployment fails, try to go to the <code>Logs Explorer</code> page in <code>gcp</code> which can help you identify why.</p> </li> <li> <p>When you have successfully deployed the model, try to make predictions with it.</p> </li> </ol> </li> <li> <p>You can finally try to redo the exercises deploying a Pytorch application. You will essentially     need to go through the same steps as the sklearn example, including uploading a trained model     to a storage, write a cloud function that loads it and return some output. You are free to choose     whatever Pytorch model you want.</p> </li> </ol>"},{"location":"s7_deployment/cloud_deployment/#cloud-run","title":"Cloud Run","text":"<p>Cloud functions are great for simple deployments, that can be encapsulated in a single script with only simple requirements. However, they do not really scale with more advance applications that may depend on multiple programming languages. We are already familiar with how we can deal with this through containers and Cloud Run is the corresponding service in GCP for deploying containers.</p>"},{"location":"s7_deployment/cloud_deployment/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>We are going to start locally by developing a small app that we can deploy. We provide two small examples to choose     from: first a small FastAPI app consisting of this     .py file     and this     dockerfile     . Secondly a small streamlit application consisting of just this     dockerfile     . You are free to choose which application to work with.</p> <ol> <li> <p>Start by going over the files belonging to your choice app and understand what it does.</p> </li> <li> <p>Next build the docker image belonging to the app</p> <pre><code>docker build -f &lt;dockerfile&gt; . -t gcp_test_app:latest\n</code></pre> </li> <li> <p>Next tag and push the image to your container registry</p> <pre><code>docker tag gcp_test_app gcr.io/&lt;project-id&gt;/gcp_test_app\ndocker push gcr.io/&lt;project-id&gt;/gcp_test_app\n</code></pre> <p>afterwards check you container registry to check that you have successfully pushed the image.</p> </li> </ol> </li> <li> <p>Next go to <code>Cloud Run</code> in the cloud consol an enable the service</p> </li> <li> <p>Click the <code>Create Service</code> button which should bring you to a page similar to the one below</p> <p> </p> <p>Do the following: * Click the select button, which will bring up all build containers and pick the one you want to deploy. In the     future you probably want to choose the Continuously deploy new revision from a source repository such that a new     version is always deployed when a new container is build. * Hereafter, give the service a name and select the region. We recommend do choose a region close to you, however     it does not really matter that much for our use case * Set the authentication method to Allow unauthenticated invocations such that we can call it without     providing credentials. In the future you may only set that authenticated invocations are allowed. * Expand the Container, Connections, Security tab and edit the port such that it matches the port exposed in your     chosen application.</p> <p>Finally, click the create button and wait for the service to be deployed (may take some time).</p> </li> <li> <p>If you manage to deploy the service you should see a image like this:</p> <p> </p> <p>You can now access you application by clicking url. This will access the root of your application, so you may need to add <code>/</code> or <code>/&lt;path&gt;</code> to the url depending on how the app works.</p> </li> <li> <p>(Optional) Everything we just did to deploy an container can be reproduced using the following command:</p> <pre><code>gcloud run deploy $APP --image $TAG --platform managed --region $REGION --allow-unauthenticated\n</code></pre> <p>and checked using these two commands</p> <pre><code>gcloud run services list\ngcloud run services describe $APP --region $REGION\n</code></pre> <p>feel free to experiment doing the deployment from the command line.</p> </li> <li> <p>As an final exercise, we recommend redoing the above deployment steps with your own developed MNIST code such that     you get more experience with deploying a machine learning application.</p> </li> </ol> <p>That ends the exercises on deployment. The exercises above is just a small taste of what deployment has to offer. In both sections we have explicitly chosen to work with serverless deployments. But what if you wanted to do the opposite e.g. being the one in charge of the management of the cluster that handles the deployed services? If you are really interested in taking deployment to the next level should get started on kubernetes which is the de-facto open-source container orchestration platform that is being used in production environments. If you want to deep dive we recommend starting here which describes how to make pipelines that are a necessary component before you start to create your own kubernetes cluster.</p>"},{"location":"s7_deployment/local_deployment/","title":"M23 - Local Deployment","text":""},{"location":"s7_deployment/local_deployment/#local-deployment","title":"Local Deployment","text":"<p>Regardless of your application, model and usecase, the first starting point of serving your model should always be to deploy it locally. The simple reason for that is debugging: if you deploy directly to the cloud you often get less verbose error message and/or the iteration time is much slower because it simply takes much longer time to deploy to the cloud than locally. Locally should therefore always be the first step with any new application.</p> <p></p> <p>For this module we are going to focus on deployment of deep learning models, in particular Pytorch models which is used throughout the course. Pytorch has historically been developed for research purposed, where iterating with quick ideas was valued over fast computations. This is evident since Pytorch uses an dynamic graph underneath to represent the computational graph that is being created whenever you are running calculations. The graph is important, as it keeps track on how to do backpropergation though your Pytorch application. However, running code dynamically is notoriously slower than compiling your code before running it. Lets therefore first consider another way of compiling our code.</p>"},{"location":"s7_deployment/local_deployment/#compilation","title":"Compilation","text":"<p>If you ever coded in any low-level language such as c, fortran or c++ you should be familiar with the term compiling. Compiling is the task of taken a computer program written in one language and translating it into another. In most cases this means taken whatever you have written in your preferred programming language and translating it into machine code that the computer can execute. But what does compilation have to do with coding Pytorch models?</p> <p>It happens to be that <code>Pytorch</code> comes with its own compiler that can optimize your model for you. It can be found in the submodule <code>torch.jit</code>. Jit stands for just-in-time, meaning that compilation runs at the same time we are executing the code. If you know anything about low-level languages such c/c++ you know that we normally compile the code before we run it. With <code>jit</code> we essentially merges the two phases into one. <code>jit</code> has two types of compilation modes, called respective script and trace. We are in the exercises going to look at script as it is the easiest to get started with and works without any code changes for nearly all kind of models. If you ever encounter that script does not work for you then trace can be used which is more general.</p> <p>The major reasons why we want to compile our models with <code>torch.jit</code> are:</p> <ul> <li>Scriptet code can be invoked in its own interpreter, which is basically a restricted Python interpreter.   This interpreter does not acquire the Global Interpreter Lock (GIL), and so many requests can be processed on the   same instance simultaneously.</li> <li>This scriptet format allows us to save the whole model to disk and load it into another environment, such as in a   server written in a language other than Python</li> <li>Scriptet code gives us a representation in which we can do compiler optimizations on the code to provide   more efficient execution</li> <li>Scriptet code allows us to interface with many backend/device runtimes that require a broader view of the   program than individual operators.</li> </ul>"},{"location":"s7_deployment/local_deployment/#exercises","title":"\u2754 Exercises","text":"<p>We are here going to look at <code>torch.jit.script</code> for compiling our code.</p> <ol> <li> <p>To see the difference in the this exercises, we start out with a large model. Download one of the large image    classification models from <code>torchvision</code> such as <code>ResNet-152</code>. For the purpose of the exercise it does not matter    if you work with a random initialized model or a pretrained version.</p> </li> <li> <p>Next try to script the model using <code>torch.jit.script</code>. You can find the documentation    here.</p> </li> <li> <p>Just to confirm that by compiling our model using <code>torch.jit.script</code> did not change the output of our model, try    checking that the output of the scripted model corresponds to the output of the non-scripted model. You can do this    on a single random datapoint, and you should check that the top-5 prediced classes are the same</p> </li> </ol> <pre><code>assert torch.allclose(unscripted_top5_indices, scripted_top5_indices)\n</code></pre> <p>Hint: use torch.topk.</p> <ol> <li>Finally, try benchmarking the non-scripted model against the scripted model. I recommend using the build-in    benchmarker in Pytorch: <code>torch.utils.benchmark.Timer</code>, which you can read more about how to use    here. Do you see a increase in performance of the    scripted model compared to the non-scriptet model. If so, what is the percentage increase in efficiency?</li> </ol>"},{"location":"s7_deployment/local_deployment/#torchserve","title":"Torchserve","text":"<p>For locally deploying our model we are going to look at Torchserve. Torchserve (illustrated below) is a combined services for packaging and serving multiple Pytorch at the same time.</p> <p> </p>  Image credit  <p>Before we go into details of Torchmetrics, an important question is why we need such an abstraction on top of our developed model. Why cant we just do:</p> <pre><code>python inference.py --my_model model_checkpoint.pt --new_datapoint img.png\n</code></pre> <p>If we where never going to do anything else than just calling the model ourself then it is probably not worth adding anything else. However, if we ever want anyone else to interact with our model, we need to comply with standard ways of requesting and sending data. This is especially true when the next step is to start deploying our model in the cloud. Torchserve essentially brings in a inference API on top of our model that turns our model into a client-server type of system: the client (user) is going to send requests to a server (our application) and the server will give an response. The request will be send as a standard HTTP requests which Torchserve will help us decode into a useful input which we can then do inference on and return the result, again as an standardized HTTP response. Torchserve is in that regard similar to FastAPI or Flask if you have ever used one of those frameworks.</p> <p>Finally, the packaging part of Torchserve is necessary because we cannot give a Torchserve a raw file of trained model weights as these essentially is just a list of floats. We need a file that both contains the model definition and the trained weights, such that the model essentially becomes independent of the python interpreter.</p>"},{"location":"s7_deployment/local_deployment/#exercises_1","title":"\u2754 Exercises","text":"<p>Torchserve can be a bit rough around the edges but is fairly easy to work with. We are largely going to follow the instructions listed in the readme file for Torchserve. The intention in these exercises is to serve a Resnet type neural network that is trained for classification on ImageNet. Additional documentation can be found here.</p> <ol> <li> <p>Install <code>torchserve</code> and its dependencies. There are separate instructions on the    homepage depending on you are using Windows, WSL or Linux/MAC.</p> </li> <li> <p>Create a folder called <code>model_store</code>. This is where we will store the model that we are going to deploy</p> </li> <li> <p>Try to run the <code>torchserve --model-store model_store</code> command. If the service starts with no errors, you    have installed it correctly and can continue the exercise. Else it is Googling time!</p> </li> <li> <p>Next lets create a model we can serve. If you have done the previous exercises on compiling using scripting, we    highly recommend to initialize and save such model</p> </li> </ol> <pre><code>model = ResnetFromTorchVision(pretrained=True)\nscript_model = torch.jit.script(model)\nscript_model.save('deployable_model.pt')\n</code></pre> <ol> <li>Call the model archiver. We have provided a file called <code>index_to_name.json</code> that maps from predicted class    indices to interpretable class name e.g. <code>1-&gt;\"goldfish\"</code>. This file should be provided as the <code>extra-files</code>    argument such that the deployed model automatically outputs the class name. Note that this files of course    only works for models trained on imagenet.</li> </ol> <pre><code>torch-model-archiver \\\n--model-name my_fancy_model\n    --version 1.0 \\\n--serialized-file path/to/serialized_model.pt \\\n--export-path model_store\n    --extra-files index_to_name.json\n    --handler image_classifier\n</code></pre> <ol> <li> <p>Checkout the <code>model_store</code> folder. Has the model archiver correctly created a model (with <code>.mar</code> extension)    inside the folder?</p> </li> <li> <p>Finally, we are going to deploy our model and use it:</p> </li> <li> <p>Start serving your model in one terminal:</p> <pre><code>torchserve --start --ncs --model-store model_store --models my_fancy_model=my_fancy_model.mar\n</code></pre> </li> <li> <p>Next, pick a image that you want to do inference on. It can be any image that you want but try to pick       one that actually contains an object from the set of imagenet classes. I have also provided a image of       my own cat in the <code>my_cat.jpg</code> file.</p> </li> <li> <p>Open another terminal, which we are going to use for inference. The easiest way to do inference is using       <code>curl</code> directly in the terminal but you are also free to experiment with the <code>requests</code> API directly in       python. Using <code>curl</code> should look something like this</p> <pre><code>curl http://127.0.0.1:8080/predictions/my_fancy_model -T my_image.jpg\n</code></pre> </li> <li> <p>Torchserve supports serving multiple models, not just one. Create a new vision model (either another resnet model    or something similar), script it, save it, archive it in the save model store folder and then re-run torchserve    like this</p> </li> </ol> <pre><code>torchserve --start --ncs --model-store model_store --models all\n</code></pre> <p>Make sure that you can do inference with both models by calling <code>curl</code>.</p> <p>That ends the module on local deployment. Hopefully in this phase you have gained a bit experience with sending HTTP requests as this will be very important in the next module when we will try to deploy the models in the cloud.</p>"},{"location":"s8_monitoring/","title":"Monitoring","text":"<p>We have now reached the end of our machine learning pipeline. We have successfully developed, trained and deployed a machine learning model. However, the question then becomes if you can trust that your newly deployed model still works as expected after 1 day without you intervening? What about 1 month? What about 1 year?</p> <p>There may be corner cases where an ML models is working as expected, but the wast majority of ML models will perform worse over time because they are not generalizing well enough. For example, assume you have just deployed an application that classifies images from phones, when suddenly a new phone comes out with a new kind of sensor that takes images that either have very weird aspect ratio or something else your model is not robust towards. There is nothing wrong with this, you can essentially just retrain your model on new data that accounts for this corner case, however you need a mechanisms that informs you.</p> <p>This is very monitoring comes into play. Monitoring practices are in charge of collecting any information about your application in some format that can then be analyzed and reacted on. Monitoring is essential to securing the longevity of your applications.</p> <p>As with many other sub-fields within MLOps we can divide monitoring into classic monitoring and ML specific monitoring. Classic monitoring (known from classic DevOps) is often about</p> <ul> <li>Errors: Is my application workings without problems?</li> <li>Logs: What is actually going on?</li> <li>Performance: How fast is my application?</li> </ul> <p>All these are basic information you are interested in regardless of what application type you are trying to deploy. However, then there are ML related monitoring that especially relates data. Take the example above, with the new phone, this we would in general consider to be data drifting problem e.g. the data you are trying to do inference on have drifted away from the distribution of data your model was trained on. Such monitoring problems are unique to machine learning applications and needs to be handled separately.</p> <p>We are in this session going to see examples of both kinds of monitoring.</p>"},{"location":"s8_monitoring/data_drifting/","title":"M25 - Data Drifting","text":""},{"location":"s8_monitoring/data_drifting/#data-drifting","title":"Data drifting","text":"<p>Data drifting is one of the core reasons for model accuracy degrades over time in production. For machine learning models, data drift is the change in model input data that leads to model performance degradation. In practical terms, this means that the model is receiving input that is outside of the scope that it was trained on, as seen in the figure below. This shows that the underlying distribution of a particular feature has slowly been increasing in value over two years</p> <p></p>  Image credit  <p>In some cases, it may be that if you normalize some feature in a better way that you are able to generalize your model better, but this is not always the case. The reason for such a drift is commonly some external factor that you essentially have no control over. That really only leaves you with one option: retrain your model on the newly received input features and deploy that model to production. This process is probably going to repeat over the lifetime of your application if you want to keep it up-to-date with the real world.</p> <p></p>  Image credit  <p>We have now come up with a solution to the data drift problem, but there is one important detail that we have not taken care of: When we should actually trigger the retraining? We do not want to wait around for our model performance to degrade, thus we need tools that can detect when we are seeing a drift in our data.</p>"},{"location":"s8_monitoring/data_drifting/#exercises","title":"\u2754 Exercises","text":"<p>For these exercises we are going to use the framework Evidently developed by EvidentlyAI. Evidently currently supports both detection for both regression and classification models. The exercises are in large taken from here and in general we recommend if you are in doubt about an exercise to look at the docs for API and examples (their documentation can be a bit lacking sometimes, so you may also have to dive into the source code).</p> <p>Additionally, we want to stress that data drift detection, concept drift detection etc. is still an active field of research and therefore exist multiple frameworks for doing this kind of detection. In addition to Evidently, we can also mention NannyML, WhyLogs and deepcheck.</p> <ol> <li> <p>Start by install Evidently</p> <pre><code>pip install evidently\n</code></pre> <p>you will also need <code>scikit-learn</code> and <code>pandas</code> installed if you do not already have it.</p> </li> <li> <p>Hopefully you already gone through session S7 on deployment. As part of the deployment     to GCP functions you should have developed a application that can classify the     iris dataset, based on a model trained by this     script     . We are going to convert this into a FastAPI application for the purpose here:</p> <ol> <li> <p>Convert your GCP function into a FastAPI application. The appropriate <code>curl</code> command should look something like     this:</p> <pre><code>curl -X 'POST' \\\n'http://127.0.0.1:8000/iris_v1/?sepal_length=1.0&amp;sepal_width=1.0&amp;petal_length=1.0&amp;petal_width=1.0' \\\n-H 'accept: application/json' \\\n-d ''\n</code></pre> <p>and the response body should look like this:</p> <pre><code>{\n\"prediction\": \"Iris-Setosa\",\n\"prediction_int\": 0\n}\n</code></pre> <p>We have implemented a solution in this file (called v1) if you need help.</p> </li> <li> <p>Next we are going to add some functionality to our application. We need to add that the input for the user is     saved to a database whenever our application is called. However, to not slow down the response to our user we     want to implement this as an background task. A background task is a function that should be executed after     the user have got their response. Implement a background task that save the user input to a database implemented     as a simple <code>.csv</code> file. You can read more about background tasks     here. The header of the database should look     something like this:</p> <pre><code>time, sepal_length, sepal_width, petal_length, petal_width, prediction\n2022-12-28 17:24:34.045649, 1.0, 1.0, 1.0, 1.0, 1\n2022-12-28 17:24:44.026432, 2.0, 2.0, 2.0, 2.0, 1\n...\n</code></pre> <p>thus both input, timestamp and predicted value should be saved. We have implemented a solution in this file (called v2) if you need help.</p> </li> <li> <p>Call you API a number of times to generate some dummy data in the database.</p> </li> </ol> </li> <li> <p>Create a new <code>data_drift.py</code> file where we are going to implement the data drifting detection and reporting. Start     by adding both the real iris data and your generated dummy data as pandas dataframes.</p> <pre><code>import pandas as pd\nfrom sklearn import datasets\nreference_data = datasets.load_iris(as_frame='auto').frame\ncurrent_data = pd.read_csv('prediction_database.csv')\n</code></pre> <p>if done correctly you will most likely end up with two dataframes that look like</p> <pre><code># reference_data\nsepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                  5.1               3.5                1.4               0.2       0\n1                  4.9               3.0                1.4               0.2       0\n...\n148                6.2               3.4                5.4               2.3       2\n149                5.9               3.0                5.1               1.8       2\n[150 rows x 5 columns]\n\n# current_data\ntime                         sepal_length   sepal_width   petal_length   petal_width   prediction\n2022-12-28 17:24:34.045649   1.0            1.0            1.0           1.0           1\n...\n2022-12-28 17:24:34.045649   1.0            1.0            1.0           1.0           1\n[10 rows x 5 columns]\n</code></pre> <p>Standardize the dataframes such that they have the same column names and drop the time column from the <code>current_data</code> dataframe.</p> </li> <li> <p>We are now ready to generate some reports about data drifting:</p> <ol> <li> <p>Try executing the following code:</p> <pre><code>from evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset\nreport = Report(metrics=[DataDriftPreset()])\nreport.run(reference_data=reference, current_data=current)\nreport.save_html('report.html')\n</code></pre> <p>and open the generated <code>.html</code> page. What does it say about your data? Have it drifted? Make sure to poke around to understand what the different plots are actually showing.</p> </li> <li> <p>Data drifting is not the only kind of reporting evidently can make. We can also get reports on the data quality.     Try first adding a few <code>Nan</code> values to your reference data. Secondly, try changing the report to</p> <pre><code>from evidently.metric_preset import DataDriftPreset, DataQualityPreset\nreport = Report(metrics=[DataDriftPreset(), DataQualityPreset()])\n</code></pre> <p>and re-run the report. Checkout the newly generated report. Again go over the generated plots and make sure that it picked up on the missing values you just added.</p> </li> <li> <p>The final report present we will look at is the <code>TargetDriftPreset</code>. Target drift means that our model is     over/under predicting certain classes e.g. or general terms the distribution of predicted values differs from     the ground true distribution of targets. Try adding the <code>TargetDriftPreset</code> to the <code>Report</code> class and re-run the     analysis and inspect the result. Have your targets drifted?</p> </li> </ol> </li> <li> <p>Evidently reports are meant for debugging, exploration and reporting of results. However, as we stated in the     beginning, what we are actually interested in methods automatically detecting when we are beginning to drift. For     this we will need to look at Test and TestSuites:</p> <ol> <li> <p>Lets start with a simple test that checks if there are any missing values in our dataset:</p> <pre><code>from evidently.test_suite import TestSuite\nfrom evidently.tests import TestNumberOfMissingValues\ndata_test = TestSuite(tests=[TestNumberOfMissingValues()])\ndata_test.run(reference_data=reference, current_data=current)\n</code></pre> <p>again we could run <code>data_test.save_html</code> to get a nice view of the results (feel free to try it out) but additionally we can also call <code>data_test.as_dict()</code> method that will give a dict with the test results. What dictionary key contains the if all tests have passed or not?</p> </li> <li> <p>Take a look at this colab notebook     that contains all tests implemented in Evidently. Pick 5 tests of your choice, where at least 1 fails by default     and implement them as a <code>TestSuite</code>. Then try changing the arguments of the test so they better fit your     usecase and get them all passing.</p> </li> </ol> </li> <li> <p>(Optional) When doing monitoring in practice, we are not always interested in running on all data collected from our     API maybe only the last <code>N</code> entries or maybe just from the last hour of observations. Since we are already logging     the timestamps of when our API is called we can use that for filtering. Implement a simple filter that either takes     an integer <code>n</code> and returns the last <code>n</code> entries in our database or some datetime <code>t</code> that filters away observations     earlier than this.</p> </li> <li> <p>Evidently by default only supports structured data e.g. tabular data (so does nearly every other framework). Thus,     the question then becomes how we can extend unstructured data such as images or text? The solution is to extract     structured features from the data which we then can run the analysis on.</p> <ol> <li> <p>(Optional) For images the simple solution would be to flatten the images and consider each pixel a feature,     however this does not work in practice because changes in the individual pixels does not really tell anything     about the image. Instead we should derive some feature such as:</p> <ul> <li>Average brightness</li> <li>Contrast of image</li> <li>Image sharpness</li> <li>...</li> </ul> <p>These are all numbers that can make up a feature vector for a image. Try out doing this yourself, for example by extracting such features from MNIST and FashionMNIST datasets, and check if you can detect a drift between the two sets.</p> </li> <li> <p>(Optional) For text a common approach is to extra some higher level embedding such as the very classical     GLOVE embedding. Try following     this tutorial     to understand how drift detection is done on text.</p> </li> <li> <p>Lets instead take a deep learning based approach to doing this. Lets consider the     CLIP model, which is normally used to do image captioning. For our purpose this     is perfect because we can use the model to get abstract feature embeddings for both images and text:</p> <pre><code>from PIL import Image\nimport requests\n# requires transformers package: pip install transformers\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# set either text=None or images=None when only the other is needed\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\nimg_features = model.get_image_features(inputs['pixel_values'])\ntext_features = model.get_text_features(inputs['input_ids'], inputs['attention_mask'])\n</code></pre> <p>Both <code>img_features</code> and <code>text_features</code> are in this case a <code>(512,)</code> abstract feature embedding, that should be able to tell us something about our data distribution. Try using this method to extract features on two different datasets like CIFAR10 and SVHN if you want to work with vision or IMDB movie review and Amazon review for text. After extracting the features try running some of the data distribution testing you just learned about.</p> </li> </ol> </li> <li> <p>(Optional) If we have multiple applications and want to run monitoring for each application we often want also the     monitoring to be a deployed application (that only we can access). Implement a <code>/monitoring/</code> endpoint that does     all the reporting we just went through such that you have two endpoints:</p> <pre><code>http://127.0.0.1:8000/iris_infer/?sepal_length=1.0&amp;sepal_width=1.0&amp;petal_length=1.0&amp;petal_width=1.0 # user endpoint\nhttp://127.0.0.1:8000/iris_monitoring/ # monitoring endpoint\n</code></pre> <p>Our monitoring endpoint should return a HTML page either showing an Evidently report or test suit. Try implementing this endpoint. We have implemented a solution in this file if you need help with how to return an HTML page from a FastAPI application.</p> </li> <li> <p>As an final exercise, we recommend that you try implementing this to run directly in the cloud. You will need to     implement this in a container e.g. GCP Run service because the data gathering from the endpoint should still be     implemented as an background task. For this to work you will need to change the following:</p> </li> <li> <p>Instead of saving the input to a local file you should either store it in GCP bucket or an         BigQuery SQL table (this is a better solution, but also         out-of-scope for this course)</p> </li> <li> <p>You can either run the data analysis locally by just pulling from cloud storage predictions and training data         or alternatively you can deploy this as its own endpoint that can be invoked. For the latter option we recommend         that this should require authentication.</p> </li> </ol> <p>That ends the module on detection of data drifting, data quality etc. If this has not already been made clear, monitoring of machine learning applications is an extremely hard discipline because it is not a clear cut when we should actually respond to feature beginning to drift and when it is probably fine. That comes down to the individual application what kind of rules that should be implemented. Additionally, the tools presented here are also in no way complete and are especially limited in one way: they are only considering the marginal distribution of data. Every analysis that we done have been on the distribution per feature (the marginal distribution), however as the image below show it is possible for data to have drifted to another distribution with the marginal being approximatively the same.</p> <p></p> <p>There are methods such as Maximum Mean Discrepancy (MMD) tests that are able to do testing on multivariate distributions, which you are free to dive into. In this course we will just always recommend to consider multiple features when doing decision regarding your deployed applications.</p>"},{"location":"s8_monitoring/monitoring/","title":"M26 - System Monitoring","text":""},{"location":"s8_monitoring/monitoring/#monitoring","title":"Monitoring","text":"<p>In this module we are going to look into more classical monitoring of applications. The key concept we are often working with here is called telemetry. Telemetry in general refer to any automatic measurement and wireless transmission of data from our application. It could be numbers such as:</p> <ul> <li>The number of requests are our application receiving per minute/hour/day. This number is of interest because it is   directly proportional to the running cost of application.</li> <li>The amount of time (on average) our application runs per request. The number is of interest because it most likely is   the core contributor to the latency that our users are experience (which we want to be low).</li> <li>...</li> </ul>"},{"location":"s8_monitoring/monitoring/#exercises","title":"\u2754 Exercises","text":"<p>The exercise here is simply to follow the instructions in this repository:</p> <p>https://github.com/duarteocarmo/dtu-mlops-monitoring</p> <p>by Duarte Carmo. The repository goes over the steps to setup an simple application that uses FastAPI to implement a application and opentelemetry to extract relevant telemetry data that is then visualized through Signoz. Importantly you should try to take a look at the implemented application to see how opentelemetry is integrated into a FastAPI application.</p> <p>If you manage to get the done with the steps in the repository we recommend that you can try to deploy this to GCP by following these steps. However, we do not really recommend it as it requires setting up a Kubernetes cluster (because we are running multiple docker containers) which is not part of the curriculum in this course</p>"},{"location":"s8_monitoring/monitoring/#alert-systems","title":"Alert systems","text":"<p>A core problem within monitoring is alert systems. The alert system is in charge of sending out alerts to relevant people when some telemetry or metric we are tracking is not behaving as it should. Alert systems are a subjective choice of when and how many should be send out and in general should be proportional with how important to the of the metric/telemetry. We commonly run into what is referred to the goldielock problem where we want just the right amount of alerts however it is more often the case that we either have</p> <ul> <li>Too many alerts, such that they become irrelevant and the really important onces are overseen, often referred to as   alert fatigue</li> <li>Or alternatively, we have too little alerts and problems that should have triggered an alert is not dealt with when   they happen which can have unforeseen consequences.</li> </ul> <p>Therefore, setting up proper alert systems can be as challenging as setting up the systems for actually the metrics we want to trigger alerts.</p>"},{"location":"s8_monitoring/monitoring/#exercises_1","title":"\u2754 Exercises","text":"<p>We are in this exercise going to look at how we can setup automatic alerting such that we get an message every time one of our applications are not behaving as expected.</p> <ol> <li> <p>Go to the <code>Monitoring</code> service. Then go to <code>Alerting</code> tab.      </p> </li> <li> <p>Start by setting up an notification channel. A recommend setting up with an email.</p> </li> <li> <p>Next lets create a policy. Clicking the <code>Add Condition</code> should bring up a window as below. You are free to setup the    condition as you want but the image is one way bo setup an alert that will react to the number of times an cloud    function is invoked (actually it measures the amount of log entries from cloud functions).</p> <p> </p> </li> <li> <p>After adding the condition, add the notification channel you created in one of the earlier steps. Remember to also     add some documentation that should be send with the alert to better describe what the alert is actually doing.</p> </li> <li> <p>When the alert is setup you need to trigger it. If you setup the condition as the image above you just need to     invoke the cloud function many times. Here is a small code snippet that you can execute on your laptop to call a     cloud function many time (you need to change the url and payload depending on your function):</p> <pre><code>import time\nimport requests\nurl = 'https://us-central1-dtumlops-335110.cloudfunctions.net/function-2'\npayload = {'message': 'Hello, General Kenobi'}\nfor _ in range(1000):\nr = requests.get(url, params=payload)\n</code></pre> </li> <li> <p>Make sure that you get the alert through the notification channel you setup.</p> </li> </ol>"},{"location":"s9_scalable_applications/","title":"Scaling applications","text":"<p>Slides</p> <p> </p> <p>This module is all about scaling the applications that we are building. We are here going to use a very narrow definition of scaling namely that we want our applications to run faster, however one should note that in general scaling is a much broader term. There are many different ways to scale your applications and we are going to look at three of these related to different tasks machine learning algorithms:</p> <ul> <li>Scaling data loading</li> <li>Scaling training</li> <li>Scaling inference</li> </ul> <p>We are going to approach the term scaling from two different angles that both should result in your application running faster. The first approach is levering multiple devices, such as using multiple CPU cores or parallelizing training across multiple GPUs. The second approach is more analytical, were we are actually going to look at how we can design smaller/faster model architectures that runs faster.</p> <p>It should be noted that this module is specific to working with Pytorch applications. In particular we are going to see how we can both improve base Pytorch code and how to utilize the Pytorch Lightning which we introduced in module M14 on boilerplate to improve the scaling of our applications. If your application is written using another framework we can guarantee that the same techniques in these modules transfers to that framework, but may require you do seek out how to specifically to it.</p> <p>If you manage to complete all modules in this session, feel free to checkout the extra module on scalable hyperparameter optimization.</p>"},{"location":"s9_scalable_applications/data_loading/","title":"M27 - Distributed Data Loading","text":""},{"location":"s9_scalable_applications/data_loading/#distributed-data-loading","title":"Distributed Data Loading","text":"<p>Core Module</p> <p>One way that deep learning fundamentally changed the way we think about data in machine learning is that more data is always better. This was very much not the case with more traditional machine learning algorithms (random forest, support vector machines etc.) where a pleatau in performance was often reached for a certain amount of data and did not improve if more was added. However, as deep learning models have become deeper and deeper and thereby more and more data hungry performance seems to be ever increasing or at least not reaching a pleatau in the same way as for traditional machine learning.</p> <p></p>  Image credit  <p>As we are trying to feed more and more data into our models and obvious first question to ask is how to do this in a efficient way. As an general rule of thumb we want the performance bottleneck to be the forward/backward e.g. the actual computation in our neural network and not the data loading. By bottleneck we here refer to the part of our pipeline that is restricting how fast we can process data. If data loading is our bottleneck, then our compute device can sit idle while waiting for data to arrive, which is both inefficient and costly. For example if you are using a cloud provider for training deep learning models, you are paying by the hour per device, and thus not using them fully can be costly in the long run.</p> <p>In the first set of exercises we are therefore going to focus on distributed data loading i.e. how do load data in parallel to make sure that we always have data ready for our compute devices. We are in the following going to look at what is going on behind the scene when we use Pytorch to parallelize data loading.</p>"},{"location":"s9_scalable_applications/data_loading/#a-closer-look-on-data-loading","title":"A closer look on Data loading","text":"<p>Before we talk distributed applications it is important to understand the physical  layout of a standard CPU (the brain of your computer).</p> <p></p> <p>Most modern CPUs is a single chip that consist of multiple cores. Each core can further be divided into threads. In most laptops the core count is 4 and commonly 2 threads per code. This means that the common laptop have 8 threads. The number of threads a compute unit has is important, because that directly corresponds to the number of parallel operations that can be executed i.e. one per thread. In a Python terminal you should be able to get the number of cores in your machine by writing (try it):</p> <pre><code>import multiprocessing\ncores = multiprocessing.cpu_count()\nprint(f\"Number of cores: {cores}, Number of threads: {2*cores}\")\n</code></pre> <p>A distributed application is in general any kind of application that parallelizes some or all of it workload. We are in these exercises only focusing on distributed data loading, which happens primarily only on the CPU. In <code>Pytorch</code> it is easy to parallelize data loading if you are using their dataset/dataloader interface:</p> <pre><code>from torch.utils.data import Dataset, DataLoader\nclass MyDataset(Dataset):\ndef __init__(self, ...):\n# whatever logic is needed to init the data set\nself.data = ...\ndef __getitem__(self, idx):\n# return one item\nreturn self.data[idx]\ndataset = MyDataset()\ndataloader = Dataloader(\ndataset,\nbatch_size=8,\nnum_workers=4  # this is the number of threds we want to parallize workload over\n)\n</code></pre> <p>Lets take a deep dive into what happens when we request a batch from our dataloader e.g. <code>next(dataloader)</code>. First we must understand that we have a thread that plays the role of the main and the remaining threads (in the above example we request 4) are called workers. When the dataloader is created, we create this structure and make sure that all threads have a copy of our dataset definition so each can call the <code>__getitem__</code> method.</p> <p></p> <p>Then comes the actual part where we request a batch for data. Assume that we have a batch size of 8 and we do not do any shuffeling. In this step the master thread then distributes the list of requested data points (<code>[0,1,2,3,4,5,6,7]</code>) to the four worker threads. With 8 indices and 4 workers, each worker will receive 2 indices.</p> <p></p> <p>Each worker thread then calls <code>__getitem__</code> method for all the indices it has received. When all processes are done, the loaded images datapoints gets send back to the master thread collected into a single structure/tensor.</p> <p></p> <p>Each arrow is corresponds to a communication between two threads, which is not a free operations. In total to get a single batch (not counting the initial startup cost) in this example we need to do 8 communication operations. This may seem like a small price to pay, but that may not be the case. If the process time of <code>__getitem__</code> is very low (data is stored in memory, we just need to index to get it) then it does not make sense to use multiprocessing. The computationally saving by doing the look-up operations in parallel is smaller than the communication cost there is between the main thread and the workers. Multiprocessing makes sense when the process time of <code>__getitem__</code> is high (data is probably stored on the harddrive).</p> <p>It is this trade-off that we are going to investigate in the exercises.</p>"},{"location":"s9_scalable_applications/data_loading/#exercises","title":"\u2754 Exercises","text":"<p>This exercise is intended to be done on the labeled faces in the wild (LFW) dataset. The dataset consist images of famous people extracted from the internet. The dataset had been used to drive the field of facial verification, which you can read more about here. We are going imagine that this dataset cannot fit in memory, and your job is therefore to construct a data pipeline that can be parallelized based on loading the raw datafiles (.jpg) at runtime.</p> <ol> <li> <p>Download the dataset and extract to a folder. It does not matter if you choose the non-aligned or aligned version of     the dataset.</p> </li> <li> <p>We provide the <code>lfw_dataset.py</code> file where we have started the process of defining a data class. Fill out the     <code>__init__</code>, <code>__len__</code> and <code>__getitem__</code>. Note that <code>__getitem__</code> expect that you return a single <code>img</code> which should     be a <code>torch.Tensor</code>. Loading should be done using PIL Image, as <code>PIL</code>     images is the default input format for torchvision for     transforms (for data augmentation).</p> </li> <li> <p>Make sure that the script runs without any additional arguments</p> <pre><code>python lfw_dataset.py\n</code></pre> </li> <li> <p>Visualize a single batch by filling out the codeblock after the first TODO right after defining the dataloader.     The visualization should show when launching the script as</p> <pre><code>python lfw_dataset.py -visualize_batch\n</code></pre> <p>Hint: this tutorial.</p> </li> <li> <p>Experiment how the number of workers influences the performance. We have already provide code that will pass over 100     batches from the dataset 5 times and calculate how long time it took, which you can play around with by calling</p> <pre><code>python lfw_dataset.py -get_timing -num_workers 1\n</code></pre> <p>Make a errorbar plot with number of workers along the x-axis and the timing along the y-axis. The errorbars should correspond to the standard deviation over the 5 runs. HINT: if it is taking too long to evaluate, measure the time over less batches (set the <code>-batches_to_check</code> flag). Also if you are not seeing an improvement, try increasing the batch size (since data loading is parallelized per batch).</p> <p>For certain machines like the Mac with M1 chipset it is nessesary to set the <code>multiprocessing_context</code> flag in the dataloder to <code>\"fork\"</code>. This essentially tells the dataloader how the worker nodes should be created.</p> </li> <li> <p>Retry the experiment where you change the data augmentation to be more complex:</p> <pre><code>lfw_trans = transforms.Compose([\ntransforms.RandomAffine(5, (0.1, 0.1), (0.5, 2.0)),\n# add more transforms here\ntransforms.ToTensor()\n])\n</code></pre> <p>by making the augmentation more computationally demanding, it should be easier to get an boost in performance when using multiple workers because the data augmentation is also executed in parallel.</p> </li> <li> <p>(Optional, requires access to GPU) If your dataset fits in GPU memory it is beneficial to set the <code>pin_memory</code> flag     to <code>True</code>. By setting this flag we are essentially telling Pytorch that they can lock the data in-place in memory     which will make the transfer between the host (CPU) and the device (GPU) faster.</p> </li> </ol> <p>This ends the module on distributed data loading in Pytorch. If you want to go into more details we highly recommend that you read this paper that goes into great details on analyzing on how data loading in Pytorch work and performance benchmarks. </p>"},{"location":"s9_scalable_applications/distributed_training/","title":"M28 - Distributed Training","text":""},{"location":"s9_scalable_applications/distributed_training/#distributed-training","title":"Distributed Training","text":"<p>In this module we are going to look at distributed training. Distributed training is one of the key ingredients to all the awesome results that deep learning models are producing. For example: Alphafold the highly praised model from Deepmind that seems to have solved protein structure prediction, was trained in a distributed fashion for a few weeks. The training was done on 16 TPUv3s (specialized hardware), which is approximately equal to 100-200 modern GPUs. This means that training Alphafold without distributed training on a single GPU (probably not even possible) would take a couple of years to train! Therefore, it is simply impossible currently to train some of the state-of-the-art (SOTA) models within deep learning currently, without taking advantage of distributed training.</p> <p>When we talk about distributed training, there are a number of different paradigms that we may use to parallelize our computations</p> <ul> <li>Data parallel (DP) training</li> <li>Distributed data parallel (DDP) training</li> <li>Sharded training</li> </ul> <p>In this module we are going to look at data parallel training, which is the original way of doing parallel training and distributed data parallel training which is an improved version of data parallel. If you want to know more about sharded training which is the newest of the paradigms you can read more about it in this blog post, which describes how sharded can save over 60% of memory used during your training.</p> <p>Finally, we want to note that for all the exercises in the module you are going to need a multi GPU setup. If you have not already gained access to multi GPU machines on GCP (see the quotas exercises in this module) you will need to find another way of running the exercises. For DTU Students I can recommend checking out this optional module on using the high performance cluster (HPC) where you can get access to multi GPU resources.</p>"},{"location":"s9_scalable_applications/distributed_training/#data-parallel","title":"Data parallel","text":"<p>While data parallel today in general is seen as obsolete compared to distributed data parallel, we are still going to investigate it a bit since it offers the most simple form of distributed computations in deep learning pipeline.</p> <p>In the figure below is shown both the forward and backward step in the data parallel paradigm</p> <p></p> <p>The steps are the following:</p> <ul> <li> <p>Whenever we try to do forward call e.g. <code>out=model(batch)</code> we take the batch and divide it equally between all     devices. If we have a batch size of <code>N</code> and <code>M</code> devices each device will be sent <code>N/M</code> datapoints.</p> </li> <li> <p>Afterwards each device receives a copy of the <code>model</code> e.g. a copy of the weights that currently parametrizes our     neural network.</p> </li> <li> <p>In this step we perform the actual forward pass in parallel. This is the actual steps that can help us scale     our training.</p> </li> <li> <p>Finally we need to send back the output of each replicated model to the primary device.</p> </li> </ul> <p>Similar to the analysis we did of parallel data loading, we cannot always expect that this will actual take less time than doing the forward call on a single GPU. If we are parallelizing over <code>M</code> devices, we essentially need to do <code>3xM</code> communication calls to send batch, model and output between the devices. If the parallel forward call does not outweigh this, then it will take longer.</p> <p>In addition, we also have the backward path to focus on</p> <ul> <li> <p>As the end of the forward collected the output on the primary device, this is also where the loss is accumulated.     Thus, loss gradients are first calculated on the primary device</p> </li> <li> <p>Next we scatter the gradient to all the workers</p> </li> <li> <p>The workers then perform a parallel backward pass through their individual model</p> </li> <li> <p>Finally, we reduce (sum) the gradients from all the workers on the main process such that we can do gradient descend.</p> </li> </ul> <p>One of the big downsides of using data parallel is that all the replicas are destroyed after each backward call. This means that we over and over again need to replicate our model and send it to the devices that are part of the computations.</p> <p>Even though it seems like a lot of logic is implementing data parallel into your code, in Pytorch we can very simply enable data parallel training by wrapping our model in the nn.DataParallel class.</p> <pre><code>from torch import nn\nmodel = MyModelClass()\nmodel = nn.DataParallel(model, device_ids=[0, 1])  # data parallel on gpu 0 and 1\npreds = model(input)  # same as usual\n</code></pre>"},{"location":"s9_scalable_applications/distributed_training/#exercises","title":"\u2754 Exercises","text":"<p>Please note that the exercise only makes sense if you have access to multiple GPUs.</p> <ol> <li> <p>Create a new script (call it <code>data_parallel.py</code>) where you take a copy of model <code>FashionCNN</code>     from the <code>fashion_mnist.py</code> script. Instantiate the model and wrap <code>torch.nn.DataParallel</code>     around it such that it can be executed in data parallel.</p> </li> <li> <p>Try to run inference in parallel on multiple devices (pass a batch multiple times and time it) e.g.</p> <pre><code>import time\nstart = time.time()\nfor _ in range(n_reps):\nout = model(batch)\nend = time.time()\n</code></pre> <p>Does data parallel decrease the inference time? If no, can you explain why that may be? Try playing around with the batch size, and see if data parallel is more beneficial for larger batch sizes.</p> </li> </ol>"},{"location":"s9_scalable_applications/distributed_training/#distributed-data-parallel","title":"Distributed data parallel","text":"<p>It should be clear that there is huge disadvantage of using the data parallel paradigm to scale your applications: the model needs to replicated on each pass (because it is destroyed in the end), which requires a large transfer of data. This is the main problem that distributed data parallel tries to solve.</p> <p></p> <p>The two key difference between distributed data parallel and data parallel that we move the model update (the gradient step) to happen on each device in parallel instead of only on the main device. This has the consequence that we do not need to move replicate the model on each step, instead we just keep a local version on each device that we keep updating. The full set of steps (as shown in the figure):</p> <ul> <li> <p>Initialize an exact copy of the model on each device</p> </li> <li> <p>From disk (or memory) we start by loading data into a section of page-locked host memory per device. Page-locked     memory is essentially a way to reverse a piece of a computers memory for a specific transfer that is going to     happen over and over again to speed it up. The page-locked regions are loaded with non-overlapping data.</p> </li> <li> <p>Transfer data from page-locked memory to each device in parallel</p> </li> <li> <p>Perform forward  pass in parallel</p> </li> <li> <p>Do a all-reduce operation on the gradients. An all-reduce operation is a so call all-to-all  operation meaning     that all processes send their own gradient to all other processes and also received from all other processes.</p> </li> <li> <p>Reduce the combined gradient signal from all processes and update the individual model in parallel. Since all     processes received the same gradient information, all models will still be in sync.</p> </li> </ul> <p>Thus, in distributed data parallel we here end up only doing a single communication call between all processes, compared to all the communication going on in data parallel. While all-reduce is a more expensive operation that many of the other communication operations that we can do, because we only have to do a single we gain a huge performance boost. Empirically distributed data parallel tends to be 2-3 times faster than data parallel.</p> <p>However, this performance increase does not come for free. Where we could implement data parallel in a single line in Pytorch, distributed data parallel is much more involving.</p>"},{"location":"s9_scalable_applications/distributed_training/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>We have provided an example of how to do distributed data parallel training in Pytorch in the two     files <code>distributed_example.py</code> and <code>distributed_example.sh</code>. You objective is to get a understanding of the necessary     components in the script to get this kind of distributed training to work. Try to answer the following questions     (HINT: try to Google around):</p> <ol> <li> <p>What is the function of the <code>DDP</code> wrapper?</p> </li> <li> <p>What is the function of the <code>DistributedSampler</code>?</p> </li> <li> <p>Why is it necessary to call <code>dist.barrier()</code> before passing a batch into the model?</p> </li> <li> <p>What does the different environment variables do in the <code>.sh</code> file</p> </li> </ol> </li> <li> <p>Try to benchmark the runs using 1 and 2 GPUs</p> </li> <li> <p>The first exercise have hopefully convinced you that it can be quite the trouble writing distributed training     applications yourself. Luckily for us, <code>Pytorch-lightning</code> can take care of this for us such that we do not have to     care about the specific details. To get your model training on multiple GPUs you need to change two arguments in the     trainer: the <code>accelerator</code> flag and the <code>gpus</code> flag. In addition to this, you can read through this     guide about any additional steps you may     need to do (for many of you, it should just work). Try running your model on multiple GPUs.</p> </li> <li> <p>Try benchmarking your training using 1 and 2 gpus e.g. try running a couple of epochs and measure how long time it     takes. How much of a speedup can you actually get? Why can you not get a speedup of 2?</p> </li> </ol>"},{"location":"s9_scalable_applications/inference/","title":"M29 - Scalable Inference","text":""},{"location":"s9_scalable_applications/inference/#scalable-inference","title":"Scalable Inference","text":"<p>Inference is task of applying our trained model to some new and unseen data, often called prediction. Thus, scaling inference is different from scaling data loading and training, mainly due to inference normally only using a single data point (or a few). As we can neither parallelize the data loading or parallelize using multiple GPUs (at least not in any efficient way), this is of no use to us when we are doing inference. Secondly, inference is often not something we do on machines that can perform large computations, as most inference today is actually either done on edge devices e.g. mobile phones or in low-cost-low-compute cloud environments. Thus, we need to be smarter about how we scale inference than just throwing more compute at it.</p> <p>In this module we are going to look at various ways that you can either reduce the size of your model and or make your model faster. Both are important for running inference fast regardless of the setup you are running your model on. We want to note that this is still very much an active area of research and therefore best practices for what to do in a specific situation can change.</p>"},{"location":"s9_scalable_applications/inference/#choosing-the-right-architecture","title":"Choosing the right architecture","text":"<p>Assume you are starting a completely new project and have to come up with a model architecture for doing this. What is you strategy? The common way to do this, is to look at prior work on similar problems that you are facing and either directly choosing the same architecture or creating some slight variation hereof. This is a great way to get started, but the architecture that you end up choosing may be optimal in terms of performance but not inference speed.</p> <p>The fact is that not all base architectures are created equal, and a 10K parameter model with one architecture can have significantly different inference speed than another 10K parameter model with another architecture. For example, consider the figure below which compares a number of models from the [timm] package, colored based on their base architecture. The general trend is that the number of images that can be processed by a model per sec (y-axis) is inverse proportional to the number of parameters (x-axis). However, we in general see that convolutional base architectures (conv) are more efficient than transformer (vit) for the same parameter budget.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises","title":"\u2754 Exercises","text":"<p>As dissed in this blogpost the largest increase in inference speed you will see (given some speficic hardware) is choosing an efficient model architechture. In the exercises below we are going to investigate the inference speed of different architechtures.</p> <ol> <li> <p>Start by checking out this     table     which contains a list of pretrained weights in <code>torchvision</code>. Try finding an</p> <ul> <li>Efficientnet</li> <li>Resnet</li> <li>Transformer based</li> </ul> <p>model that have in the range of 20-30 mio parameters.</p> </li> <li> <p>Write a small script that initialize all models and does inference with them. It should look something like this</p> <pre><code>import time\nfrom torchvision import models\nm1 = models.ModelArchitechture1()\nm2 = models.ModelArchitechture2()\nm3 = models.ModelArchitechture3()\ninput = torch.randn(100, 3, 256, 256)\nfor i, m in enumerate([m1, m2, m3]):\ntic = time.time()\nfor _ in range(n_reps):\n_ = m(input)\ntoc = time.time()\nprint(f\"Model {i} took: {(toc - tic) / n_reps}\")\n</code></pre> </li> <li> <p>Does the results make sense? Based on the above figure we would expect that efficientnet is faster than resnet,     which is faster than the transformer based model. Is this also what you are seeing?</p> </li> <li> <p>To figure out why one net is more efficient than another we can try to count the operations each network need to     do for inference. A operation here we can define as a     FLOP (floating point operation) which is any mathematical operation (such as     +, -, *, /) or assignment that involves floating-point numbers. Luckily for us someone has already created a python     package for calculating this in pytorch: ptflops</p> <ol> <li> <p>Install the package</p> <pre><code>pip install ptflops\n</code></pre> </li> <li> <p>Try calling the <code>get_model_complexity_info</code> function from the <code>ptflops</code> package on the networks from the     previous exercise. What are the results?</p> </li> </ol> </li> <li> <p>In the table from the initial exercise, you could also see the overall performance of each network on the     Imagenet-1K dataset. Given this performance, the inference speed, the flops count what network would you choose     to use in a production setting? Discuss when choosing one over another should be considered.</p> </li> </ol>"},{"location":"s9_scalable_applications/inference/#quantization","title":"Quantization","text":"<p>Quantization is a technique where all computations are performed with integers instead of floats. We are essentially taking all continuous signals and converting them into discretized signals.</p> <p></p>  Image credit  <p>As discussed in this blogpost series, while <code>float</code> (32-bit) is the primarily used precision in machine learning because is strikes a good balance between memory consumption, precision and computational requirement it does not mean that during inference we can take advantage of quantization to improve the speed of our model. For instance:</p> <ul> <li> <p>Floating-point computations are slower than integer operations</p> </li> <li> <p>Recent hardware have specialized hardware for doing integer operations</p> </li> <li> <p>Many neural networks are actually not bottlenecked by how many computations they need to do but by how fast we can     transfer data e.g. the memory bandwidth and cache of your system is the limiting factor. Therefore working with 8-bit     integers vs 32-bit floats means that we can approximately move data around 4 times as fast.</p> </li> <li> <p>Storing models in integers instead of floats save us approximately 75% of the ram/harddisk space whenever we save     a checkpoint. This is especially useful in relation to deploying models using docker (as you hopefully remember) as     it will lower the size of our docker images.</p> </li> </ul> <p>But how do we convert between floats and integers in quantization? In most cases we often use a linear affine quantization:</p> <p>$$ x_{int} = \\text{round}\\left( \\frac{x_{float}}{s} + z \\right) $$</p> <p>where $s$ is a scale and $z$ is the so called zero point. But how does to doing inference in a neural network. The figure below shows all the conversations that we need to make to our standard inference pipeline to actually do computations in quantized format.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Lets look at how quantized tensors look in Pytorch</p> <ol> <li> <p>Start by creating a tensor that contains both random numbers</p> </li> <li> <p>Next call the <code>torch.quantize_per_tensor</code> function on the tensor. What does the quantized tensor     look like? How does the values relate to the <code>scale</code> and <code>zero_point</code> arguments.</p> </li> <li> <p>Finally, try to call the <code>.dequantize()</code> method on the tensor. Do you get a tensor back that is     close to what you initially started out with.</p> </li> </ol> </li> <li> <p>As you hopefully saw in the first exercise we are going to perform a number of rounding errors when     doing quantization and naively we would expect that this would accumulate and lead to a much worse model.     However, in practice we observe that quantization still works, and we actually have a mathematically     sound reason for this. Can you figure out why quantization still works with all the small rounding     errors? HINT: it has to do with the central limit theorem</p> </li> <li> <p>Lets move on to quantization of our model. Follow this     tutorial from Pytorch on how to do quantization. The goal is     to construct a model <code>model_fc32</code> that works on normal floats and a quantized version <code>model_int8</code>. For simplicity     you can just use one of the models from the tutorial.</p> </li> <li> <p>Lets try to benchmark our quantized model and see if all the trouble that we went through actually paid of. Also     try to perform the benchmark on the non-quantized model and see if you get a difference. If you do not get an     improvement, explain why that may be.</p> </li> </ol>"},{"location":"s9_scalable_applications/inference/#pruning","title":"Pruning","text":"<p>Pruning is another way for reducing the model size and maybe improve performance of our network. As the figure below illustrates, in pruning we are simply removing weights in our network that we do not consider important for the task at hand. By removing, we here mean that the weight gets set to 0. There are many ways to determine if a weight is important but the general rule that the importance of a weight is proportional to the magnitude of a given weight. This makes intuitively sense, since weights in all linear operations (fully connected or convolutional) are always multiplied onto the incoming value, thus a small weight means a small outgoing activation.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises_2","title":"\u2754 Exercises","text":"<ol> <li> <p>We provide a start script that implements the famous     LeNet in this     file.     Open and run it just to make sure that you know the network.</p> </li> <li> <p>Pytorch have already some pruning methods implemented in its package.     Import the <code>prune</code> module from <code>torch.nn.utils</code> in the script.</p> </li> <li> <p>Try to prune the weights of the first convolutional layer by calling</p> <pre><code>prune.random_unstructured(module_1, name=\"weight\", amount=0.3)\n</code></pre> <p>Try printing the <code>named_parameters</code>, <code>named_buffers</code> before and after the module is pruned. Can you explain the difference and what is the connection to the <code>module_1.weight</code> attribute. Hint: You can read about the prune method here.</p> </li> <li> <p>Try pruning the bias of the same module this time using the <code>l1_unstructured</code> function from the pruning module. Again     check the  <code>named_parameters</code>, <code>named_buffers</code> argument to make sure you understand the difference between L1 pruning     and unstructured pruning.</p> </li> <li> <p>Instead of pruning only a single module in the model lets try pruning the hole model. To do this we just need to     iterate over all <code>named_modules</code> in the model like this:</p> <pre><code>for name, module in new_model.named_modules():\nprune.l1_unstructured(module, name='weight', amount=0.2)\n</code></pre> <p>But what if we wanted to apply different pruning to different layers. Implement a pruning scheme where</p> <ul> <li>The weights of convolutional layers are L1 pruned with <code>amount=0.2</code></li> <li>The weights of linear layers are unstructured pruned with <code>amount=0.4</code></li> </ul> <p>Print <code>print(dict(new_model.named_buffers()).keys())</code> after the pruning to confirm that all weights have been correctly pruned.</p> </li> <li> <p>The pruning we have looked at until know have only been local in nature e.g. we have applied the pruning     independently for each layer, not accounting globally for how much we should actually prune. As you may realize this     can quickly lead to an network that is pruned too much. Instead, the more common approach is too prune globally     where we remove the smallest <code>X</code> amount of connections:</p> <ol> <li> <p>Start by creating a tuple over all the weights with the following format</p> <pre><code>parameters_to_prune = (\n(model.conv1, 'weight'),\n# fill in the rest of the modules yourself\n(model.fc3, 'weight'),\n)\n</code></pre> <p>The tuple needs to have length 5. Challenge: Can you construct the tuple using <code>for</code> loops, such that the code works for arbitrary size networks?</p> </li> <li> <p>Next prune using the <code>global_unstructured</code> function to globally prune the tuple of parameters</p> <pre><code>prune.global_unstructured(\nparameters_to_prune,\npruning_method=prune.L1Unstructured,\namount=0.2,\n)\n</code></pre> </li> <li> <p>Check that the amount that have been pruned is actually equal to the 20% specified in the pruning. We provide     the following function that for a given submodule (for example <code>model.conv1</code>) computes the amount of pruned     weights</p> <pre><code>def check_prune_level(module: nn.Module):\nsparsity_level = 100 * float(torch.sum(module.weight == 0) / module.weight.numel())\nprint(f\"Sparsity level of module {sparsity_level}\")\n</code></pre> </li> </ol> </li> <li> <p>With a pruned network we really want to see if all our effort actually ended up with a network that is faster and/or     smaller in memory. Do the following to the globally pruned network from the previous exercises:</p> <ol> <li> <p>First we need to make the pruning of our network permanent. Right now it is only semi-permanent as we are still     keeping a copy of the original weights in memory. Make the change permanent by calling <code>prune.remove</code> on every     pruned module in the model. Hint: iterate over the <code>parameters_to_prune</code> tuple.</p> </li> <li> <p>Next try to measure the time of a single inference (repeated 100 times) for both the pruned and non-pruned network</p> <pre><code>import time\ntic = time.time()\nfor _ in range(100):\n_ = network(torch.randn(100, 1, 28, 28))\ntoc = time.time()\nprint(toc - tic)\n</code></pre> <p>Is the pruned network actually faster? If not can you explain why?</p> </li> <li> <p>Next lets measure the size of our network (called <code>pruned_network</code>) and a freshly initialized network (called     <code>network</code>):</p> <pre><code>torch.save(pruned_network.state_dict(), 'pruned_network.pt')\ntorch.save(network.state_dict(), 'network.pt')\n</code></pre> <p>Lookup the size of each file. Are the pruned network actually smaller? If not can you explain why?</p> </li> <li> <p>Repeat the last exercise, but this time start by converting all pruned weights to sparse format first by calling     the <code>.to_sparse()</code> method on each pruned weight. Is the saved model smaller now?</p> </li> </ol> </li> </ol> <p>This ends the exercises on pruning. As you probably realized in the last couple of exercises, then pruning does not guarantee speedups out of the box. This is because linear operations in Pytorch does not handle sparse structures out of the box. To actually get speedups we would need to deep dive into the sparse tensor operations, which again does not even guarantee that a speedup because the performance of these operations depends on the sparsity structure of the pruned weights. Investigating this is out of scope for these exercises, but we highly recommend checking it out if you are interested in sparse networks.</p>"},{"location":"s9_scalable_applications/inference/#knowledge-distillation","title":"Knowledge distillation","text":"<p>Knowledge distillation is somewhat similar to pruning, in the sense that it tries to find a smaller model that can perform equally well as a large model, however it does so in a completly different way. Knowledge distillation is a model compression technique that builds on the work of Bucila et al. in which we try do distill/compress the knowledge of a large complex model (also called the teacher model) into a simpler model (also called the student model).</p> <p>The best known example of this is the DistilBERT model. The DistilBERT model is a smaller version of the large natural-language procession model Bert, which achives 97% of the performance of Bert while only containing 40% of the weights and being 60% faster. You can see in the figure below how it is much smaller in size compared to other models developed at the same time.</p> <p></p>  Image credit  <p>Knowledge distillation works by assuming we have a big teacher that is already performing well that we want to compress. By runnning our training set through our large model we get a softmax distribution for each and every training sample. The goal of the students, is to both match the original labels of the training data but also match the softmax distribution of the teacher model. The intuition behind doing this, is that teacher model needs to be more complex to learn the complex inter-class relasionship from just (one-hot) labels. The student on the other hand gets directly feed with softmax distributions from the teacher that explicit encodes this inter-class relasionship and thus does not need the same capasity to learn the same as the teacher.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises_3","title":"\u2754 Exercises","text":"<p>Lets try implementing model distillation ourself. We are going to see if we can achive this on the cifar10 dataset. Do note that exercise below can take quite long time to finish because it involves training multiple networks and therefore involve some waiting.</p> <ol> <li> <p>Start by install the <code>transformers</code> and <code>datasets</code> packages from Huggingface</p> <pre><code>pip install transformers\npip install datasets\n</code></pre> <p>which we are going to download the cifar10 dataset and a teacher model.</p> </li> <li> <p>Next download the cifar10 dataset</p> <pre><code>from datasets import load_dataset\ndataset = load_dataset(\"cifar10\")\n</code></pre> </li> <li> <p>Next lets initialize our teacher model. For this we consider a large transformer based model:</p> <pre><code>from transformers import AutoFeatureExtractor, AutoModelForImageClassification\nextractor = AutoFeatureExtractor.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\nmodel = AutoModelForImageClassification.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\n</code></pre> </li> <li> <p>To get the logits (un-normalized softmax scores) from our teacher model for a single datapoint from the training     dataset you would extract it like this:</p> <pre><code>sample_img = dataset['train'][0]['img']\npreprocessed_img = extractor(dataset['train'][0]['img'], return_tensors='pt')\noutput =  model(**preprocessed_img)\nprint(output.logits)\n# tensor([[ 3.3682, -0.3160, -0.2798, -0.5006, -0.5529, -0.5625, -0.6144, -0.4671, 0.2807, -0.3066]])\n</code></pre> <p>Repeat this process for the hole training dataset and store the result somewhere.</p> </li> <li> <p>Implement a simple convolutional model. You can create a custom one yourself or use a small one from <code>torchvision</code>.</p> </li> <li> <p>Train the model on cifar10 to convergence, so you have a base result on how the model is performing.</p> </li> <li> <p>Redo the training, but this time add knowledge distillation to your training objective. It should look like this:</p> <pre><code>for batch in dataset:\n# ...\nimg, target, teacher_logits = batch\npreds = model(img)\nloss = torch.nn.functional.cross_entropy(preds, target)\nloss_teacher = torch.nn.functional.cross_entropy(preds, teacher_logits)\nloss = loss + loss_teacher\nloss.backward()\n# ...\n</code></pre> </li> <li> <p>Compare the final performance obtained with and without knowledge distillation. Did the performance improve or not?</p> </li> </ol> <p>This ends the module on scaling inference in machine learning models.</p>"}]}